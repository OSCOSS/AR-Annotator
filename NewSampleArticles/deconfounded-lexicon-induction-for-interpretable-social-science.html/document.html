<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>Deconfounded Lexicon Induction for Interpretable Social Science</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">Deconfounded Lexicon Induction for Interpretable Social Science</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="ReidPryzant(GraduateSchoolofBusinessStanfordUniversity)" rel="bibo:authorList" resource="#ReidPryzant(GraduateSchoolofBusinessStanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Reid Pryzant (Graduate School of Business Stanford University)
                        </span>
                    </dd><dd id="KellyShen(GraduateSchoolofBusinessStanfordUniversity)" rel="bibo:authorList" resource="#KellyShen(GraduateSchoolofBusinessStanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Kelly Shen (Graduate School of Business Stanford University)
                        </span>
                    </dd><dd id="DanJurafsky(GraduateSchoolofBusinessStanfordUniversity)" rel="bibo:authorList" resource="#DanJurafsky(GraduateSchoolofBusinessStanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Dan Jurafsky (Graduate School of Business Stanford University)
                        </span>
                    </dd><dd id="StefanWager(GraduateSchoolofBusinessStanfordUniversity)" rel="bibo:authorList" resource="#StefanWager(GraduateSchoolofBusinessStanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Stefan Wager (Graduate School of Business Stanford University)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>NLP algorithms are increasingly used in computational social science to take linguistic observations and predict outcomes like human preferences or actions. Making these social models transparent and interpretable often requires identifying features in the input that predict outcomes while also controlling for potential confounds. We formalize this need as a new task: inducing a lexicon that is predictive of a set of target variables yet uncorrelated to a set of confounding variables. We introduce two deep learning algorithms for the task. The first uses a bifurcated architecture to separate the explanatory power of the text and confounds. The second uses an adversarial discriminator to force confound-invariant text encodings. Both elicit lexicons from learned weights and attentional scores. We use them to induce lexicons that are predictive of timely responses to consumer complaints (controlling for product), enrollment from course descriptions (controlling for subject), and sales from product descriptions (controlling for seller). In each domain our algorithms pick words that are associated with narrative persuasion; more predictive and less confound-related than those of standard feature weighting and lexicon induction techniques like regression and log odds.</p></section></p><div class="article-part article-body"><section id="1Introduction" inlist="" rel="schema:hasPart" resource="#1Introduction">
                            <h2 property="schema:name">1 Introduction</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1Introduction" typeof="deo:Introduction">
			                <p> Applications of NLP to computational social science and data science increasingly use lexical features (words, prefixes, etc) to help predict nonlinguistic outcomes like sales, stock prices, hospital readmissions, and other human actions or preferences. Lexical features are useful beyond predictive performance. They enhance interpretability in machine learning because practitioners know why their system works. Lexical features can also be used to understand the subjective properties of a text.</p><p>For social models, we need to be able to select lexical features that predict the desired outcome(s) while also controlling for potential confounders. For example, we might want to know which words in a product description lead to greater sales, regardless of the item‚Äôs price. Words in a description like ‚Äúluxury‚Äù or ‚Äúbargain‚Äù might increase sales but also interact with our confound (price). Such words don‚Äôt reflect the unique part of text‚Äôs effect on sales and should not be selected. Similarly, we might want to know which words in a consumer complaint lead to speedy administrative action, regardless of the product being complained about; which words in a course description lead to higher student enrollment, regardless of the course topic. These instances are associated with narrative persuasion: language that is responsible for altering cognitive responses or attitudes (Spence, 1983; Van Laer et al., 2013). In general, we want words which are predictive of their targets yet decorrelated from confounding information. The lexicons constituted by these words are useful in their own right (to develop causal domain theories or for linguistic analysis) but also as interpretable features for down-stream modeling. Such work could help widely in applications of NLP to tasks like linking text to sales figures (Ho and Wu, 1999), to voter preference (Luntz, 2007; Ansolabehere and Iyengar, 1995), to moral belief (Giles et al., 2008; Keele et al., 2009), to police respect (Voigt et al., 2017), to financial outlooks (Grinblatt and Keloharju, 2001; Chatelain and Ralf, 2012), to stock prices (Lee et al., 2014), and even to restaurant health inspections (Kang et al., 2013). Identifying linguistic features that are indicative of such outcomes and decorrelated with confounds is a common activity among social scientists, data scientists, and other machine learning practitioners. Indeed, it is essential for developing transpar-ent and interpretable machine learning NLP models. Yet there is no generally accepted and rigorously evaluated procedure for the activity. Practitioners have conducted it on a largely ad-hoc basis, applying various forms of logistic and linear regression, confound-matching, or association quantifiers like mutual information or log-odds to achieve their aims, all of which have known drawbacks (Imai and Kim, 2016; Gelman and Loken, 2014; Wurm and Fisicaro, 2014; Estevez et al. ¬¥ , 2009; Szumilas, 2010). We propose to overcome these drawbacks via two new algorithms that consider the causal structure of the problem. The first uses its architecture to learn the part of the text‚Äôs effect which the confounds cannot explain. The second uses an adversarial objective function to match text encoding distributions regardless of confound treatment. Both elicit lexicons by considering learned weights or attentional scores. In summary, we 1. Formalize the problem into a new task. 2. Propose a pair of well-performing neural network based algorithms. 3. Conduct the first systematic comparison of algorithms in the space, spanning three domains: consumer complaints, course enrollments, and e-commerce product descriptions. The techniques presented in this paper will help scientists (1) better interpret the relationship between words and real-world phenomena, and (2) render their NLP models more interpretable1 . </p></div>
                    	</section><section id="2DeconfoundedLexiconInduction" inlist="" rel="schema:hasPart" resource="#2DeconfoundedLexiconInduction">
                            <h2 property="schema:name">2 Deconfounded Lexicon Induction</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2DeconfoundedLexiconInduction" typeof="">
			                <p> We begin by formalizing this language processing activity into a task. We have access to text(s) T, target variable(s) Y , and confounding variable(s) C.</p></div>
                    	</section><section id="3ProposedAlgorithms" inlist="" rel="schema:hasPart" resource="#3ProposedAlgorithms">
                            <h2 property="schema:name">3 Proposed Algorithms</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#3ProposedAlgorithms" typeof="">
			                <p> We continue by describing the pair of novel algorithms we are proposing for deconfounded lexicon induction problems. </p><figure data-equation="" data-image="6" data-figure-category="figure" data-caption="" id="F19058561" data-image-src="/media/images/7186b5be-b131-4006-a66a-2f085e889b76.png"><div><img src="7186b5be-b131-4006-a66a-2f085e889b76.png"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure></div>
                    	</section><section id="3.1DeepResidualization(DR)" inlist="" resource="#3.1DeepResidualization(DR)">
                            <h3 property="schema:name">3.1 Deep Residualization (DR) </h3>
                        <p><strong>Motivation.</strong> Our first method is directly motivated by the setup from Section 2. Recall that I(L) measures the amount by which L(T) can improve predictions of Y made from the confounders C. We accordingly build a neural network architecture that first predicts Y directly from C as well as possible, and then seeks to fine-tune those predictions using T. </p><p><strong>Description</strong>. <span class="comment ref do" data-id="3473624629" rel="schema:hasPart" typeof="dctypes:Text" resource="r-3473624629"><mark id="3473624629" property="schema:description">
                        First we pass the confounds through a feed-forward neural network (FFNN) to obtainpreliminary predictions YÀÜ 0 
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#3473624629" resource="http://localhost:8000/document/5//comment-3473624629">
       		              üí¨
                    </a>
                </sup></span>. We also encode the text into a continuous vector e ‚àà Rd via two alternative mechanisms: </p><ol><li><p>DR+ATTN: the text is converted into a sequence of embeddings and fed into Long Short-Term Memory (LSTM) cell(s) (Hochreiter and Schmidhuber, 1997) followed by an attention mechanism inspired by Bahdanau et al. (2015). If the words of a text have been embedded as vectors x1, x2, ..., xn then e is calculated as a weighted average of hidden states, where the weights are decided by a FFNN whose parameters are shared across timesteps: h0 = ~0 ht = LSTM(xt , ht‚àí1) lt = ReLU(Wattnht) ¬∑ v attn pt = exp(lt) Pexp(li) e = Xpihi </p></li><li><p>DR+BOW: the text is converted into a vector of word frequencies, which is compressed with a two-layer feedforward neural network (FFNN): t = [freq1, freq2, ..., freqk] h = ReLU(Whiddent) e = ReLU(Woutputt) We then concatenate e with YÀÜ 0 and feed the result through another neural network to generate fi- nal predictions YÀÜ . If Y is continuous we compute loss with Lcontinuous = ||YÀÜ ‚àí Y ||2 If Y is categorical we compute loss with Lcategorical = ‚àíp ‚àó log pb‚àó Where pb‚àó corresponds to the predicted probability of the correct class. The errors from YÀÜ are propagated through the whole model, but the errors from YÀÜ 0 are only used to train its progenitor (Figure 1). Note the similarities between this model and the popular residualizing regression (RR) technique (Jaeger et al., 2009; Baayen et al., 2010, inter alia). Both use the text to improve an estimate generated from the confounds. RR treats this as two separate regression tasks, by regressing the confounds against the variables of interest, and then using the residuals as features, while our model introduces the capacity for nonlinear interactions by backpropagating between RR‚Äôs steps. </p><p><strong>Lexicon Induction.</strong> We elicit lexicons from +ATTN style models by (1) running inference on a test set, but rather than saving those predictions, saving the attentional distribution over each source text, and (2) mapping each word to its average attentional score and selecting the k highest-scoring words. For +BOW style models, we take the matrix that compresses the text‚Äôs word frequency vector, then score each word by computing the l1 norm of the column that multiplies it, with the intuition that important words are dotted with big vectors in order to be a large component of e. </p><section id="3.2AdversarialSelector(A)" inlist="" resource="#3.2AdversarialSelector(A)">
                            <h3 property="schema:name">3.2 Adversarial Selector (A)</h3>
                        <p><strong>Motivation</strong>. We begin by observing that a desirable L can explain Y , but is unrelated to C, which implies it should should struggle to predict C. The Adversarial Selector draws inspiration from this.</p></section></li></ol><p>It learns adversarial encodings of T which are useful for predicting Y , but not useful for predicting C. It is depicted in Figure 2. </p><p><strong>Description</strong>. First, we encode T into e ‚àà Rd via the same mechanisms as the Deep Residualizer of Section 3.1. e is then passed to a series of FFNNs (‚Äúprediction heads‚Äù) which are trained to predict each target and confound with the same loss functions as that of Section 3.1. As gradients backpropagate from the confound prediction heads to the encoder, we pass them through a gradient reversal layer in the style of Ganin et al. (2016) and Britz et al. (2017), which multiplies gradients by ‚àí1. If the cumulative loss of the target variables is Lt and that of the confounds is Lc, then the loss which is implicitly used to train the encoder is Le = Lt ‚àí Lc, thereby encouraging the encoder to learn representations of the text which are not useful for predicting the confounds. Lexicons are elicited from this model via the same mechanism as the Deep Residualizer of Section 3.1. </p><aside class="note do"><blockquote cite="3473624629"><article id="3473624629" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/5/#3473624629"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/5/#3473624629">
            <time datetime="1540460692875" datatype="xsd:dateTime" property="schema:datePublished" content="1540460692875">
                1540460692875
            </time>
        </a>
    </dd>
</dl>
<section id="comment-3473624629" rel="oa:hasBody" resource="i:#comment-3473624629">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-3473624629" typeof="oa:TextualBody">
        be careful
    </div>
</section>

<br/><br/></article></blockquote></aside></section><section id="4Experiments" inlist="" rel="schema:hasPart" resource="#4Experiments">
                            <h2 property="schema:name">4 Experiments </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#4Experiments" typeof="deo:Evaluation">
			                <p>We evaluate the approaches described in Sections 3 and 5 by generating and evaluating deconfounded lexicons in three domains: financial complaints, e-commerce product descriptions, and course descriptions. In each case the goal is to find words which can always help someone net a positive outcome (fulfillment, sales, enrollment), regardless of their situation. This involves finding words associated with narrative persuasion: predictive of human decisions or preferences but decorrelated from non-linguistic information which could also explain things. We analyze the resulting lexicons, especially with respect to the classic Aristotelian modes of persuasion: logos, pathos, and ethos. We compare the following algorithms: Regression (R), Regression with Confound features (RC), Mixed effects Regression (M), Residualizing Regressions (RR), Log-Odds Ratio (OR), Mutual Information (MI), and MI/OR with regresssion (R+MI and R+OR). See Section 5 for a discussion of these baselines, and the online supplementary information for implementation details. We also compare the proposed algorithms: Deep Residualization using word frequencies (DR+BOW) and embeddings (DR+ATTN), and Adversarial Selection using word frequencies (A+BOW) and embeddings (A+ATTN). In Section 2 we observed that I(L) measures the improvement in predictive power that L(T) affords a model already having access to C. Thus, we evaluate each algorithm by (1) regressing C on Y , (2) drawing a lexicon L, (3) regressing C + L(T) on Y , and (4) measuring the size of gap in test prediction error between the models of step (1) and (3). For classification problems, we measured error with cross-entropy (XE): XE = ‚àí X i pi log ÀÜpi performance = XEC ‚àí XEL(T),C And for regression, we computed the mean squared error (MSE): MSE = 1 n X i (YÀÜ i ‚àí Yi) 2 performance = MSEC ‚àí MSEL(T),C Because we fix lexicon size but vary lexicon content, lexicons with good words will score highly under this metric, yielding the large performance improvements when combined with C. We also report the average strength of association between words in L and C. For categorical confounds, we measure Cramer‚Äôs V (V ) (Cramer¬¥ , 2016), and for continuous confounds, we use the point-biserial correlation coefficient (rpb) (Glass and Hopkins, 1970). Note that rpb is mathematically equivalent to Pearson correlation in bivariate settings. Here the best lexicons will score the lowest. We implemented neural models with the Tensorflow framework (Abadi et al., 2016) and optimized using Adam (Kingma and Ba, 2014). We implemented linear models with the scikit learn package (Pedregosa et al., 2011). We implemented mixed models with the lme4 R package (Bates et al., 2014). We refer to the online supplementary materials for per-experiment hyperparameters. For each dataset, we constructed vocabularies from the 10,000 most frequently occurring tokens, and randomly selected 2,000 examples for evaluation. We then conducted a wide hyperparameter search and used lexicon performance on the evaluation set to select final model parameters. We then used these parameters to induce lexicons from 500 random train/test splits. Significance is estimated with a bootstrap procedure: we counted the number of trials each algorithm ‚Äúwon‚Äù (i.e. had the largest errorC ‚àí errorL(T),C). We also report the average performance and correlation of all the lexicons generated from each split. We ran these experiments using lexicon sizes of k = 50, 150, 250, and 500 and observed similar behavior. The results reported in the following sections are for k = 150, and the words in Tables 1, and 2, 3 are from randomly selected lexicons (other lexicons had similar characteristics). </p></div>
                    	</section><section id="4.1ConsumerFinancialProtectionBureau(CFPB)Complaints" inlist="" resource="#4.1ConsumerFinancialProtectionBureau(CFPB)Complaints">
                            <h3 property="schema:name"><strong>4.1 Consumer Financial Protection Bureau (CFPB) Complaints</strong> </h3>
                        <p><strong>Setup</strong>. We consider 189,486 financial complaints publicly filed with the Consumer Financial Protection Bureau (CFPB)2 . The CFPB is a product of Dodd-Frank legislation which solicits and addresses complaints from consumers regarding a variety of financial products: mortgages, credit reports, etc. Some submissions are handled on a timely basis (&lt; 15 days) while others languish. We are interested in identifying salient words which help push submissions through the bureaucracy and obtain timely responses, regardless of the specific nature of the complaint. Thus, our target variable is a binary indicator of whether the complaint obtained a timely response. Our confounds are twofold, (1) a categorical variable tracking the type of issue (131 categories), and (2) a categorical variable tracking the financial product (18 categories). For the proposed DR+BOW, DR+ATTN, A+BOW, and A+ATTN models, we set |e| to 1, 64, 1, and 256, respectively. Results. In general, this seems to be a tractable classification problem, and the confounds alone are moderately predictive of timely response (XEC = 1.06). The proposed methods appear to perform the best, and DR+BOW achieved the largest performance/correlation ratio (Figure 3).</p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Afantenos &amp;  Asher, 2012)</span></p></section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Afantenos, &amp;  Asher (Eds.). (2012). Modelling strategic conversation: Model, annotation design and corpus.</cite></li>
</ol>
			            </div>
                    </section><script>jQuery( document ).ready(function() {
    			        jQuery(this).find('span.comment').each(function () {
                            var id=jQuery(this).attr('data-id');
                            var top=jQuery(this).offset().top - 40;
                            jQuery(document).find('article[id="'+id+'"]').each(function () {
                                jQuery(this).css('top',top);
                            });
                        });
                    });</script>
			    </div>
			</article>
		</main>
	</body>
</html>