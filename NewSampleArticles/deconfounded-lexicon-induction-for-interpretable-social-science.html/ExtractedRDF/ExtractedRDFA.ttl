<http://www.dc4plus.com/references/rdf_sem.html#4Experiments> <http://schema.org/description> "\n			                <p>We evaluate the approaches described in Sections 3 and 5 by generating and evaluating deconfounded lexicons in three domains: financial complaints, e-commerce product descriptions, and course descriptions. In each case the goal is to find words which can always help someone net a positive outcome (fulfillment, sales, enrollment), regardless of their situation. This involves finding words associated with narrative persuasion: predictive of human decisions or preferences but decorrelated from non-linguistic information which could also explain things. We analyze the resulting lexicons, especially with respect to the classic Aristotelian modes of persuasion: logos, pathos, and ethos. We compare the following algorithms: Regression (R), Regression with Confound features (RC), Mixed effects Regression (M), Residualizing Regressions (RR), Log-Odds Ratio (OR), Mutual Information (MI), and MI/OR with regresssion (R+MI and R+OR). See Section 5 for a discussion of these baselines, and the online supplementary information for implementation details. We also compare the proposed algorithms: Deep Residualization using word frequencies (DR+BOW) and embeddings (DR+ATTN), and Adversarial Selection using word frequencies (A+BOW) and embeddings (A+ATTN). In Section 2 we observed that I(L) measures the improvement in predictive power that L(T) affords a model already having access to C. Thus, we evaluate each algorithm by (1) regressing C on Y , (2) drawing a lexicon L, (3) regressing C + L(T) on Y , and (4) measuring the size of gap in test prediction error between the models of step (1) and (3). For classification problems, we measured error with cross-entropy (XE): XE = \u2212 X i pi log \u02C6pi performance = XEC \u2212 XEL(T),C And for regression, we computed the mean squared error (MSE): MSE = 1 n X i (Y\u02C6 i \u2212 Yi) 2 performance = MSEC \u2212 MSEL(T),C Because we fix lexicon size but vary lexicon content, lexicons with good words will score highly under this metric, yielding the large performance improvements when combined with C. We also report the average strength of association between words in L and C. For categorical confounds, we measure Cramer\u2019s V (V ) (Cramer\u00B4 , 2016), and for continuous confounds, we use the point-biserial correlation coefficient (rpb) (Glass and Hopkins, 1970). Note that rpb is mathematically equivalent to Pearson correlation in bivariate settings. Here the best lexicons will score the lowest. We implemented neural models with the Tensorflow framework (Abadi et al., 2016) and optimized using Adam (Kingma and Ba, 2014). We implemented linear models with the scikit learn package (Pedregosa et al., 2011). We implemented mixed models with the lme4 R package (Bates et al., 2014). We refer to the online supplementary materials for per-experiment hyperparameters. For each dataset, we constructed vocabularies from the 10,000 most frequently occurring tokens, and randomly selected 2,000 examples for evaluation. We then conducted a wide hyperparameter search and used lexicon performance on the evaluation set to select final model parameters. We then used these parameters to induce lexicons from 500 random train/test splits. Significance is estimated with a bootstrap procedure: we counted the number of trials each algorithm \u201Cwon\u201D (i.e. had the largest errorC \u2212 errorL(T),C). We also report the average performance and correlation of all the lexicons generated from each split. We ran these experiments using lexicon sizes of k = 50, 150, 250, and 500 and observed similar behavior. The results reported in the following sections are for k = 150, and the words in Tables 1, and 2, 3 are from randomly selected lexicons (other lexicons had similar characteristics). </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
_:N1178a273545b429e936cc54d4c9e5035 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N4fcf5ef598ca4f4782c5ce8694c8aa18 .
<http://www.dc4plus.com/references/rdf_sem.html#4Experiments> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Evaluation> .
<http://www.dc4plus.com/references/rdf_sem.html#4Experiments> <http://schema.org/name> "4 Experiments " .
<http://www.dc4plus.com/references/userURI#1> <http://schema.org/name> "\n           Wrishi\n       " .
_:N1178a273545b429e936cc54d4c9e5035 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#1Introduction> .
_:Nd2fa4cb3fcc446e797e9ba43ffb054e3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N2856fce940c448079a97f53bd9b30e29 .
_:N2856fce940c448079a97f53bd9b30e29 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#4Experiments> .
_:N4fcf5ef598ca4f4782c5ce8694c8aa18 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:Nd2fa4cb3fcc446e797e9ba43ffb054e3 .
<http://www.dc4plus.com/references/rdf_sem.html#ReidPryzant(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/author> _:N4a5ca634f9fe4172b8b92043c2791043 .
<http://localhost:8000/document/5/#3473624629#comment-3473624629> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/oa#TextualBody> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/name> "Deconfounded Lexicon Induction for Interpretable Social Science" .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/ScholarlyArticle> .
<http://www.dc4plus.com/references/rdf_sem.html#3ProposedAlgorithms> <http://schema.org/name> "3 Proposed Algorithms" .
<http://www.dc4plus.com/references/rdf_sem.html#StefanWager(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/author> _:Nd761c3bea2b14b07b5eb3ae23e99abee .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/abstract> "<p>NLP algorithms are increasingly used in computational social science to take linguistic observations and predict outcomes like human preferences or actions. Making these social models transparent and interpretable often requires identifying features in the input that predict outcomes while also controlling for potential confounds. We formalize this need as a new task: inducing a lexicon that is predictive of a set of target variables yet uncorrelated to a set of confounding variables. We introduce two deep learning algorithms for the task. The first uses a bifurcated architecture to separate the explanatory power of the text and confounds. The second uses an adversarial discriminator to force confound-invariant text encodings. Both elicit lexicons from learned weights and attentional scores. We use them to induce lexicons that are predictive of timely responses to consumer complaints (controlling for product), enrollment from course descriptions (controlling for subject), and sales from product descriptions (controlling for seller). In each domain our algorithms pick words that are associated with narrative persuasion; more predictive and less confound-related than those of standard feature weighting and lexicon induction techniques like regression and log odds.</p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/net/biblio#Paper> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#KellyShen(GraduateSchoolofBusinessStanfordUniversity)> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/prov#Entity> .
<http://www.dc4plus.com/references/userURI#1> <http://schema.org/image> <https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80> .
<http://www.dc4plus.com/references/rdf_sem.html#2DeconfoundedLexiconInduction> <http://schema.org/name> "2 Deconfounded Lexicon Induction" .
<http://www.dc4plus.com/references/userURI#1> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Person> .
<http://www.dc4plus.com/references/rdf_sem.html#StefanWager(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/publisher> _:Nd761c3bea2b14b07b5eb3ae23e99abee .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Introduction> .
<http://www.dc4plus.com/references/r-3473624629> <http://schema.org/description> "\n                        First we pass the confounds through a feed-forward neural network (FFNN) to obtainpreliminary predictions Y\u02C6 0 \n                    " .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/hasPart> _:N14439a3222af47d19e43bff78d2ad681 .
<http://www.dc4plus.com/references/rdf_sem.html#ReidPryzant(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/publisher> _:N4a5ca634f9fe4172b8b92043c2791043 .
_:N14439a3222af47d19e43bff78d2ad681 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Reference> .
<http://www.dc4plus.com/references/rdf_sem.html#DanJurafsky(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/creator> _:N0acfe3a9a27c47eb8b90379e30fbfcb0 .
<http://localhost:8000/document/5/#3473624629#comment-3473624629> <http://schema.org/description> "\n        be careful\n    "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#StefanWager(GraduateSchoolofBusinessStanfordUniversity)> .
<http://www.dc4plus.com/references/rdf_sem.html#3.1DeepResidualization(DR)> <http://schema.org/hasPart> <http://www.dc4plus.com/references/r-3473624629> .
_:Nd2fa4cb3fcc446e797e9ba43ffb054e3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#3ProposedAlgorithms> .
<http://www.dc4plus.com/references/rdf_sem.html#DanJurafsky(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/publisher> _:N0acfe3a9a27c47eb8b90379e30fbfcb0 .
<http://localhost:8000/document/5/#3473624629> <http://schema.org/creator> <http://www.dc4plus.com/references/userURI#1> .
_:N4a5ca634f9fe4172b8b92043c2791043 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
_:Nd761c3bea2b14b07b5eb3ae23e99abee <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://localhost:8000/document/5/#3473624629> <http://www.w3.org/ns/oa#motivatedBy> <http://www.w3.org/ns/oa#replying> .
<http://www.dc4plus.com/references/rdf_sem.html#KellyShen(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/author> _:N30d97bed812b4363a45a5f08bb34524a .
<http://localhost:8000/document/5/#3473624629> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/oa#Annotation> .
<http://www.dc4plus.com/references/r-3473624629> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/dc/dcmitype/Text> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/hasPart> _:N1178a273545b429e936cc54d4c9e5035 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Document> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://rdfs.org/sioc/ns#Post> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/ontology/bibo/Document> .
<http://www.dc4plus.com/references/rdf_sem.html#2DeconfoundedLexiconInduction> <http://schema.org/description> "\n			                <p> We begin by formalizing this language processing activity into a task. We have access to text(s) T, target variable(s) Y , and confounding variable(s) C.</p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://localhost:8000/document/5/#3473624629#comment-3473624629> <http://www.w3.org/1999/02/22-rdf-syntax-ns#value> "\n        be careful\n    "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://localhost:8000/document/5/#3473624629> <http://schema.org/datePublished> "1540460692875"^^<http://www.w3.org/2001/XMLSchema#dateTime> .
<http://www.dc4plus.com/references/rdf_sem.html#ReidPryzant(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/creator> _:N4a5ca634f9fe4172b8b92043c2791043 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#ReidPryzant(GraduateSchoolofBusinessStanfordUniversity)> .
<http://localhost:8000/document/5/#3473624629> <http://schema.org/name> "\n    Wrishi\n    replies\n" .
<http://www.dc4plus.com/references/rdf_sem.html#KellyShen(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/publisher> _:N30d97bed812b4363a45a5f08bb34524a .
<http://www.dc4plus.com/references/rdf_sem.html#3.1DeepResidualization(DR)> <http://schema.org/name> "3.1 Deep Residualization (DR) " .
<http://www.dc4plus.com/references/rdf_sem.html#4.1ConsumerFinancialProtectionBureau(CFPB)Complaints> <http://schema.org/name> "4.1 Consumer Financial Protection Bureau (CFPB) Complaints " .
_:N2856fce940c448079a97f53bd9b30e29 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#nil> .
<http://localhost:8000/document/5/#3473624629> <http://www.w3.org/ns/oa#hasBody> <http://localhost:8000/document/5/#3473624629#comment-3473624629> .
_:N4fcf5ef598ca4f4782c5ce8694c8aa18 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#2DeconfoundedLexiconInduction> .
<http://www.dc4plus.com/references/rdf_sem.html#DanJurafsky(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/author> _:N0acfe3a9a27c47eb8b90379e30fbfcb0 .
<http://www.dc4plus.com/references/rdf_sem.html#3.2AdversarialSelector(A)> <http://schema.org/name> "3.2 Adversarial Selector (A)" .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://schema.org/description> "\n			                <p> Applications of NLP to computational social science and data science increasingly use lexical features (words, prefixes, etc) to help predict nonlinguistic outcomes like sales, stock prices, hospital readmissions, and other human actions or preferences. Lexical features are useful beyond predictive performance. They enhance interpretability in machine learning because practitioners know why their system works. Lexical features can also be used to understand the subjective properties of a text.</p><p>For social models, we need to be able to select lexical features that predict the desired outcome(s) while also controlling for potential confounders. For example, we might want to know which words in a product description lead to greater sales, regardless of the item\u2019s price. Words in a description like \u201Cluxury\u201D or \u201Cbargain\u201D might increase sales but also interact with our confound (price). Such words don\u2019t reflect the unique part of text\u2019s effect on sales and should not be selected. Similarly, we might want to know which words in a consumer complaint lead to speedy administrative action, regardless of the product being complained about; which words in a course description lead to higher student enrollment, regardless of the course topic. These instances are associated with narrative persuasion: language that is responsible for altering cognitive responses or attitudes (Spence, 1983; Van Laer et al., 2013). In general, we want words which are predictive of their targets yet decorrelated from confounding information. The lexicons constituted by these words are useful in their own right (to develop causal domain theories or for linguistic analysis) but also as interpretable features for down-stream modeling. Such work could help widely in applications of NLP to tasks like linking text to sales figures (Ho and Wu, 1999), to voter preference (Luntz, 2007; Ansolabehere and Iyengar, 1995), to moral belief (Giles et al., 2008; Keele et al., 2009), to police respect (Voigt et al., 2017), to financial outlooks (Grinblatt and Keloharju, 2001; Chatelain and Ralf, 2012), to stock prices (Lee et al., 2014), and even to restaurant health inspections (Kang et al., 2013). Identifying linguistic features that are indicative of such outcomes and decorrelated with confounds is a common activity among social scientists, data scientists, and other machine learning practitioners. Indeed, it is essential for developing transpar-ent and interpretable machine learning NLP models. Yet there is no generally accepted and rigorously evaluated procedure for the activity. Practitioners have conducted it on a largely ad-hoc basis, applying various forms of logistic and linear regression, confound-matching, or association quantifiers like mutual information or log-odds to achieve their aims, all of which have known drawbacks (Imai and Kim, 2016; Gelman and Loken, 2014; Wurm and Fisicaro, 2014; Estevez et al. \u00B4 , 2009; Szumilas, 2010). We propose to overcome these drawbacks via two new algorithms that consider the causal structure of the problem. The first uses its architecture to learn the part of the text\u2019s effect which the confounds cannot explain. The second uses an adversarial objective function to match text encoding distributions regardless of confound treatment. Both elicit lexicons by considering learned weights or attentional scores. In summary, we 1. Formalize the problem into a new task. 2. Propose a pair of well-performing neural network based algorithms. 3. Conduct the first systematic comparison of algorithms in the space, spanning three domains: consumer complaints, course enrollments, and e-commerce product descriptions. The techniques presented in this paper will help scientists (1) better interpret the relationship between words and real-world phenomena, and (2) render their NLP models more interpretable1 . </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
_:N30d97bed812b4363a45a5f08bb34524a <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://www.dc4plus.com/references/rdf_sem.html#StefanWager(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/creator> _:Nd761c3bea2b14b07b5eb3ae23e99abee .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Article> .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://schema.org/name> "1 Introduction" .
<http://localhost:8000/document/5/#3473624629#comment-3473624629> <http://schema.org/name> "Comment" .
_:N0acfe3a9a27c47eb8b90379e30fbfcb0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#DanJurafsky(GraduateSchoolofBusinessStanfordUniversity)> .
<http://www.dc4plus.com/references/rdf_sem.html#3ProposedAlgorithms> <http://schema.org/description> "\n			                <p> We continue by describing the pair of novel algorithms we are proposing for deconfounded lexicon induction problems. </p><figure data-caption=\"\" data-equation=\"\" data-figure-category=\"figure\" data-image=\"6\" data-image-src=\"/media/images/7186b5be-b131-4006-a66a-2f085e889b76.png\" id=\"F19058561\"><div><img src=\"7186b5be-b131-4006-a66a-2f085e889b76.png\"/></div><figcaption><span class=\"figure-cat-figure\" data-figure-category=\"figure\">figure 1: </span></figcaption></figure>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://www.w3.org/ns/activitystreams#Article> .
<http://www.dc4plus.com/references/r-3473624629> <http://purl.org/spar/cito/hasReplyFrom> <http://localhost:8000/document/5//comment-3473624629> .
<http://www.dc4plus.com/references/rdf_sem.html#KellyShen(GraduateSchoolofBusinessStanfordUniversity)> <http://schema.org/creator> _:N30d97bed812b4363a45a5f08bb34524a .

