<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#UrvashiKhandelwalEmail:urvashik@stanford.edu(StanfordUniversity)> .
_:N7694f75de167424c99635f30c893ef76 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
_:N19c0ea091d904af18367f7caf1214bbd <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#nil> .
<http://www.dc4plus.com/references/rdf_sem.html#3Approach> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/ProblemStatement> .
_:N29adbf4269f241e0bb085bf0dbb147da <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N19c0ea091d904af18367f7caf1214bbd .
<http://www.dc4plus.com/references/rdf_sem.html#UrvashiKhandelwalEmail:urvashik@stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:N34daa0181fa34a35b3578a091148709e .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://www.w3.org/ns/activitystreams#Article> .
<http://www.dc4plus.com/references/rdf_sem.html#3Approach> <http://schema.org/name> "3 Approach " .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://schema.org/name> "1 Introduction " .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/hasPart> _:N138fa7a610b04dbfaab468b8ffe9598a .
_:Ne53777a904e94e44a5e24c0ac58519b3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#3Approach> .
<http://www.dc4plus.com/references/rdf_sem.html#HeHeEmail:hehe@stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:N7694f75de167424c99635f30c893ef76 .
<http://www.dc4plus.com/references/rdf_sem.html#4Howmuchcontextisused?> <http://schema.org/description> "\n			                <p> LSTMs are designed to capture long-range dependencies in sequences (Hochreiter and Schmidhuber, 1997). In practice, LSTM language models are provided an infinite amount of prior context, which is as long as the test sequence goes. However, it is unclear how much of this history has a direct impact on model performance. In this section, we investigate how many tokens of context achieve a similar loss (or 1-2% difference in model perplexity) to providing the model infinite context. We consider this the effective context size. <strong>LSTM language models have an effective context size of about 200 tokens on average</strong>. We determine the effective context size by varying the number of tokens fed to the model. In particular, at test time, we feed the model the most recent n tokens: \u03B4truncate(wt\u22121, . . . , w1) = (wt\u22121, . . . , wt\u2212n), (1) where n &gt; 0 and all tokens farther away from the target wt are dropped.3 We compare the dev loss (NLL) from truncated context, to that of the infinite-context setting where all previous words are fed to the model. The resulting increase in loss indicates how important the dropped tokens are for the model. Figure 1a shows that the difference in dev loss, between truncated- and infinite-context variants of the test setting, gradually diminishes as we increase n from 5 tokens to 1000 tokens. In particular, we only see a 1% increase in perplexity as we move beyond a context of 150 tokens on PTB and 250 tokens on Wiki. Hence, we provide empirical evidence to show that LSTM language models do, in fact, model long-range dependencies, without help from extra context vectors or caches. 3Words at the beginning of the test sequence with fewer than n tokens in the context are ignored for loss computation. <strong>Changing hyperparameters does not change the effective context size</strong>. NLM performance has been shown to be sensitive to hyperparameters such as the dropout rate and model size (Melis et al., 2018). To investigate if these hyperparameters affect the effective context size as well, we train separate models by varying the following hyperparameters one at a time: (1) number of timesteps for truncated back-propogation (2) dropout rate, (3) model size (hidden state size, number of layers, and word embedding size). In Figure 1b, we show that while different hyperparameter settings result in different perplexities in the infinite-context setting, the trend of how perplexity changes as we reduce the context size remains the same. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
_:N138fa7a610b04dbfaab468b8ffe9598a <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Reference> .
<http://localhost:8000/document/8/#841546297> <http://schema.org/datePublished> "1540460904487"^^<http://www.w3.org/2001/XMLSchema#dateTime> .
_:Nb61af48df09243719761f05325f2264c <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://rdfs.org/sioc/ns#Post> .
<http://localhost:8000/document/8/#841546297> <http://schema.org/creator> <http://www.dc4plus.com/references/userURI#1> .
<http://www.dc4plus.com/references/rdf_sem.html#HeHeEmail:hehe@stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:N7694f75de167424c99635f30c893ef76 .
<http://www.dc4plus.com/references/rdf_sem.html#2LanguageModeling> <http://schema.org/description> "\n			                <p>Language models assign probabilities to sequences of words. In practice, the probability can be factorized using the chain rule P(w1, . . . , wt) = Y t i=1 P(wi |wi\u22121, . . . , w1), and language models compute the conditional probability of a target word wt given its preceding context, w1, . . . , wt\u22121. Language models are trained to minimize the negative log likelihood of the training corpus: NLL = \u2212 1 T X T t=1 log P(wt |wt\u22121, . . . , w1), and the model\u2019s performance is usually evaluated by perplexity (PP) on a held-out set: PP = exp(NLL). When testing the effect of ablations, we focus on comparing differences in the language model\u2019s losses (NLL) on the dev set, which is equivalent to relative improvements in perplexity. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#3Approach> <http://schema.org/description> "\n			                <p>Our goal is to investigate the effect of contextual features such as the length of context, word order and more, on LSTM performance. Thus, we use ablation analysis, during evaluation, to measure changes in model performance in the absence of certain contextual information. PTB Wiki Dev Test Dev Test # Tokens 73,760 82,430 217,646 245,569 Perplexity (no cache) 59.07 56.89 67.29 64.51 Avg. Sent. Len. 20.9 20.9 23.7 22.6 Table 1: Dataset statistics and performance relevant to our experiments. Typically, when testing the language model on a held-out sequence of words, all tokens prior to the target word are fed to the model; we call this the infinite-context setting. In this study, we observe the change in perplexity or NLL when the model is fed a perturbed context \u03B4(wt\u22121, . . . , w1), at test time. \u03B4 refers to the perturbation function, and we experiment with perturbations such as dropping tokens, shuffling/reversing tokens, and replacing tokens with other words from the vocabulary.1 It is important to note that we do not train the model with these perturbations. This is because the aim is to start with an LSTM that has been trained in the standard fashion, and discover how much context it uses and which features in nearby vs. long-range context are important. Hence, the mismatch in training and test is a necessary part of experiment design, and all measured losses are upper bounds which would likely be lower, were the model also trained to handle such perturbations. We use a standard LSTM language model, trained and finetuned using the Averaging SGD optimizer (Merity et al., 2018).2 We also augment the model with a cache only for Section 6.2, in order to investigate why an external copy mechanism is helpful. A short description of the architecture and a detailed list of hyperparameters is listed in Appendix A, and we refer the reader to the original paper for additional details. We analyze two datasets commonly used for language modeling, Penn Treebank (PTB) (Marcus et al., 1993; Mikolov et al., 2010) and Wikitext-2 (Wiki) (Merity et al., 2017). PTB consists of Wall Street Journal news articles with 0.9M tokens for training and a 10K vocabulary. Wiki is a larger and more diverse dataset, containing Wikipedia articles across many topics with 2.1M tokens for training and a 33K vocabulary. Additional dataset statistics are provided in Ta- 1Code for our experiments available at https:// github.com/urvashik/lm-context-analysis 2 Public release of their code at https://github. com/salesforce/awd-lstm-lm ble 1. In this paper, we present results only on the dev sets, in order to avoid revealing details about the test sets. However, we have confirmed that all results are consistent with those on the test sets. In addition, for all experiments we report averaged results from three models trained with different random seeds. Some of the figures provided contain trends from only one of the two datasets and the corresponding figures for the other dataset are provided in Appendix B. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/userURI#1> <http://schema.org/name> "\n           Wrishi\n       " .
<http://localhost:8000/document/8/#841546297#comment-841546297> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/oa#TextualBody> .
_:N2f8e453e71284284a098bb79811aadb9 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#2LanguageModeling> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/ScholarlyArticle> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/name> "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context" .
<http://localhost:8000/document/8/#841546297> <http://www.w3.org/ns/oa#hasBody> <http://localhost:8000/document/8/#841546297#comment-841546297> .
<http://www.dc4plus.com/references/rdf_sem.html#2LanguageModeling> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Model> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Document> .
<http://localhost:8000/document/8/#841546297> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/oa#Annotation> .
_:Ne53777a904e94e44a5e24c0ac58519b3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N29adbf4269f241e0bb085bf0dbb147da .
<http://www.dc4plus.com/references/rdf_sem.html#5Nearbyvs.long-rangecontext> <http://schema.org/name> "5 Nearby vs. long-range context " .
<http://www.dc4plus.com/references/rdf_sem.html#5.2Typesofwordsandtheregionofcontext> <http://schema.org/name> "5.2 Types of words and the region of context" .
<http://www.dc4plus.com/references/rdf_sem.html#DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:Nb61af48df09243719761f05325f2264c .
<http://www.dc4plus.com/references/rdf_sem.html#PengQiEmail:pengqi@stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:Nf8751cdf97c94ddc991c3dcf5ed66360 .
_:N9902c9b979904573a9fb9d26e20cd36a <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N2f8e453e71284284a098bb79811aadb9 .
_:Nf8751cdf97c94ddc991c3dcf5ed66360 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
_:N29adbf4269f241e0bb085bf0dbb147da <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#4Howmuchcontextisused?> .
<http://www.dc4plus.com/references/rdf_sem.html#HeHeEmail:hehe@stanford.edu(StanfordUniversity)> <http://schema.org/author> _:N7694f75de167424c99635f30c893ef76 .
<http://www.dc4plus.com/references/rdf_sem.html#DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:Nb61af48df09243719761f05325f2264c .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Article> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)> .
<http://localhost:8000/document/8/#841546297#comment-841546297> <http://schema.org/description> "\n        A few more to go\n    "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#PengQiEmail:pengqi@stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:Nf8751cdf97c94ddc991c3dcf5ed66360 .
<http://www.dc4plus.com/references/rdf_sem.html#DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)> <http://schema.org/author> _:Nb61af48df09243719761f05325f2264c .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/hasPart> _:N9902c9b979904573a9fb9d26e20cd36a .
<http://www.dc4plus.com/references/rdf_sem.html#5.1Doeswordordermatter?> <http://schema.org/hasPart> <http://www.dc4plus.com/references/r-841546297> .
<http://www.dc4plus.com/references/rdf_sem.html#4Howmuchcontextisused?> <http://schema.org/name> "4 How much context is used?" .
<http://www.dc4plus.com/references/r-841546297> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/dc/dcmitype/Text> .
<http://www.dc4plus.com/references/rdf_sem.html#5.1Doeswordordermatter?> <http://schema.org/name> "5.1 Does word order matter? " .
_:N19c0ea091d904af18367f7caf1214bbd <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#5Nearbyvs.long-rangecontext> .
<http://localhost:8000/document/8/#841546297#comment-841546297> <http://schema.org/name> "Comment" .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#PengQiEmail:pengqi@stanford.edu(StanfordUniversity)> .
<http://localhost:8000/document/8/#841546297> <http://www.w3.org/ns/oa#motivatedBy> <http://www.w3.org/ns/oa#replying> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#HeHeEmail:hehe@stanford.edu(StanfordUniversity)> .
<http://www.dc4plus.com/references/r-841546297> <http://schema.org/description> "\n                        We can locate the region of context beyond which the local word order has no relevance, by permuting word order locally at various points within the context.\n                    " .
<http://localhost:8000/document/8/#841546297> <http://schema.org/name> "\n    Wrishi\n    replies\n" .
_:N2f8e453e71284284a098bb79811aadb9 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:Ne53777a904e94e44a5e24c0ac58519b3 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/ontology/bibo/Document> .
<http://www.dc4plus.com/references/rdf_sem.html#UrvashiKhandelwalEmail:urvashik@stanford.edu(StanfordUniversity)> <http://schema.org/author> _:N34daa0181fa34a35b3578a091148709e .
<http://localhost:8000/document/8/#841546297#comment-841546297> <http://www.w3.org/1999/02/22-rdf-syntax-ns#value> "\n        A few more to go\n    "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#4.1Dodifferenttypesofwordsneeddifferentamountsofcontext?> <http://schema.org/name> "4.1 Do different types of words need different amounts of context?" .
<http://www.dc4plus.com/references/userURI#1> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Person> .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Introduction> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/net/biblio#Paper> .
_:N34daa0181fa34a35b3578a091148709e <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://www.dc4plus.com/references/rdf_sem.html#PengQiEmail:pengqi@stanford.edu(StanfordUniversity)> <http://schema.org/author> _:Nf8751cdf97c94ddc991c3dcf5ed66360 .
<http://www.dc4plus.com/references/rdf_sem.html#5Nearbyvs.long-rangecontext> <http://schema.org/description> "\n			                <p>An effective context size of 200 tokens allows for representing linguistic information at many levels of abstraction, such as words, sentences, topics, etc. In this section, we investigate the importance of contextual information such as word order and word identity. Unlike prior work that studies LSTM embeddings at the sentence level, we look at both nearby and faraway context, and analyze how the language model treats contextual information presented in different regions of the context. </p><figure data-caption=\"\" data-equation=\"\" data-figure-category=\"figure\" data-image=\"9\" data-image-src=\"/media/images/d12c851d-dad3-4393-b00d-a293374662ff.jpg\" id=\"F91087711\"><div><img src=\"d12c851d-dad3-4393-b00d-a293374662ff.jpg\"/></div><figcaption><span class=\"figure-cat-figure\" data-figure-category=\"figure\">figure 1: </span></figcaption></figure><figure data-caption=\"\" data-equation=\"\" data-figure-category=\"figure\" data-image=\"10\" data-image-src=\"/media/images/7f1b5bd6-9ae2-441e-ab9c-68f004192726.jpg\" id=\"F8094741\"><div><img src=\"7f1b5bd6-9ae2-441e-ab9c-68f004192726.jpg\"/></div><figcaption><span class=\"figure-cat-figure\" data-figure-category=\"figure\">figure 2: </span></figcaption></figure>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/r-841546297> <http://purl.org/spar/cito/hasReplyFrom> <http://localhost:8000/document/8//comment-841546297> .
<http://www.dc4plus.com/references/rdf_sem.html#2LanguageModeling> <http://schema.org/name> "2 Language Modeling " .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/abstract> "<p>We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.</p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#UrvashiKhandelwalEmail:urvashik@stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:N34daa0181fa34a35b3578a091148709e .
_:N9902c9b979904573a9fb9d26e20cd36a <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#1Introduction> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/prov#Entity> .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://schema.org/description> "\n			                <p>Language models are an important component of natural language generation tasks, such as machine translation and summarization. They use context (a sequence of words) to estimate a probability distribution of the upcoming word. For several years now, neural language models (NLMs) (Graves, 2013; Jozefowicz et al., 2016; Grave et al., 2017a; Dauphin et al., 2017; Melis et al., 2018; Yang et al., 2018) have consistently outperformed classical n-gram models, an improvement often attributed to their ability to model long-range dependencies in faraway context. Yet, how these NLMs use the context is largely unexplained. Recent studies have begun to shed light on the information encoded by Long Short-Term Memory (LSTM) networks. They can remember sentence lengths, word identity, and word order (Adi et al., 2017), can capture some syntactic structures such as subject-verb agreement (Linzen et al., 2016), and can model certain kinds of semantic compositionality such as negation and intensification (Li et al., 2016). However, all of the previous work studies LSTMs at the sentence level, even though they can potentially encode longer context. Our goal is to complement the prior work to provide a richer understanding of the role of context, in particular, long-range context beyond a sentence. We aim to answer the following questions: (i) How much context is used by NLMs, in terms of the number of tokens? (ii) Within this range, are nearby and long-range contexts represented differently? (iii) How do copy mechanisms help the model use different regions of context? We investigate these questions via ablation studies on a standard LSTM language model (Merity et al., 2018) on two benchmark language modeling datasets: Penn Treebank and WikiText-2. Given a pretrained language model, we perturb the prior context in various ways at test time, to study how much the perturbed information affects model performance. Specifically, we alter the context length to study how many tokens are used, permute tokens to see if LSTMs care about word order in both local and global contexts, and drop and replace target words to test the copying abilities of LSTMs with and without an external copy mechanism, such as the neural cache (Grave et al., 2017b). The cache operates by first recording tar- get words and their context representations seen in the history, and then encouraging the model to copy a word from the past when the current context representation matches that word\u2019s recorded context vector. We find that the LSTM is capable of using about 200 tokens of context on average, with no observable differences from changing the hyperparameter settings. Within this context range, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. Finally, we find that LSTMs can regenerate some words seen in the nearby context, but heavily rely on the cache to help them copy words from the long-range context. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/userURI#1> <http://schema.org/image> <https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80> .

