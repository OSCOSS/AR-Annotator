<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="UrvashiKhandelwalEmail:urvashik@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#UrvashiKhandelwalEmail:urvashik@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Urvashi Khandelwal <i>Email: urvashik@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="HeHeEmail:hehe@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#HeHeEmail:hehe@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            He He <i>Email: hehe@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="PengQiEmail:pengqi@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#PengQiEmail:pengqi@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Peng Qi <i>Email: pengqi@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Dan Jurafsky <i>Email: jurafsky@stanford.edu</i> (Stanford University)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.</p></section></p><div class="article-part article-body"><section id="1Introduction" inlist="" rel="schema:hasPart" resource="#1Introduction">
                            <h2 property="schema:name">1 Introduction </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1Introduction" typeof="deo:Introduction">
			                <p>Language models are an important component of natural language generation tasks, such as machine translation and summarization. They use context (a sequence of words) to estimate a probability distribution of the upcoming word. For several years now, neural language models (NLMs) (Graves, 2013; Jozefowicz et al., 2016; Grave et al., 2017a; Dauphin et al., 2017; Melis et al., 2018; Yang et al., 2018) have consistently outperformed classical n-gram models, an improvement often attributed to their ability to model long-range dependencies in faraway context. Yet, how these NLMs use the context is largely unexplained. Recent studies have begun to shed light on the information encoded by Long Short-Term Memory (LSTM) networks. They can remember sentence lengths, word identity, and word order (Adi et al., 2017), can capture some syntactic structures such as subject-verb agreement (Linzen et al., 2016), and can model certain kinds of semantic compositionality such as negation and intensification (Li et al., 2016). However, all of the previous work studies LSTMs at the sentence level, even though they can potentially encode longer context. Our goal is to complement the prior work to provide a richer understanding of the role of context, in particular, long-range context beyond a sentence. We aim to answer the following questions: (i) How much context is used by NLMs, in terms of the number of tokens? (ii) Within this range, are nearby and long-range contexts represented differently? (iii) How do copy mechanisms help the model use different regions of context? We investigate these questions via ablation studies on a standard LSTM language model (Merity et al., 2018) on two benchmark language modeling datasets: Penn Treebank and WikiText-2. Given a pretrained language model, we perturb the prior context in various ways at test time, to study how much the perturbed information affects model performance. Specifically, we alter the context length to study how many tokens are used, permute tokens to see if LSTMs care about word order in both local and global contexts, and drop and replace target words to test the copying abilities of LSTMs with and without an external copy mechanism, such as the neural cache (Grave et al., 2017b). The cache operates by first recording tar- get words and their context representations seen in the history, and then encouraging the model to copy a word from the past when the current context representation matches that word’s recorded context vector. We find that the LSTM is capable of using about 200 tokens of context on average, with no observable differences from changing the hyperparameter settings. Within this context range, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. Finally, we find that LSTMs can regenerate some words seen in the nearby context, but heavily rely on the cache to help them copy words from the long-range context. </p></div>
                    	</section><section id="2LanguageModeling" inlist="" rel="schema:hasPart" resource="#2LanguageModeling">
                            <h2 property="schema:name">2 Language Modeling </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2LanguageModeling" typeof="deo:Model">
			                <p>Language models assign probabilities to sequences of words. In practice, the probability can be factorized using the chain rule P(w1, . . . , wt) = Y t i=1 P(wi |wi−1, . . . , w1), and language models compute the conditional probability of a target word wt given its preceding context, w1, . . . , wt−1. Language models are trained to minimize the negative log likelihood of the training corpus: NLL = − 1 T X T t=1 log P(wt |wt−1, . . . , w1), and the model’s performance is usually evaluated by perplexity (PP) on a held-out set: PP = exp(NLL). When testing the effect of ablations, we focus on comparing differences in the language model’s losses (NLL) on the dev set, which is equivalent to relative improvements in perplexity. </p></div>
                    	</section><section id="3Approach" inlist="" rel="schema:hasPart" resource="#3Approach">
                            <h2 property="schema:name">3 Approach </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#3Approach" typeof="deo:ProblemStatement">
			                <p>Our goal is to investigate the effect of contextual features such as the length of context, word order and more, on LSTM performance. Thus, we use ablation analysis, during evaluation, to measure changes in model performance in the absence of certain contextual information. PTB Wiki Dev Test Dev Test # Tokens 73,760 82,430 217,646 245,569 Perplexity (no cache) 59.07 56.89 67.29 64.51 Avg. Sent. Len. 20.9 20.9 23.7 22.6 Table 1: Dataset statistics and performance relevant to our experiments. Typically, when testing the language model on a held-out sequence of words, all tokens prior to the target word are fed to the model; we call this the infinite-context setting. In this study, we observe the change in perplexity or NLL when the model is fed a perturbed context δ(wt−1, . . . , w1), at test time. δ refers to the perturbation function, and we experiment with perturbations such as dropping tokens, shuffling/reversing tokens, and replacing tokens with other words from the vocabulary.1 It is important to note that we do not train the model with these perturbations. This is because the aim is to start with an LSTM that has been trained in the standard fashion, and discover how much context it uses and which features in nearby vs. long-range context are important. Hence, the mismatch in training and test is a necessary part of experiment design, and all measured losses are upper bounds which would likely be lower, were the model also trained to handle such perturbations. We use a standard LSTM language model, trained and finetuned using the Averaging SGD optimizer (Merity et al., 2018).2 We also augment the model with a cache only for Section 6.2, in order to investigate why an external copy mechanism is helpful. A short description of the architecture and a detailed list of hyperparameters is listed in Appendix A, and we refer the reader to the original paper for additional details. We analyze two datasets commonly used for language modeling, Penn Treebank (PTB) (Marcus et al., 1993; Mikolov et al., 2010) and Wikitext-2 (Wiki) (Merity et al., 2017). PTB consists of Wall Street Journal news articles with 0.9M tokens for training and a 10K vocabulary. Wiki is a larger and more diverse dataset, containing Wikipedia articles across many topics with 2.1M tokens for training and a 33K vocabulary. Additional dataset statistics are provided in Ta- 1Code for our experiments available at https:// github.com/urvashik/lm-context-analysis 2 Public release of their code at https://github. com/salesforce/awd-lstm-lm ble 1. In this paper, we present results only on the dev sets, in order to avoid revealing details about the test sets. However, we have confirmed that all results are consistent with those on the test sets. In addition, for all experiments we report averaged results from three models trained with different random seeds. Some of the figures provided contain trends from only one of the two datasets and the corresponding figures for the other dataset are provided in Appendix B. </p></div>
                    	</section><section id="4Howmuchcontextisused?" inlist="" rel="schema:hasPart" resource="#4Howmuchcontextisused?">
                            <h2 property="schema:name">4 How much context is used?</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#4Howmuchcontextisused?" typeof="">
			                <p> LSTMs are designed to capture long-range dependencies in sequences (Hochreiter and Schmidhuber, 1997). In practice, LSTM language models are provided an infinite amount of prior context, which is as long as the test sequence goes. However, it is unclear how much of this history has a direct impact on model performance. In this section, we investigate how many tokens of context achieve a similar loss (or 1-2% difference in model perplexity) to providing the model infinite context. We consider this the effective context size. <strong>LSTM language models have an effective context size of about 200 tokens on average</strong>. We determine the effective context size by varying the number of tokens fed to the model. In particular, at test time, we feed the model the most recent n tokens: δtruncate(wt−1, . . . , w1) = (wt−1, . . . , wt−n), (1) where n &gt; 0 and all tokens farther away from the target wt are dropped.3 We compare the dev loss (NLL) from truncated context, to that of the infinite-context setting where all previous words are fed to the model. The resulting increase in loss indicates how important the dropped tokens are for the model. Figure 1a shows that the difference in dev loss, between truncated- and infinite-context variants of the test setting, gradually diminishes as we increase n from 5 tokens to 1000 tokens. In particular, we only see a 1% increase in perplexity as we move beyond a context of 150 tokens on PTB and 250 tokens on Wiki. Hence, we provide empirical evidence to show that LSTM language models do, in fact, model long-range dependencies, without help from extra context vectors or caches. 3Words at the beginning of the test sequence with fewer than n tokens in the context are ignored for loss computation. <strong>Changing hyperparameters does not change the effective context size</strong>. NLM performance has been shown to be sensitive to hyperparameters such as the dropout rate and model size (Melis et al., 2018). To investigate if these hyperparameters affect the effective context size as well, we train separate models by varying the following hyperparameters one at a time: (1) number of timesteps for truncated back-propogation (2) dropout rate, (3) model size (hidden state size, number of layers, and word embedding size). In Figure 1b, we show that while different hyperparameter settings result in different perplexities in the infinite-context setting, the trend of how perplexity changes as we reduce the context size remains the same. </p></div>
                    	</section><section id="4.1Dodifferenttypesofwordsneeddifferentamountsofcontext?" inlist="" resource="#4.1Dodifferenttypesofwordsneeddifferentamountsofcontext?">
                            <h3 property="schema:name">4.1 Do different types of words need different amounts of context?</h3>
                        <p>The effective context size determined in the previous section is aggregated over the entire corpus, which ignores the type of the upcoming word. Boyd-Graber and Blei (2009) have previously investigated the differences in context used by different types of words and found that function words rely on less context than content words. We investigate whether the effective context size varies across different types of words, by categorizing them based on either frequency or parts-ofspeech. Specifically, we vary the number of context tokens in the same way as the previous section, and aggregate loss over words within each class separately. </p><p><strong>Infrequent words need more context than frequent words.</strong> We categorize words that appear at least 800 times in the training set as frequent, and the rest as infrequent. Figure 1c shows that the loss of frequent words is insensitive to missing context beyond the 50 most recent tokens, which holds across the two datasets. Infrequent words, on the other hand, require more than 200 tokens. </p><p><strong>Content words need more context than function words.</strong> Given the parts-of-speech of each word, we define content words as nouns, verbs and adjectives, and function words as prepositions and determiners.4 Figure 1d shows that the loss of nouns and verbs is affected by distant context, whereas when the target word is a determiner, the model only relies on words within the last 10 tokens. 4We obtain part-of-speech tags using Stanford CoreNLP (Manning et al., 2014). 5 10 20 50 100 200 500 1000 Context Size (number of tokens) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Increase in Loss PTB Wiki (a) Varying context size. 10 50 100 200 500 1000 Context Size (number of tokens) 60 70 80 90 100 110 Perplexity Default Model, PTB LSTM Hidden 575 (vs. 1150) 2 layers (vs. 3) Word Emb 200 (vs. 400) No LSTM layer dropout (vs. 0.25) No recurrent dropout (vs. 0.5) BPTT 100 (vs. 70) BPTT 10 (vs. 70) (b) Changing model hyperparameters. 5 10 20 50 100 200 500 1000 Context Size (number of tokens) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Increase in Loss infrequent words,PTB frequent words,PTB infrequent words,Wiki frequent words,Wiki (c) Frequent vs. infrequent words. 5 10 20 50 100 200 500 1000 Context Size (number of tokens) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Increase in Loss NN,Wiki JJ,Wiki VB,Wiki IN,Wiki DT,Wiki (d) Different parts-of-speech. Figure 1: Effects of varying the number of tokens provided in the context, as compared to the same model provided with infinite context. Increase in loss represents an absolute increase in NLL over the entire corpus, due to restricted context. All curves are averaged over three random seeds, and error bars represent the standard deviation. (a) The model has an effective context size of 150 on PTB and 250 on Wiki. (b) Changing model hyperparameters does not change the context usage trend, but does change model performance. We report perplexities to highlight the consistent trend. (c) Infrequent words need more context than frequent words. (d) Content words need more context than function words. </p><p><strong>Discussion</strong>. Overall, we find that the model’s effective context size is dynamic. It depends on the target word, which is consistent with what we know about language, e.g., determiners require less context than nouns (Boyd-Graber and Blei, 2009). In addition, these findings are consistent with those previously reported for different language models and datasets (Hill et al., 2016; Wang and Cho, 2016).</p></section><section id="5Nearbyvs.long-rangecontext" inlist="" rel="schema:hasPart" resource="#5Nearbyvs.long-rangecontext">
                            <h2 property="schema:name">5 Nearby vs. long-range context </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#5Nearbyvs.long-rangecontext" typeof="">
			                <p>An effective context size of 200 tokens allows for representing linguistic information at many levels of abstraction, such as words, sentences, topics, etc. In this section, we investigate the importance of contextual information such as word order and word identity. Unlike prior work that studies LSTM embeddings at the sentence level, we look at both nearby and faraway context, and analyze how the language model treats contextual information presented in different regions of the context. </p><figure data-equation="" data-image="9" data-figure-category="figure" data-caption="" id="F91087711" data-image-src="/media/images/d12c851d-dad3-4393-b00d-a293374662ff.jpg"><div><img src="d12c851d-dad3-4393-b00d-a293374662ff.jpg"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure><figure data-equation="" data-image="10" data-figure-category="figure" data-caption="" id="F8094741" data-image-src="/media/images/7f1b5bd6-9ae2-441e-ab9c-68f004192726.jpg"><div><img src="7f1b5bd6-9ae2-441e-ab9c-68f004192726.jpg"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 2: </span></figcaption></figure></div>
                    	</section><section id="5.1Doeswordordermatter?" inlist="" resource="#5.1Doeswordordermatter?">
                            <h3 property="schema:name">5.1 Does word order matter? </h3>
                        <p>Adi et al. (2017) have shown that LSTMs are aware of word order within a sentence. We investigate whether LSTM language models are sensitive to word order within a larger context window. To determine the range in which word order affects model performance, we permute substrings in the context to observe their effect on dev loss compared to the unperturbed baseline. In particular, we perturb the context as follows, δpermute(wt−1, . . . , wt−n) = (wt−1, .., ρ(wt−s1−1, .., wt−s2 ), .., wt−n) (2) where ρ ∈ {shuffle,reverse} and (s1, s2] denotes the range of the substring to be permuted. We refer to this substring as the permutable span. For 1 5 10 15 20 30 50 100 200 Distance of perturbation from target (number of tokens) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Increase in Loss Shuffle 20 token windows PTB Reverse 20 token windows PTB Shuffle 20 token windows Wiki Reverse 20 token windows Wiki (a) Perturb order locally, within 20 tokens of each point. 1 5 10 15 20 30 50 100 200 Distance of perturbation from target (number of tokens) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Increase in Loss Shuffle entire context Reverse entire context Replace context with random sequence (b) Perturb global order, i.e. all tokens in the context before a given point, in Wiki. Figure 2: Effects of shuffling and reversing the order of words in 300 tokens of context, relative to an unperturbed baseline. All curves are averages from three random seeds, where error bars represent the standard deviation. (a) Changing the order of words within a 20-token window has negligible effect on the loss after the first 20 tokens. (b) Changing the global order of words within the context does not affect loss beyond 50 tokens. the following analysis, we distinguish local word order, within 20-token permutable spans which are the length of an average sentence, from global word order, which extends beyond local spans to include all the farthest tokens in the history. We consider selecting permutable spans within a context of n = 300 tokens, which is greater than the effective context size. </p><p><strong>Local word order only matters for the most recent 20 tokens</strong>. <span class="comment ref do" data-id="841546297" rel="schema:hasPart" typeof="dctypes:Text" resource="r-841546297"><mark id="841546297" property="schema:description">
                        We can locate the region of context beyond which the local word order has no relevance, by permuting word order locally at various points within the context.
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#841546297" resource="http://localhost:8000/document/8//comment-841546297">
       		              💬
                    </a>
                </sup></span> We accomplish this by varying s1 and setting s2 = s1 + 20. Figure 2a shows that local word order matters very much within the most recent 20 tokens, and far less beyond that. </p><p><strong>Global order of words only matters for the most recent 50 tokens</strong>. Similar to the local word order experiment, we locate the point beyond which the general location of words within the context is irrelevant, by permuting global word order. We achieve this by varying s1 and fixing s2 = n. Figure 2b demonstrates that after 50 tokens, shuffling or reversing the remaining words in the context has no effect on the model performance. In order to determine whether this is due to insensitivity to word order or whether the language model is simply not sensitive to any changes in the long-range context, we further replace words in the permutable span with a randomly sampled sequence of the same length from the training set. The gap between the permutation and replacement curves in Figure 2b illustrates that the identity of words in the far away context is still relevant, and only the order of the words is not. </p><p><strong>Discussion</strong>. These results suggest that word order matters only within the most recent sentence, beyond which the order of sentences matters for 2-3 sentences (determined by our experiments on global word order). After 50 tokens, word order has almost no effect, but the identity of those words is still relevant, suggesting a high-level, rough semantic representation for these faraway words. In light of these observations, we define 50 tokens as the boundary between nearby and longrange context, for the rest of this study. Next, we investigate the importance of different word types in the different regions of context. </p><aside class="note do"><blockquote cite="841546297"><article id="841546297" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/8/#841546297"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/8/#841546297">
            <time datetime="1540460904487" datatype="xsd:dateTime" property="schema:datePublished" content="1540460904487">
                1540460904487
            </time>
        </a>
    </dd>
</dl>
<section id="comment-841546297" rel="oa:hasBody" resource="i:#comment-841546297">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-841546297" typeof="oa:TextualBody">
        A few more to go
    </div>
</section>

<br/><br/></article></blockquote></aside></section><section id="5.2Typesofwordsandtheregionofcontext" inlist="" resource="#5.2Typesofwordsandtheregionofcontext">
                            <h3 property="schema:name">5.2 Types of words and the region of context</h3>
                        <p> Open-class or content words such as nouns, verbs, adjectives and adverbs, contribute more to the semantic context of natural language than function words such as determiners and prepositions. Given our observation that the language model represents long-range context as a rough semantic representation, a natural question to ask is how important are function words in the long-range</p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Hemsworth, 2011)</span></p></section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Hemsworth (Ed.). (2011). Guardians of the Galaxy.</cite></li>
</ol>
			            </div>
                    </section><script>jQuery( document ).ready(function() {
    			        jQuery(this).find('span.comment').each(function () {
                            var id=jQuery(this).attr('data-id');
                            var top=jQuery(this).offset().top - 40;
                            jQuery(document).find('article[id="'+id+'"]').each(function () {
                                jQuery(this).css('top',top);
                            });
                        });
                    });</script>
			    </div>
			</article>
		</main>
	</body>
</html>