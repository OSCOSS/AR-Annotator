<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>Decoupling Strategy and Generation in Negotiation Dialogues</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">Decoupling Strategy and Generation in Negotiation Dialogues</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="HeHeEmail:hehe@cs.stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#HeHeEmail:hehe@cs.stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            He He <i>Email: hehe@cs.stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="DerekChenEmail:derekchen14@cs.stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#DerekChenEmail:derekchen14@cs.stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Derek Chen <i>Email: derekchen14@cs.stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="AnushaBalakrishnanEmail:anusha@cs.stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#AnushaBalakrishnanEmail:anusha@cs.stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Anusha Balakrishnan <i>Email: anusha@cs.stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="PercyLiangEmail:pliang@cs.stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#PercyLiangEmail:pliang@cs.stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Percy Liang <i>Email: pliang@cs.stanford.edu</i> (Stanford University)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing $50) and the execution of that strategy (e.g., generating ‚ÄúThe bike is brand new. Selling for just $50!‚Äù). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches. </p></section></p><div class="article-part article-body"><section id="1Introduction" inlist="" rel="schema:hasPart" resource="#1Introduction">
                            <h2 property="schema:name">1 Introduction </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1Introduction" typeof="deo:Introduction">
			                <p>A good negotiator needs to decide on the strategy for achieving a certain goal (e.g., proposing $6000) and the realization of that strategy via generation of natural language (e.g., ‚ÄúI really need a car so I can go to work, but all I have is 6000, any more and I won‚Äôt be able to feed my children.‚Äù). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuayahuitl et al. ¬¥ , 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent‚Äôs goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016).<span class="comment ref do" data-id="3379904875" rel="schema:hasPart" typeof="dctypes:Text" resource="r-3379904875"><mark id="3379904875" property="schema:description">
                         To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achieving a fair deal) with the same language generator.
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#3379904875" resource="http://localhost:8000/document/13//comment-3379904875">
       		              üí¨
                    </a>
                </sup></span> Our framework consists of three components shown in Figure 1: First, the parser identifies keywords and entities to map each utterance to a coarse dialogue act capturing the highlevel strategic move. Then, the dialogue manager chooses a responding dialogue act based on a sequence-to-sequence model over coarse dialogue acts learned from parsed training dialogues. Finally, the generator produces an utterance given the dialogue act and the utterance history. Our framework follows that of traditional goaloriented dialogue systems (Young et al., 2013), with one important difference: coarse dialogue acts are not intended to and cannot capture the full meaning of an utterance. As negotiation dialogues are fairly open-ended, the generator needs to depend on the full utterance history. For example, consider the first turn in Figure 1. We cannot generate a response given only the dialogue act inform; we must also look at the previous question. However, we still optimize the dialogue manager in the coarse dialogue act space using supervised learning, reinforcement learning, or domain-specific knowledge. Existing human-human negotiation datasets are grounded in closed-domain games with a fixed set of objects such as Settlers of Catan (lumber, coal, brick, wheat, and sheep) (Afantenos et al., 2012; Asher et al., 2016) or item division (book, hat, and ball) (DeVault et al., 2015; Lewis et al., 2017). These objects lack the richness of the real world. To study human negotiation in more open-ended settings that involve real goods, we scraped postings of items for sale from craigslist.org as our negotiation scenario. By hiring workers on Amazon Mechanical Turk (AMT) to play the role of buyers and sellers, we collected a new dataset (CRAIGSLISTBARGAIN) of negotiation dialogues.1 Compared to existing datasets, our more realistic scenario invites richer negotiation behavior involving open-ended aspects such as cheap talk or side offers. We evaluate two families of systems modeling coarse dialogue acts and words respectively, which are optimized by supervised learning, reinforcement learning, or domain knowledge. Each system is evaluated on our new CRAIGSLISTBARGAIN dataset and the DEALORNODEAL dataset of Lewis et al. (2017) by asking AMT workers to chat with the system in an A/B testing setting. We focus on two metrics: task-specific scores (e.g., utility) and human-likeness. We show that reinforcement learning on coarse dialogue acts avoids</p><p>degenerate solutions, which was a problem in Li et al. (2016); Lewis et al. (2017). Our modular model maintains reasonable human-like behavior while still optimizes the objective. Furthermore, we find that models trained over coarse dialogue acts are stronger negotiators (even with only supervised learning) and produce more diverse utterances than models trained over words. Finally, the interpretability of coarse dialogue acts allows system developers to combine the learned dialogue policy with hand-coded rules, thus imposing stronger control over the desired strategy. </p></div>
                    	<aside class="note do"><blockquote cite="3379904875"><article id="3379904875" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/13/#3379904875"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/13/#3379904875">
            <time datetime="1540461346830" datatype="xsd:dateTime" property="schema:datePublished" content="1540461346830">
                1540461346830
            </time>
        </a>
    </dd>
</dl>
<section id="comment-3379904875" rel="oa:hasBody" resource="i:#comment-3379904875">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-3379904875" typeof="oa:TextualBody">
        Just 3  more after this
    </div>
</section>

<br/><br/></article></blockquote></aside></section><section id="2CraigslistNegotiationDataset" inlist="" rel="schema:hasPart" resource="#2CraigslistNegotiationDataset">
                            <h2 property="schema:name">2 Craigslist Negotiation Dataset </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2CraigslistNegotiationDataset" typeof="">
			                <p>Previous negotiation datasets were collected in the context of games. For example, Asher et al. (2016) collected chat logs from online Settlers of Catan. Lewis et al. (2017) asked two people to divide a set of hats, books, and balls. While such games are convenient for grounding and evaluation, it restricts the dialogue domain and the richness of the language. Most utterances are direct offers such as ‚Äúhas anyone got wood for me?‚Äù and ‚ÄúI want the ball.‚Äù, whereas real-world negotiation would involve more information gathering and persuasion. To encourage more open-ended, realistic negotiation, we propose the CRAIGSLISTBARGAIN task. Two agents are assigned the role of a buyer and a seller; they are asked to negotiate the price of an item for sale on Craigslist given a description and photos. As with the real platform, the listing price is shown to both agents. We additionally suggest a private price to the buyer as a target. Agents chat freely in alternating turns. Either agent can enter an offer price at any time, which can be accepted or rejected by the partner. Agents also have the option to quit, in which case the task is completed with no agreement. To generate the negotiation scenarios, we scraped postings on sfbay.craigslist.org from the 6 most popular categories (housing, furniture, cars, bikes, phones, and electronics). Each posting produces three scenarios with the buyer‚Äôs target prices at 0.5x, 0.7x and 0.9x of the listing price. Statistics of the scenarios are shown in Table 2. We collected 6682 human-human dialogues on AMT using the interface shown in Appendix A Figure 2. The dataset statistics in Table 3 show that CRAIGSLISTBARGAIN has longer dialogues and more diverse utterances compared to prior datasets. Furthermore, workers were encouraged to embellish the item and negotiate side offers such as free delivery or pick-up. This highly relatable scenario leads to richer dialogues such as the one shown in Table 1. We also observed various persuasion techniques listed in Table 4 such as embellishment, side offers, and appeals to sympathy. </p></div>
                    	</section><section id="3Approach" inlist="" rel="schema:hasPart" resource="#3Approach">
                            <h2 property="schema:name">3 Approach </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#3Approach" typeof="deo:ProblemStatement">
			                </div>
                    	</section><section id="3.1Motivation" inlist="" resource="#3.1Motivation">
                            <h3 property="schema:name">3.1 Motivation </h3>
                        <p>While end-to-end neural models have made promising progress in dialogue systems (Wen et al., 2017a; Dhingra et al., 2017), we find they struggle to simultaneously learn the strategy and the rich utterances necessary to succeed in the CRAIGSLISTBARGAIN domain, e.g., Table 8(a) shows a typical dialogue between a human and a sequence-to-sequence-based bot, where the bot easily agrees. We wish to now separate negotiation strategy and language generation. Suppose the buyer says: ‚ÄúAll right. Well I think 275 is a little high for a 10 year old TV. Can you lower the price some? How about 150?‚Äù We can capture the highest-order bit with a coarse dialogue act propose(price=150). Then, to generate the seller‚Äôs response, the agent can first focus on this coarse dialogue act rather than having to ingest the freeform text all at once. Once a counter price is decided, the rest is open-ended justification for the proposed price, e.g., emphasizing the quality of the TV despite its age. Motivated by these observations, we now describe a modular framework that extracts coarse dialogue acts from utterances, learns to optimize strategy in the dialogue act space, and uses retrieval to fill in the open-ended parts conditioned on the full dialogue history. </p></section><section id="3.2Overview" inlist="" resource="#3.2Overview">
                            <h3 property="schema:name">3.2 Overview </h3>
                        <p>Our goal is to build a dialogue agent that takes the dialogue history, i.e. a sequence of utterances x1, . . . , xt‚àí1 along with the dialogue scenario c (e.g., item description), and produces a distribution over the responding utterance xt . For each utterance xt (e.g., ‚ÄúI am willing to pay $15‚Äù), we define a coarse dialogue act zt (e.g., propose(price=15)); the coarse dialogue act serves as a logical skeleton which does not attempt to capture the full semantics of the utterance. Following the strategy of traditional goal-oriented dialogue systems (Young et al., 2013), we broadly define our model in terms of the following three modules: </p><ol><li><p>A <strong>parser</strong> that (deterministically) maps an input utterance xt‚àí1 into a coarse dialogue act zt‚àí1 given the dialogue history x restriction is the manager examining the dialogue acts, which we show will reduce the risk of degeneracy during reinforcement learning Section 4.4. We now describe each module in detail (Figure 1). </p></li><li><p>A <strong>manager</strong> that predicts the responding dialogue act zt given past coarse dialogue acts z</p></li><li><p>. A <strong>generator</strong> that turns the coarse dialogue act zt to a natural language response xt given the full dialogue history x</p></li></ol></section><section id="3.3Parser" inlist="" resource="#3.3Parser">
                            <h3 property="schema:name">3.3 Parser </h3>
                        <p>Our framework is centered around the coarse dialogue act z, which consists of an intent and a set of arguments. For example, ‚ÄúI am willing to pay $15‚Äù is mapped to propose(price=15). The fact that our coarse dialogue acts do not intend to capture the full semantics of a sentence allows us to use a simple rule-based parser. It detects the intent and its arguments by regular expression matching and a few if-then rules. Our parser starts by detecting entities (e.g., prices, objects) and matching keyword patterns (e.g., ‚Äúgo lower‚Äù). These signals are checked against an ordered list of rules, where we choose the first matched intent in the case of multiple matches. An unknown act is output if no rule is triggered. The list of intent parsing rules used are shown in Table 5. Please refer to Appendix B for argument parsing based on entity detection. </p></section><section id="3.4Manager" inlist="" resource="#3.4Manager">
                            <h3 property="schema:name">3.4 Manager </h3>
                        <p>The dialogue manager decides what action zt the dialogue agent should take at each time step t given the sequence of past coarse dialogue acts by maximizing the likelihood of the training data. We use a standard sequence-to-sequence model with attention. Each coarse dialogue act is represented as a sequence of tokens, i.e. an intent followed by each of its arguments, e.g., ‚Äúoffer 150‚Äù. During the agent‚Äôs listening turn, an LSTM encodes the received coarse dialogue act; during its speaking turn, another LSTM decodes the tokens in the coarse dialogue act. The hidden states are carried over the entire dialogue to provide full history. The vocabulary of coarse dialogue acts is much smaller than the word vocabulary. For example, our implementation includes fewer than 10 intents and argument values are normalized and binned (see Section 4.2). </p><p><strong>Reinforcement learning</strong>. Supervised learning aims to mimic the average human behavior, but sometimes we want to directly optimize for a particular dialogue goal. In reinforcement learning, we define a reward R(z1:T ) on the entire sequence of coarse dialogue acts. Specifically, we experiment with three reward functions: ‚Ä¢ Utility is the objective of a self-interested agent. For CRAIGSLISTBARGAIN, we set the utility function to be a linear function of the final price, such that the buyer has a utility of 1 at their target price, the seller has a utility of 1 at the listing price, and both agents have a utility of zero at the midpoint of the listing price and the buyer‚Äôs target price, making it a zero-sum game. For DEALORNODEAL, utility is the total value of objects given to the agent. ‚Ä¢ Fairness aims to achieve equal outcome for both agents, i.e. the difference between two agents‚Äô utilities. ‚Ä¢ Length is the number of utterances in a dialogue, thus encourages agents to chat as long as possible. The reward is ‚àí1 if no agreement is reached. We use policy gradient (Williams, 1992) for optimization. Given a sampled trajectory z1:T and the final reward r, let ai be the i-th generated token (i.e. ‚Äúaction‚Äù taken by the policy) along the trajectory. We update the parameters Œ∏ by Œ∏ ‚Üê Œ∏ ‚àí Œ∑ X i ‚àáŒ∏ log pŒ∏(ai | a).</p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Afantenos, Asher, Benamara, &amp; Cadilhac, 2012)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:3034136047}]">(Asher &amp; Hunter, 2016)</span></p></section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Afantenos, Asher, Benamara, &amp; Cadilhac (Eds.). (2012). Modelling strategic conversation: Model, annotation design and corpus.</cite></li>
  <li><cite>Asher, &amp; Hunter (Eds.). (2016). Discourse structure and dialogue acts in multiparty dialogue: the STAC corpus.</cite></li>
</ol>
			            </div>
                    </section><script>jQuery( document ).ready(function() {
    			        jQuery(this).find('span.comment').each(function () {
                            var id=jQuery(this).attr('data-id');
                            var top=jQuery(this).offset().top - 40;
                            jQuery(document).find('article[id="'+id+'"]').each(function () {
                                jQuery(this).css('top',top);
                            });
                        });
                    });</script>
			    </div>
			</article>
		</main>
	</body>
</html>