<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>An Information-Theoretic Explanation of Adjective Ordering Preferences</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">An Information-Theoretic Explanation of Adjective Ordering Preferences</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="MichaelHahnEmail:mhahn2@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#MichaelHahnEmail:mhahn2@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Michael Hahn <i>Email: mhahn2@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="JudithDegenEmail:jdegen@stanford.edu(StanfordUniversity,MIT)" rel="bibo:authorList" resource="#JudithDegenEmail:jdegen@stanford.edu(StanfordUniversity,MIT)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Judith  Degen <i>Email: jdegen@stanford.edu</i> (Stanford University, MIT)
                        </span>
                    </dd><dd id="RichardFutrellEmail:futrell@mit.edu(MIT)" rel="bibo:authorList" resource="#RichardFutrellEmail:futrell@mit.edu(MIT)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Richard  Futrell <i>Email: futrell@mit.edu</i> (MIT)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p> Across languages, adjectives are subject to ordering restrictions. Recent research shows that these are predicted by adjective subjectivity, but the question remains open why this is the case. We first conduct a corpus study and not only replicate the subjectivity effect, but also find a previously undocumented effect of mutual information between adjectives and nouns. We then describe a rational model of adjective use in which listeners explicitly reason about judgments made by different speakers, formalizing the notion of subjectivity as agreement between speakers. We show that, once incremental processing is combined with memory limitations, our model predicts effects both of subjectivity and mutual information. We con- firm the adequacy of our model by evaluating it on corpus data, finding that it correctly predicts ordering in unseen data with an accuracy of 96.2 %. This suggests that adjective ordering can be explained by general principles of human communication and language processing.</p></section></p><div class="article-part article-body"><section id="Introduction" inlist="" rel="schema:hasPart" resource="#Introduction">
                            <h2 property="schema:name">Introduction </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#Introduction" typeof="deo:Introduction">
			                <p>Across languages, sequences of modifying adjectives show preferences for some orderings over others. In English, ‘large wooden table’ is preferred to ‘wooden large table’, and ‘beautiful green shirt’ is preferred to ‘green beautiful shirt’. Such preferences exist across geographically and typologically diverse languages (Dixon, 1982; Sproat &amp; Shih, 1991). A variety of explanations for these preferences have been offered in the literature, including both semantic and syntactic ones. Syntactic accounts assume a rigid syntactic ordering of projections hosting different kinds of adjectives (Scott, 2002; Cinque, 2010). Semantic accounts have appealed to notions such as specificity (Ziff, 1960), inherentness (Whorf, 1945), absoluteness (Sproat &amp; Shih, 1991), conceptformability (Svenonius, 2008), and subjectivity (Hetzron, 1978; Hill, 2012; Scontras, Degen, &amp; Goodman, 2017). While not all of these hypotheses have been verified on a broader empirical basis, there is strong empirical support for the idea that adjective subjectivity determines ordering: Scontras et al. (2017) compared order preferences with ratings of subjectivity for individual adjectives in English, and showed that subjectivity explained over 60 % of the variance in order preference ratings. They found that more subjective adjectives tend to occur before less subjective ones. If these preferences occurred in only a few languages, it would be reasonable to accept this as an arbitrary fact of grammar. But the cross-linguistic stability of the patterns calls for a general explanation: As they occur in languages with widely different grammatical structures, we can expect that such an explanation will make reference to general principles of human communication and cognition. The aim of this paper is to present such an explanation. We first describe a corpus analysis, demonstrating effects of both subjectivity and mutual information on adjective ordering. We then provide an explanatory model of rational adjective use that predicts these effects, and verify that it correctly accounts for the corpus data. </p></div>
                    	</section><section id="CorpusAnalysis:SubjectivityandMutualInformationEffects" inlist="" rel="schema:hasPart" resource="#CorpusAnalysis:SubjectivityandMutualInformationEffects">
                            <h2 property="schema:name">Corpus Analysis: Subjectivity and Mutual Information Effects</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#CorpusAnalysis:SubjectivityandMutualInformationEffects" typeof="deo:Discussion">
			                <p> While previous hypotheses about adjective ordering such as ‘specificity’ and ‘inherentness’ of adjectives to nouns (Ziff, 1960; Whorf, 1945)) suggest that adjective ordering should depend on the noun, Scontras et al. (2017) found no evidence for noun-specific effects. As their study used selected out-ofcontext noun phrases, one might wonder whether such effects can be shown using corpus data. As a formalization of speci- ficity, we consider<em> Pointwise Mutual Information</em>: </p><p>PMI(Adj,Noun) = logP(Adj|Noun)−logP(Adj) (1) </p><p>where P(Adj|Noun) is the probability that the adjective Adj occurs, given the noun Noun. This concept is a common measure of collocation (Manning &amp; Schuetze, 1999), and measures the degree to which the two words appear together more frequently than would be expected by chance. If an adjective is specific to a noun, we expect the adjective to appear more frequently with the noun than with most other nouns, which is captured by PMI. Following the specificity theory, our hypothesis is that adjectives with higher mutual information with the noun tend to come closer to the noun. Indeed, words with high mutual information occur closer together in language (Qian &amp; Jaeger, 2012; Gildea &amp; Jaeger, 2015). </p><p><strong>Methods and Results </strong>We used the BookCorpus (Zhu et al., 2015), a corpus of 11,038 English novels, encompassing about 74 Million sentences.1 We estimated mutual information between adjectives and nouns from a randomly selected set of sentences, amounting to about 70 % of the corpus. The conditional probabilities P(Noun|Adj) are determined by counting all occurrences where Noun occurred directly after Adj. However, these counts will be impacted by the existing adjective ordering preferences, creating a potential confound. To eliminate this confound, we randomized the order of adjectives occurring in a sequence when counting occurrences. We then extracted all occurrences of two adjectives between a determiner and a noun from a held-out section amounting to 10 % of the corpus. We retained those occurrences where both adjectives occurred in the experiment of 1A reviewer points out that BookCorpus contains duplicate novels, which might result in imprecise MI estimates. Future work should confirm our results on further datasets. β SE z p PMI A1 – N −0.501 0.041 −12.2 &lt; 2.2 · 10−16 PMI A2 – N 0.501 0.041 12.2 &lt; 2.2 · 10−16 Subjectivity A1 8.28 1.35 6.12 9.36 · 10−10 Subjectivity A2 −8.28 1.35 −6.12 9.36 · 10−10 Table 1: Logistic mixed-effects model predicting whether two given adjectives A1,A2 were ordered as A1A2 (coded +1) or A2A1 (coded 0), from mutual information and subjectivity. Scontras et al. (2017), in order to use their experimentally measured subjectivities. 4699 datapoints remained. For each corpus, we ran a logistic mixed-effects model predicting the order of each pair of adjectives, including as fixed effects (1) subjectivity of the two adjectives from the data collected by Scontras et al. (2017), (2) mutual information between the noun and each of the two adjectives. The two adjectives were entered as random intercepts. The resulting models are shown in Table 1. We observed main effects of both mutual information and subjectivity, such that more objective adjectives and adjectives with higher mutual information with the noun occurred closer to it. Model comparison with a corresponding model without mutual information predictors (BIC 241, p &lt; 2.2·10−16) or without subjectivity predictors (BIC 120, p = 2.3 · 10−10) confirms that both types of predictors contribute independently. </p><figure data-equation="" data-image="7" data-figure-category="figure" data-caption="" id="F40105561" data-image-src="/media/images/fc6b87fb-b1d9-4a49-8c4c-e9ebb899443b.png"><div><img src="fc6b87fb-b1d9-4a49-8c4c-e9ebb899443b.png"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure></div>
                    	</section><section id="TheFunctionofSubjectiveAdjectives" inlist="" rel="schema:hasPart" resource="#TheFunctionofSubjectiveAdjectives">
                            <h2 property="schema:name">The Function of Subjective Adjectives </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#TheFunctionofSubjectiveAdjectives" typeof="">
			                <p>Our goal is a formal model of adjective ordering preferences that falls out of considerations of communicatively efficient adjective use. One route is by understanding adjectives as restricting a set of referents, picking out from contextually given objects denoted by a noun the one that matches the adjective, as done in much of the literature on content selection in referring expressions (Sedivy, Tanenhaus, Chambers, &amp; Carlson, 1999). However, establishing reference is not the only use of noun phrases, nor do all adjectives simply restrict a set of referents. Adjectives often are used non-restrictively, describing or commenting on some referent. As an example, consider ‘Forrest looks at the massive crowd.’ (Roth, 1993) – this does not mean that there were two crowds, and Forrest Gump chose to look at the one that was massive – instead, the sentence means that Forrest looked at the crowd, and the narrator (or Forrest) considered the crowd to be massive. In the literature, this use is known as non-restrictive use, as opposed to restrictive use that picks out one of the contextually given elements matching the noun. Our model will thus be centered not around restriction of reference, but around speakers communicating descriptions of and attitudes to referents. Scontras et al. (2017) et al. used two operationalizations of subjectivity: In their main experiment, they directly asked participants ‘how subjective’ a given adjective was. They validated this measure by another experiment in which they described two people disagreeing on a judgment, and asking whether both people could conceivably be right. These measures were highly correlated (r 2 = 0.91). The latter criterion is known as faultless disagreement: Adjectives are subjective if people can reasonably disagree, without anyone having to be in error (Kolbel, 2004). In line ¨ with the ‘faultless disagreement’ diagnostic, subjectivity is typically understood to refer to judgments whose truth is relative to individuals (Kolbel, 2004; Lasersohn, 2005). ¨ It seems, therefore, that the most natural way of modeling subjective meaning is by explicitly making reference to the opinions of different persons. In our model, we will assume that listeners infer not just properties of objects, but they infer and reason about judgments made by different speakers. </p><p><strong>A Model of Adjective</strong> </p><p>Use In this section, we describe a simple formal model of adjective use. Given that subjectivity essentially refers to the potential for disagreement across speakers, we will explicitly model judgments made by different speakers about objects. Judgments are objective if speakers tend to agree, while they are more subjective if speakers are less likely to agree. We formalize adjectives as expressing judgments A ∈ {green,beautiful,...}, made by a person s about a referent x.<span class="comment ref do" data-id="803692423" rel="schema:hasPart" typeof="dctypes:Text" resource="r-803692423"><mark id="803692423" property="schema:description">
                         In the case of highly objective adjectives, such as material adjectives, speakers will mostly agree on their judgments, while they may disagree for more subjective adjectives.
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#803692423" resource="http://localhost:8000/document/6//comment-803692423">
       		              💬
                    </a>
                </sup></span> The possible states of the worlds are truth-value assignments to the set of expressions {A(s, x) : A an adjective, x a referent,s a person} where A(s, x) indicates that person s judges referent x to have property A (e.g., green, beautiful, ...). We assume that there are fixed sets of persons, referents, and properties. This is illustrated in Figure 1, showing a typical world state: Two speakers mostly agree on more objective judgments, such as material and color, and agree less on more subjective judgments such as size or beauty. In our model, listeners aim to infer not just judgments of one speaker, but a full world state including multiple persons. This is useful when we consider that a listener might later interact with other persons. For instance, we expect that a listener learning that one of the persons in Figure 1 judges a referent to be ‘green’ will find it useful to infer that the other person likely applies the same judgment – that is, listeners will generalize objective judgments across people. </p><p><strong>World Prior and Inter-Speaker Agreement</strong> A world state is a truth value assignment to all the expressions A(s, x), across adjectives, persons, and objects. Speakers and listeners share probabilistic prior beliefs about which world states are more or less likely to be true, formalized by a prior distribution over world states. In our setting, adjectives differ in the correlation between judgments by different speakers about the same object. Formally, we assume that for each adjective A, there is a number κ(A) such that, under the prior METAL X GREEN X X LARGE X X BEAUTIFUL X X METAL X GREEN X X LARGE X X BEAUTIFUL X Figure 1: A typical world state: Speakers are likely to agree on more objective judgments, and less likely to agree on more subjective judgments. over world states, the Pearson correlation between the truth values of A(s, x) and A(s 0 , x) is equal to κ(A), whenever s,s 0 are two different speakers. In the special setting where there are two persons s1,s2, this reduces to two Bernoulli variables with fixed means and correlation, and we can write A(s1, x) ∼ Bernoulli(φ) A(s2, x) ∼ Bernoulli((1−κ(A))· φ+κ(A)·A(s1, x)) (2) with φ ∈ [0,1]. The magnitude of κ(A) formalizes correlation of judgments across speakers: Adjectives that show agreement across speakers have κ(A) close to 1. For more subjective adjectives, κ(A) is smaller. </p><p><strong>Communication: Rational Listeners and Speakers</strong> In our model, speakers aim to communicate judgments about objects by uttering three-word phrases consisting of two adjectives and a noun. An utterance A1A2N is true for the speaker in a world if the speaker judges that the adjectives A1,A2 both apply to the referent of the noun. That is, the truth value depends on those parts of the world state that relate to the speaker, but not on those that relate to other persons. In this model, we assume that there is a known mapping from nouns to entities, though this assumption can be relaxed. Our model is couched in the framework of Bayesian models of communication (Franke, 2010; Frank &amp; Goodman, 2012; Goodman &amp; Frank, 2016), consisting of a literal listener and a speaker reasoning about the listener. We will start with a listener who hears an utterance, and incrementally updates her belief about the world. While incrementality will not be necessary for deriving the core prediction of our model, we want to make explicit how the model fits with the known psycholinguistic fact that adjectives are processed incrementally (Sedivy et al., 1999). When hearing a sequence A1A2N, the listener maintains a buffer of the words heard so far, and conditions her belief by restricting to those worlds compatible with possible continuations of the buffer: P 0 listener(w) := Pprior(w) P 1 listener(w) ∝ P 0 listener(w)· δ∃u=A1A 0 2 N0 : w|=su P 2 listener(w) ∝ P 1 listener(w)· δ∃u=A1A2N0 : w|=su P 3 listener(w) ∝ P 2 listener(w)· δw|=sA1A2N (3) where w |=s u is a shorthand for ‘the utterance u is true for the speaker in the world state w’, and δ... is 1 if the condition in the subscript is true, else 0. In Figure 2, we visualize the incremental updates for the utterance ‘big green tree’. When choosing which utterance u to utter, speakers trade off communicative utility U(u) and the cost of production C(u) using a softmax decision rule: Pspeaker(u) ∝ exp(α·(U(u)−β ·C(u))) (4) Here, α &gt; 0 indicates the degree of rationality, while β &gt; 0 trades off utility and cost. When the speaker has perfect knowledge of the world state, a natural choice for U(u) is the negative surprisal of the true world state under the posterior belief of the listener (Frank &amp; Goodman, 2012). In our case, the speaker does not have full information about the world, as she might not know the judgments made by other speakers. Therefore, we take for U(u) the expected negative posterior surprisal of the ground truth about the other speakers: This quantity is equal (up to a constant independent of u) to the negative KL divergence between the speaker’s belief and the listener’s posterior belief after hearing the utterance – a common utility function in rational models of language use (Goodman &amp; Stuhlmuller, 2013; Regier, Kemp, &amp; Kay, ¨ 2015): U(u) :=−KL(Pspeaker||Plistener(·|u)) =∑w Pspeaker(w)log Plistener(w|u) Pspeaker(w) (5) We assume that Pspeaker is equal to the prior conditioned on the ground truth judgments of the speaker. For the cost C(u), we take the surprisal of the utterance u = A1A2N according to a general language model – e.g., describing the statistics of a community’s language use. C(A1A2N) = −logP(A1A2N) (6) Unlike the utility function, this cost function is purely a property of the surface string A1A2N in the statistics of the language, without reference to meaning. We assume that the speaker also computes these probabilities incrementally word-by-word. We assume that the language model encodes no prior ordering preference – both orderings of an adjective pair will have the same probability and thus the same cost as long as this probability P(A1A2N) is evaluated exactly. </p></div>
                    	<aside class="note do"><blockquote cite="803692423"><article id="803692423" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/6/#803692423"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/6/#803692423">
            <time datetime="1540460761374" datatype="xsd:dateTime" property="schema:datePublished" content="1540460761374">
                1540460761374
            </time>
        </a>
    </dd>
</dl>
<section id="comment-803692423" rel="oa:hasBody" resource="i:#comment-803692423">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-803692423" typeof="oa:TextualBody">
        Ok then
    </div>
</section>

<br/><br/></article></blockquote></aside></section><section id="AddingNoise" inlist="" rel="schema:hasPart" resource="#AddingNoise">
                            <h2 property="schema:name">Adding Noise </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#AddingNoise" typeof="">
			                <p>So far, while sequences such as ‘beautiful green tree’ and ‘green beautiful tree’ result in different sequences of belief updates in the listener, the final result will be identical. Thus, both sequences so far have the same communicative utility. Similarly, in a setting where no prior order preferences are encoded in the language model, they have the same cost. We now show how processing and specifically memory limitations break this symmetry, predicting both subjectivity and mutual information effects. It has by now been established that, during language production and language comprehension, linguistic material further in the past becomes Timesteps T0 T1 T2 T3 New Input big green tree Buffer big big green big green tree Compatible big beautiful tree big beautiful tree big green tree big green tree continuations beautiful green car big green car big green car ... ... ... Speaker Other Speaker Other Speaker Other Speaker Other BEAUTIFUL BIG GREEN Figure 2: Simulated incremental inference in a listener hearing ‘big green tree’, about judgments made by the speaker and another person about two objects. The listener maintains a buffer of words received so far, and in each step, considers all possible continuations (top). The bottom part shows the listener’s incremental posterior belief about the world. For expository purposes, we assume a simple setting where there are two persons, two objects, and three properties, with κ(beautiful) = 0.3, κ(big) = 0.5, κ(green) = 0.95. The strength of the color in each cell indicates the listener’s degree of belief that the given person (column) would judge a given property (row) to apply the given object (column). In each step, the listener considers all world states that are compatible with potential continuations of the buffer, and accordingly updates her belief about the speaker’s judgments. To the extent that persons tend to agree about properties, the listener can infer that the other person likely has the same judgments. This effect is strong for the objective property (‘green’), and weak for the subjective property (‘big’). Timesteps T3 New Input tree Buffer ??? green tree Compatible beautiful green tree continuations big green tree Speaker Other BEAUTIFUL BIG GREEN Timesteps T3 New Input tree Buffer ??? big tree Compatible beautiful big tree continuations green big tree Speaker Other BEAUTIFUL BIG GREEN Figure 3: Simulated posterior listener belief if the first adjective is lost when the noun is reached, for input ‘big green tree’ (top) and ‘green big tree’ (bottom), in the same setting as Figure 2. If the objective adjective is retained (top), information generalizes across speakers. Retaining the subjective adjective (bottom) is less useful due to potential for disagreement between speakers. harder to access and integrate with new material. Classical families of examples is include dependency locality effects (Gibson, 1998) and models of cue retrieval in sentence processing (McElree, 2001; Lewis &amp; Vasishth, 2005). To formally integrate such memory limitations into our model, we follow Futrell and Levy (2017), assuming that during incremental processing, previous words in the input may be deleted stochastically. Crucially, the probability of a word being deleted increases as one goes further back in the sequence (Futrell &amp; Levy, 2017). In our model, there are two places where incremental processing can be affected by noise: the listener’s incremental belief updates, and the computation of cost. </p><p><strong>Noisy Belief Updates</strong> First, let us consider what happens when the listener’s buffer is affected by progressive noise. Let us consider the simple case where, at each step, at most the two last words were integrated: The belief updates after hearing the two adjectives are as before. When encountering the noun, the first adjective is the furthest away from the current input word, and – in this case – deleted from the buffer. When computing the posterior, only the last two words are available, and the listener considers the possible completions of the now incomplete buffer (compare Equation 3): Pb3 listener(w) ∝ P 2 listener(w)· δ∃A 0 1 :w|=sA 0 1 A2N (7) where, as before, w |=s u is a shorthand for ‘the utterance u is true for the speaker in the world state w’. As noise is stochastic, utility U(u) is now the expected KL-divergence, where the expectation is taken over the possible noise patterns. In Figure 3, we illustrate the listener’s state when reaching the noun, for the two possible orderings of the more subjec- tive adjective ‘big’ and the less subjective adjective ‘green’. Depending on which adjective was subject to deletion, the listener has different posterior beliefs not just about the speaker, but also about the other person: Due to the objective nature of ‘green’, integrating this adjective (top) provides information that generalizes across speakers. As loss is progressive, the first adjective is more likely to be lost when the noun is reached. Thus, placing the objective adjective closer to the noun is predicted to, on average, result in lower levels of uncertainty about the full state of the world. </p><p><strong>Noise in the Cost</strong> The second place that involves incremental computation and will thus be affected by progressive noise is the cost term. If the first adjective is lost when computing the conditional probability P(N|A1A2) of the noun in context, the calculation marginalizes out the first adjective and the resulting quantity will be P(N|A2). Thus, cost will be estimated as −logP(A1)P(A2|A1)P(N|A2) in this case. Using the defi- nition of PMI, we can write C(A1A2N)−C(A2A1N) = λ ·(PMI(A1,N)−PMI(A2,N)) (8) More generally, Futrell and Levy (2017) show that the estimated surprisal will be biased towards this value when loss is progressive. Thus, we predict that putting the adjective with higher PMI with the noun closer to it results in lower cost. </p></div>
                    	</section><section id="Simulations" inlist="" rel="schema:hasPart" resource="#Simulations">
                            <h2 property="schema:name">Simulations </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#Simulations" typeof="">
			                <p>We implemented the model in the probabilistic programming language WebPPL (Goodman &amp; Stuhlmuller, 2014). We con- ¨ structed contexts with 20 objects, four properties, and two persons (one speaker and one other person). For simplicity, we consider the case where only the first adjective is subject to loss, at loss rate λ ∈ [0,1]. For inference, we randomly sample 10,000 worlds from the world prior and compute the listener model by exact enumeration of these samples. We have described how the model predicts subjectivity and mutual information effects, but one might wonder how robust these effects are to changes in parameter values. We considered the predictions the model makes for different values of the inter-speaker correlations κ(A1),...,κ(Anad j), the loss probability λ, the rationality parameters α,β &gt; 0, and the prior probability φ in (2). We sampled α ∼ Γ(5,1), β ∼ Γ(5,1), λ ∼ Uniform(0,1), φ ∼ σ(N (0,0.5)). We first considered the setup where A1 is more subjective than A2 – that is, κ(A1) &lt; κ(A2), while taking PMI(A1,N) = PMI(A2,N). The correlations for other adjectives are uniformly random. We sampled 10,000 parameter settings subject to this constraint. For every single setting, we found U(A1A2N) &gt; U(A2A1N) – placing adjectives with higher inter-speaker correlation closer to the noun increased utility. In Figure 4, we plot utility difference as a function of κ(A1) − κ(A2). Utility difference is directly proportional to the difference in inter-speaker correlations. We then carried out the same with κ(A1) = κ(A2) and PMI(A1,N) &lt; PMI(A2,N) – that is, assuming that A2 is more Figure 4: Left: Utility Difference between Orderings, as a function of the difference between inter-speaker correlations κ(A1), κ(A2). Across parameter settings, putting the subjective adjective earlier results in higher utility. Right: Cost difference, as a function of PMI difference. Placing the adjective with higher PMI closer to the noun results in lower cost. Both plots show LOESS-smoothed means. predictive of the noun. In this case, as shown in Equation 8, difference in cost is proportional to difference in PMI (Figure 4). Thus, assuming noisy memory, both subjectivity and MI are predicted to affect order preferences of the speaker. </p></div>
                    	</section><section id="TestingagainstCorpusData" inlist="" rel="schema:hasPart" resource="#TestingagainstCorpusData">
                            <h2 property="schema:name">Testing against Corpus Data</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#TestingagainstCorpusData" typeof="">
			                <p> We tested the speaker model against the data from our corpus analysis described above. For the inter-speaker correlations κ(A), we took κ(A) to be one minus the average subjectivity score from Scontras et al. (2017). For the cost term, we used mutual information data from the corpus analysis. We used Bayesian data analysis to infer the numerical parameters of our model (rationality parameters α,β, loss rate λ, prior probabilty φ) from the BookCorpus data we used in the beginning. We specified priors α· λ ∼ N (0,10), α· β · λ ∼ N (0,10), φ ∼ σ(N (0,2)), where σ is the inverselogit function.2 To obtain approximate posterior distributions, we used variational inference with minibatches in Pyro (http://pyro.ai/). We obtained posterior means α · λ = 5.07 (σ 2 = 0.243), α·β·λ = 0.39 (σ 2 = 0.033), and φ = 0.095 (logit(φ) = −2.253, σ 2 = 0.13). The fitted values suggest that utility is weighted much more strongly than cost, and that most judgments are relatively unlikely a priori. Plugging in the posterior means for these parameters, the model achieves a classification accuracy of 93.7 % on the task of predicting adjective order on the dataset.3 A model with only the cost term would achieve an accuracy of 69 %, while a model with only the utility term achieves an accuracy of 93.3 %, very close to the accuracy of the full model.4 This highlights the central role of the utility term – and thus sub- 2Our model does not make it possible to obtain independent estimates of α,β and λ. 3Logistic regression models with surprisal and PMI predictors would achieve the same accuracy. However, note that our model is an explanatory cognitive model, as opposed to a data analysis. 4While the cost term does not contribute much in terms of accuracy, a mixed-effects analysis analogous to the corpus analysis above confirms that it contributes significantly (p &lt; 2.2 · 10−16). jectivity – for ordering. To test whether results generalize to unseen data, we used a further held-out 20 % of the corpus. Classification accuracy was 93.1 %. As the prior probability that an adjective is applied to objects might not be uniform, we also considered the setting where φ in (2) varies with the adjective. We assumed a hierarchical model with hyperparameters φ0 ∼ N (0,2), S 2 ∼ N (0,1), and parameters φ(A) ∼ σ(N (φ0,S 2 )) for each adjective A. We obtained similar estimates: α · λ = 5.6 (σ 2 = 0.25),α·β = 0.36 (σ 2 = 0.088), φ0 = −2.1 (σ 2 = 0.12), S 2 = 0.31 (σ 2 = 0.069). Classification accuracy increases to 97.3 % on the original dataset, and to 96.2 % on the heldout set, which shows that the improvement obtained from the increase in model complexity generalizes to unseen data. Future research should test the prediction that the inferred values for φ(A) correspond to the prior probability that a speaker would apply a given adjective to an object (Equation 2). </p></div>
                    	</section><section id="Discussion" inlist="" rel="schema:hasPart" resource="#Discussion">
                            <h2 property="schema:name">Discussion</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#Discussion" typeof="deo:Discussion">
			                <p> We provided an explanatory cognitive model of adjective ordering, building on the insight that subjectivity and specificity, formalized by mutual information, impact adjective ordering. We first conducted a corpus study and showed that ordering is impacted independently by subjectivity and mutual information. We then presented a model of adjective use in which listeners infer judgments made by speakers and other persons. We integrated this model with a recent model of memory limitations in language processing, and showed that it predicts both subjectivity and mutual information effects. We evaluated the model on corpus data, finding that it predicts adjective ordering with an accuracy of 96.2 %. In the following, we discuss some of the implications of this work, and questions that it raises. Research has shown that subjective material more generally tends to appear at the periphery of phrases and clauses, and that diachronic meaning change towards more subjective meanings correlates with movement to the periphery (Traugott, 2010). This is in line with our proposal: Our analysis should equally apply to other types of subjective material, predicting that memory limitations favor placing them further away from the head. Future research should test our model on other types of subjective content. We have assumed that the speaker’s communicative goal is communicating descriptions and attitudes, rather than establishing reference. This was motivated by the observation that adjectives are often not used for establishing reference. Future research should compare our account with accounts of adjective ordering preferences that rely on the assumption that adjectives are used primarily for reference resolution. In languages where adjectives follow the noun, such as Spanish or Arabic, typically the reverse order is observed (Dixon, 1982). Our account seems to make the correct prediction: In such languages, the noun is more likely to be lost when the second (subjective, in this case) adjective is reached. We furthermore make the prediction that, in such languages, adjectives with higher mutual information with the noun will also be more likely to come closer to the noun. Recently, Dye, Milin, Futrell, and Ramscar (2017) interpreted prenominal modifiers as smoothing entropy, making nouns more equally predictable, and speculated that this may account for adjective ordering preferences. A notable difference between this theory and ours is that theirs predicts major differences between ordering patterns of prenominal and postnominal adjectives, whereas ours is symmetric. In conclusion, the work reported here suggests that adjective ordering preferences are plausibly the result of efficiently trading off cost and informational utility of utterances for the purpose of communicating maximally generalizable information about objects. </p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Cinque, 2010)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:973027577}]">(Dixon, 1982)</span></p></div>
                    	</section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Cinque (Ed.). (2010). The syntax of adjectives. MIT Press.</cite></li>
  <li><cite>Dixon (Ed.). (1982). Where have all the adjectives gone? And other essays in semantics and syntax. Berlin: Mouton.</cite></li>
</ol>
			            </div>
                    </section><script>jQuery( document ).ready(function() {
    			        jQuery(this).find('span.comment').each(function () {
                            var id=jQuery(this).attr('data-id');
                            var top=jQuery(this).offset().top - 40;
                            jQuery(document).find('article[id="'+id+'"]').each(function () {
                                jQuery(this).css('top',top);
                            });
                        });
                    });</script>
			    </div>
			</article>
		</main>
	</body>
</html>