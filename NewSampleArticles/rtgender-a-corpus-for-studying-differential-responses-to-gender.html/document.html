<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>RtGender: A Corpus for Studying Differential Responses to Gender</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">RtGender: A Corpus for Studying Differential Responses to Gender</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="RobVoigtaEmail:robvoigt@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#RobVoigtaEmail:robvoigt@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Rob Voigta <i>Email: robvoigt@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="DavidJurgensbEmail:jurgens@umich.edu(UniversityofMichigan)" rel="bibo:authorList" resource="#DavidJurgensbEmail:jurgens@umich.edu(UniversityofMichigan)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            David Jurgensb <i>Email: jurgens@umich.edu</i> (University of Michigan)
                        </span>
                    </dd><dd id="VinodkumarPrabhakaranaEmail:vinodkpg@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#VinodkumarPrabhakaranaEmail:vinodkpg@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Vinodkumar Prabhakarana <i>Email: vinodkpg@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#DanJurafskyEmail:jurafsky@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Dan Jurafsky <i>Email: jurafsky@stanford.edu</i> (Stanford University)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>Like many social variables, gender pervasively influences how people communicate with one another. However, prior computational work has largely focused on linguistic gender difference and communications about gender, rather than communications directed to people of that gender, in part due to lack of data. Here, we fill a critical need by introducing a multi-genre corpus of more than 25M comments from five socially and topically diverse sources tagged for the gender of the addressee. Using these data, we describe pilot studies on how differential responses to gender can be measured and analyzed and present 30k annotations for the sentiment and relevance of these responses, showing that across our datasets responses to women are more likely to be emotive and about the speaker as an individual (rather than about the content being responded to). Our dataset enables studying socially important questions like gender bias, and has potential uses for downstream applications such as dialogue systems, gender detection or obfuscation, and debiasing language generation. Keywords: gender-annotated corpora, gender difference, gender bias, discourse, computational social science.</p></section></p><div class="article-part article-body"><section id="1.Introduction" inlist="" rel="schema:hasPart" resource="#1.Introduction">
                            <h2 property="schema:name">1. Introduction </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1.Introduction" typeof="deo:Introduction">
			                <p>Language is a means for the construction of identity and social categories like gender; social issues such as gender bias, in turn, often take form in language. Linguistic datasets have been used both to debunk gender-biased myths — for example, contrary to stereotype women are not actually more talkative than men (Mehl et al., 2007) — and to identify social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with comments given in response to the source texts. We collect such data from a variety of contexts, including: • Facebook (Politicians): Responses to Facebook posts from members of the U.S. House and Senate • Facebook (Public Figures): Responses to Facebook posts from other public figures, e.g., television hosts, journalists, and athletes • TED: Responses to presentations from TED speakers 1Throughout this paper we use the terms “woman” and “man” as labels for gender in preference to “female” and “male” since the latter terms are more commonly used as markers of sex. 2We use “source” to refer to the producer of the text being responded to (online posts and talk videos), and “responder” for the producer of the comment or response, regardless of its format. • Fitocracy: Responses to posts about fitness progress • Reddit: Responses to Reddit comments across a variety of subreddits These diverse datasets offer multiple perspectives on responses to gender. The first two sources (from Facebook and TED) represent the “broadcast” case, in which source texts (online posts and speech) from a small number of individuals (experts, authorities, and other public figures) receive a large number of responses which the source is unlikely to read and a discussion between the source and the responder is unlikely to continue. The second two (Fitocracy and Reddit) represent the “personal” case in which the responses are individualized, the source and responder may know one another and have an ongoing interaction afterwards. </p></div>
                    	</section><section id="2.ResponsestoGender" inlist="" rel="schema:hasPart" resource="#2.ResponsestoGender">
                            <h2 property="schema:name">2. Responses to Gender</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2.ResponsestoGender" typeof="">
			                <p> Here we aim to encourage research on responses to gender. Contrasting with language about or portraying a given gender which address abstract representations of social categories, responses to gender are directed towards an individual person. We know that social characteristics of the addressee influence linguistic behavior; existing computational work has shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 Dataset Source Individuals Source Text Count Response Count Response Word Count BROADCAST Facebook (Politicians) M: 306 W: 96 399,037 13,866,507 376,114,950 Facebook (Public Figures) M: 41 W: 64 117,811 10,667,500 123,753,913 TED Talks M: 1,071 W: 349 1,671 190,425 15,549,984 PERSONAL Fitocracy M: 52,432 W: 47,498 318,535 318,535 6,606,087 Reddit M: 19,010 W: 11,116 1,453,512 1,453,512 44,537,612 Table 1: Basic statistics about the subcorpora within RtGender. Jha and Mamidi, 2017). Nevertheless, biased responses to social categories like gender can lead to marginalization (Sue, 2010) and negatively impact a person’s self-esteem and ability through mechanisms such as stereotype threat (Spencer et al., 1999). Perhaps most related to our work, Fu et al. (2016) analyze questions directed at men and women tennis players, finding that questions directed at men tend to be more about the game while questions directed at women are more likely to stray to topics about their appearance and off-court relationships. Tsou et al. (2014) similarly find comments on TED talks are more likely to be about the presenter than the content if the presenter is a woman. In looking at responses to gender in our datasets (§3.), we note that some instances of gender bias may be overt, such as direct references to stereotypes (“Cool story babe, now make me a sandwich” - Facebook comment to television anchor Megyn Kelly) or inappropriate comments about physical appearances (“wow she is very sexy...”- TED comment to researcher Rachel Botsman); however, the larger problem is a subtle one in part because much of social bias is also not overt but rather implicit (Greenwald et al., 1985; Nosek et al., 2011). More commonly, many biases are exhibited through small but systematic differences in language which normally go unnoticed but, when viewed in aggregate, reveal large scale patterns in behavior towards a particular gender. In the next section, we present RtGender – a corpus of responses to gender, compiled according to the following desired characteristics. First, it would be sufficiently large to allow for uncovering the subtle type of differentiation and bias mentioned above. Second, it would cover multiple genres and linguistic contexts to facilitate generalizable results. Third, it would allow for content and topic in the source texts to be controlled as much as possible so that researchers could know people are responding to the same types of sources, especially given existing research demonstrating gender-correlated clustering behavior by topic (Argamon et al., 2003; Bamman et al., 2014). Fourth, it would contain source texts from both authority figures and everyday persons, to facilitate the analysis of such subtle phenomena as implicit bias towards women authority figures (Rudman and Kilianski, 2000), while allowing for comparison to non-authority figures. Finally, it would ideally have gender labels for both the sources and the responders, to allow for gender-interaction analysis of interesting psychological phenomena like the propagation of self-favorable gender stereotypes (Rudman et al., 2001). </p></div>
                    	</section><section id="3.RtGenderDatasets" inlist="" rel="schema:hasPart" resource="#3.RtGenderDatasets">
                            <h2 property="schema:name">3. RtGender Datasets </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#3.RtGenderDatasets" typeof="">
			                <p>We present five distinct datasets regarding responses to gender which fulfill many of the aforementioned desiderata. These data represent a variety of interactional contexts and relationships between the source and the responders. The Facebook and TED “broadcast” datasets presented here contain many instances of responses to people in positions of authority or renown (politicians, topic experts, television personalities), and so can be analyzed with prior knowledge about the power differential between the source and the responders. The Fitocracy and Reddit “personal” datasets will allow research to contrast responses to gender in the public domain with more one-on-one interactions. In these datasets having interactional dyads of postresponse also opens possibilities for studying normativity, for instance by asking whether comments on non-normative posts are more likely to exhibit elements of bias. </p></div>
                    	</section><section id="3.1.Facebook" inlist="" resource="#3.1.Facebook">
                            <h3 property="schema:name">3.1. Facebook </h3>
                        <p>Our largest dataset is comprised of top-level comments on Facebook posts from public figures, scraped from their public pages. We only include top-level comments (that is, comments directly responding to the post) to reduce the in- fluence of comment-internal discussion so each comment is a response directly to the original poster. Each post is associated with the page of its relevant public figure, and includes metadata such as whether the post was text-only or included an image, video, or link. The posts and responses in question are all public; however, to protect the anonymity of Facebook users in our dataset we remove all identifying user information as well as Facebook-internal information such as User IDs and Post IDs, replacing these with randomized ID numbers. Therefore users whose comments appear multiple times in our dataset may be compared, but without revealing their identity. We also only report commenter first names, since this is less identifying but still allows for running genderidentification algorithms. As a baseline for convenience we provide masculine/feminine ratios for these first names from Bergsma and Lin (2006). We collect posts and their associated top-level comments for the categories of speakers described below. In each case we find the page for the speaker with a novel method for finding gender-labeled speakers from Wikipedia. Specifically, our method takes as input a Wikipedia category page such as https://en.wikipedia.org/wiki/ Category:American_female_tennis_players, and for each name listed runs a search for public pages using Facebook’s Graph API. If an exact match for the name appears in the top three results, and the category of the page matches a relevant category (for instance, ”Public Figure” or ”Athlete” in the case of female tennis players), and their gender is listed, and the page is “verified” with Facebook, we accept it as a member of that category and scrape the 2815 Category Example Remarks on Appearance Hot presenter. Patronizing Tone Stick to actually talking about the tenets of the topic and defer the blah blah blah to the politicians alone.. Doubting Expert Knowledge I always thought the first approach to scientific study was to examine all evidence that dissproves an hypothesis. Self-promotion is Perceived Negatively After watching this, I know much more about Rachel Pike and what she does than the actual subject matter. Table 2: Examples of categories of comments displaying potential forms of gender bias from the TED dataset. These categories were primarily observed in comments to women presenters. relevant posts and comments. </p><p><strong>Politicians</strong>. This subset contains all posts and associated top-level comments for all 412 current members of the United States Senate and House who have public Facebook pages meeting the requirements outlined above. This dataset inherently includes a strong control for content, since members of Congress tend to be talking about the same sorts of topics; each Congressperson is also labeled with their party affiliation to further facilitate controlling for cross-party stylistic and topical differences. </p><p><strong>Public Figures</strong>. Beyond Congress, we consider other US public figures from the realms of journalism, fiction writing, television, film, and athletics. This subset contains posts and associated top-level comments for 105 such public figures, currently drawn from the following sets of Wikipedia categories: • American television news anchors, American television journalists, American television talk show hosts, Political analysts • American film actresses, American male film actors, American television actresses, American male television actors • American male tennis players, American female tennis players, Olympic track and field athletes of the United States • 21st-century American novelists </p></section><section id="3.2.TED" inlist="" resource="#3.2.TED">
                            <h3 property="schema:name">3.2. TED</h3>
                        <p> TED Talks are influential videos from experts on a variety of topics ranging from education, business, science, tech and creativity.3 The TED website also allows viewers to post comments in response to each video, which provides us an opportunity to study how these responses are impacted by the gender of the expert presenters. We include a dataset scraped from the TED website of 190,425 labeled for presenter gender. Gender labels were initially drawn from Mirkin et al. (2015) and the remaining labels were done manually. Talks that consisted solely of a dance or music performance, or talks presented by more than one speaker were excluded. This domain has been previously explored in NLP: we know that TED presenters are more commonly male, and videos of talks by male speakers are more viewed and liked on YouTube (Sugimoto et al., 2013). Furthermore, considering responses to gender, Tsou et al. (2014) note that commenters are more emotional when the presenter is a woman. However, existing sources of this data such as 3https://www.ted.com/talks https://www.idiap.ch/dataset/ted are not labeled for presenter gender. </p></section><section id="3.3.Fitocracy" inlist="" resource="#3.3.Fitocracy">
                            <h3 property="schema:name">3.3. Fitocracy </h3>
                        <p>Fitocracy is a social media fitness website in which users log and discuss their fitness-related activities. We include a dataset of 318,536 “status updates” and their corresponding top-level comments from Fitocracy which users have posted about their progress. We include only the first comment after a post because comments are not nested, so discussions can diverge as following comments may quickly become responses to previous comments; however, the first post is necessarily in direct response to the original post. Building upon the observations of Fu et al. (2016) on gender differences in questions posed to tennis players, we view Fitocracy as an ideal dataset for examining how gender stereotypes around fitness and sports play out in everyday interactions. In this dataset we have confident selfreported labels for the gender of most posters and commenters; over 91% of users of Fitocracy self-report both gender and age on their profile pages, and we include this information in the dataset. </p><h6 id="H3369081">3.4. Reddit </h6><p>Posts on Reddit are a common source of data for computational linguistic analysis; in this corpus, we include a dataset of 1,453,512 Reddit post-response pairs for which we know the gender of the source poster. The data was gathered by finding gender-indicating flairs used on different subreddits (e.g,. “male” on /r/AskMen). For each of these users, we then find all of their posts in other subreddits and collect the first response to each post - as in other contexts, we take only the first response to guarantee it is directed towards the source poster. We also tag the responder for gender when we have that data available, which occurs for about 9.2% of our examples. This dataset covers a wide variety of subreddits, so while the sources of our gender tags are from a relatively limited domain, ultimately researchers can control for content substantially by sampling the dataset in particular subreddits of interest. </p></section><section id="4.AnalysisandChallenges" inlist="" rel="schema:hasPart" resource="#4.AnalysisandChallenges">
                            <h2 property="schema:name">4. Analysis and Challenges </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#4.AnalysisandChallenges" typeof="deo:Discussion">
			                <p>In this section we discuss a preliminary qualitative and quantitative analysis regarding differential responses to gender in our new datasets, designed to illustrate the kind of studies they enable. 4https://www.fitocracy.com/ 2816 Source Gender WOMAN MAN Responder Gender WOMAN girl, gorgeous, can, if, yay, exercise, it, find, girlie, to, love, we, feel, ”, do, each, mama, site, or, yoga, walk, help, started, go, healthy HAPPY EMOJI, thank, thanks, ..., haha, ?, no, well, problem, :p, mister, !, pleasure, follow, props, prop, lol, welcome, :d, bomb, handsome, very, for, my, course MAN HAPPY EMOJI, !, you, welcome, thank, your, follow, great, pp, pleasure, hope, love, back, following, very, luck, you’re, girl, well, :d, are, awesome, thanks, for, young, beautiful, smile, hi, fun man, bro, mate, dude, ., brother, buddy, [NUMBER], brah, bench, ,, bud, shit, 0x0, yeah, i, squat, press, lifts, sets, fuck, gains, 0kg, chest, last, strength, week, guy, this, ohp Table 3: Top 30 words in comments in the Fitocracy dataset by log-odds based on the gender of the commenter and original poster. Source Gender Dataset Prediction Accuracy Facebook (Political) 63.9% Facebook (Public Figures) 80.3% TED Talks 80.5% Fitocracy 57.7% Reddit 53.5% Table 4: Cross-validation accuracy across contexts at predicting the gender of the source from the text of their post/talk. Table 2 presents some qualitative examples of TED comments directed towards women presenters that exhibit possible gender bias. Some are overt, such as remarks on appearance, but others are more subtle. For authors of each gender, Table 3 gives the top 30 words in Fitocracy responses most associated with the responder’s gender, computed using the weighted log-odds method of Monroe et al. (2008). The word preferences of responders show a substantial gender-correlated signal in this data. Men commenting on posts by men use many close terms of informal address (“bro,” “dude,” “brother,” “buddy”) and likewise for women commenting on posts by women (“girl,” “girlie,” “mama”). Cross-gender comments, however, are more emotive, with prominent use of emoticons, emoji, and exclamation points, as well as more playful and interactive language (talk of ”following” each other and use of second person pronouns) and discussion of the addressee’s appearance, e.g., “beautiful” (M→W) and “handsome” (W→M). Each dataset in turn presents a unique challenge for researchers. The Facebook data is large and noisy: the comments are relatively unmoderated and may also be responding to photos and URLs in the source posts, rather than just the textual content of the post itself. The TED talks exemplify the challenge of separating gender difference from topical choice, since selection bias on the part of the TED organizers means there are more talks from men and talks from women are more likely to be about gendered topics. For Fitocracy, as Table 3 shows, the language used is often very positive overall, so a computational definition of bias must be able to also capture benevolent differential treatment. The Reddit data covers a very broad spectrum of topical content, and in the majority of subreddits gendered flair is not visible so the signal for differential responses to gender is much more subtle. One important axis of variation across the linguistic environments of these contexts is to consider how differently men and women tend to speak in that context; a simple way to quantify this is to ask how well a predictive model can distinguish source posts written by men versus women. Table 3.3. shows ten-fold cross-validation accuracies of a simple unigram logistic regression model at predicting the gender of the source from the text in the source post. In the case of TED, accuracy is given at predicting gender from a sample of lines in the source transcript. Notice the wide diversity across contexts. While gender differences in the “personal” Reddit and Fitocracy contexts are relatively minimal, gender difference of the source is amplified in the “broadcast” contexts where posts by men and women are highly separable even with a simple unigram model. </p></div>
                    	</section><section id="5.RelevanceandSentimentAnnotations" inlist="" rel="schema:hasPart" resource="#5.RelevanceandSentimentAnnotations">
                            <h2 property="schema:name">5. Relevance and Sentiment Annotations </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#5.RelevanceandSentimentAnnotations" typeof="">
			                <p>Our pilot analyses revealed that the RtGender datasets have the potential to offer interesting insights on differential responses to gender across diverse domains. To expand the possible range of questions that may be asked of this data, we conducted a crowdsourced annotation effort on a sample of the responses across our datasets. Inspired by the annotation task for TED talk comments proposed by Tsou et al. (2014), we labeled over 15,000 postresponse pairs with annotations for the relevance of the response to the source and the sentiment of the response. For this task we asked crowd workers on Amazon Mechanical Turk to read a post-response pair and mark whether it was relevant to the CONTENT ONLY, POSTER ONLY, CONTENT AND POSTER, or if it was IRRELEVANT. In the case of comments on TED talks since there was no “original post” we provided a list of the talk’s keywords to help participants determine its relevance. We then asked about the sentiment of response (POSITIVE, NEGATIVE, MIXED, or NEUTRAL), regardless of its relevance. Our annotation interface is shown in Figure 1. Crowd workers were paid $0.20 for completing one run of 5 post-response pairs. To control for annotation quality, on each run for a random one of the pairs we replaced the response with a snippet of text with a known expected response. These were drawn from the following sources: • Random sentences from articles in the New York Times in 2007 (expected response: IRRELEVANT) 2817 Figure 1: Screenshot of our relevance and sentiment annotation interface. Dataset Annotated Examples Facebook (Politicians) 3,872 Facebook (Public Figures) 2,884 TED Talks 2,648 Fitocracy 2,900 Reddit 2,728 Table 5: Quantity of available post-response pairs labeled with relevance and sentiment annotations for each dataset. • Random phrases with known polarity from the Stanford Sentiment Treebank (Socher et al., 2013) (expected response: IRRELEVANT or POSITIVE/NEGATIVE, respectively) • Poster/speaker-directed utterances automatically generated with a heuristic method to have known polarity (expected response: relevant to POSTER ONLY or CONTENT+POSTER; known POSITIVE/NEGATIVE polarity as appropriate). Examples: – you are just fantastic, believe in yourself! (POSITIVE) – Stop trying. You are so garbage! (NEGATIVE) Any runs for which these control questions were answered incorrectly were discarded, constituting 11.9% of total runs. We performed basic analyses on these annotations to better understand the types of future research they might enable. Firstly, we ran a set of mixed-effects models predicting aspects of response relevance and sentiment, with gender as a fixed effect and dataset context as a random effect. Our overall results replicate Tsou et al. (2014), finding that in general responses to women were more likely to be about the source poster or speaker as an individual (b=0.20, p&lt;0.01) and were more emotive (having non-neutral sentiment) (b=0.17, p&lt;0.01) than responses to men. Interestingly, sentiment in responses to women was higher across the board; whether this represents “benevolent sexism” or genuine positive sentiment is an interesting and complex topic for future research. However, the contexts represented by each dataset acted very differently. When we restrict the above analyses to only the “personal” Fitocracy and Reddit contexts we find no gender-based differences in response relevance (b=0.01, p=0.87), and the magnitude of the emotiveness difference is greatly reduced (b=0.11, p=0.046). This finding suggests a potential powerful effect of social distance in exacerbating gender bias, in line with classic social psychological findings on how group diffusion of responsibility can lead to increased dehumanization (Bandura et al., 1975). Figure 2: Cross-context characteristics of the responses per the relevance and sentiment annotations in RtGender.</p><figure data-equation="" data-image="19" data-figure-category="figure" data-caption="" id="F12222281" data-image-src="/media/images/73a32ed1-7c39-4825-9f5c-15aba479a38e.png"><div><img src="73a32ed1-7c39-4825-9f5c-15aba479a38e.png"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure></div>
                    	</section><section id="6.Conclusion" inlist="" rel="schema:hasPart" resource="#6.Conclusion">
                            <h2 property="schema:name">6. Conclusion </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#6.Conclusion" typeof="deo:Conclusion">
			                <p>Gender is a performative social phenomenon in which individual behavior is often shaped – subtly, over a lifetime – by the responses to that behavior (Lakoff, 1973; Butler, 1990). To encourage computational study in this area, in this paper we presented five large datasets in a corpus called RtGender that capture differential responses to gender online in a variety of genres, contexts, and social roles of the interacting participants, and publicly available for research purposes.5 We labeled a sample of the responses in the corpus with annotations for relevance and sentiment, and gave some initial analyses of the data and resulting annotations. We found qualitative and quantitative evidence for gender bias in the responses, suggesting a need for future work in this area that we hope this corpus will facilitate. By studying responses to gender we can learn a great deal about the social construction of gender and other social categories in general. </p></div>
                    	</section><section id="Acknowledgments" inlist="" rel="schema:hasPart" resource="#Acknowledgments">
                            <h2 property="schema:name">Acknowledgments </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#Acknowledgments" typeof="">
			                <p><span class="comment ref do" data-id="3023898809" rel="schema:hasPart" typeof="dctypes:Text" resource="r-3023898809"><mark id="3023898809" property="schema:description">
                        This work was supported by the Stanford Data Science Initiative and the National Science Foundation through award IIS-1526745. The first author gratefully acknowledges the support of the Stanford Interdisciplinary Graduate Fellowship. 
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#3023898809" resource="http://localhost:8000/document/16//comment-3023898809">
       		              💬
                    </a>
                </sup></span></p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Bamman, Eisenstein, &amp; Schnoebelen, 2014)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:3177469615}]">(Bandura, Underwood, &amp; Fromson, 1975)</span></p></div>
                    	<aside class="note do"><blockquote cite="3023898809"><article id="3023898809" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/16/#3023898809"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/16/#3023898809">
            <time datetime="1540461448292" datatype="xsd:dateTime" property="schema:datePublished" content="1540461448292">
                1540461448292
            </time>
        </a>
    </dd>
</dl>
<section id="comment-3023898809" rel="oa:hasBody" resource="i:#comment-3023898809">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-3023898809" typeof="oa:TextualBody">
        It's over! :)
    </div>
</section>
<h3 property="schema:name" style="display:none">
        Answers</h3>
    <dl class="author-name">
        <dt>Authors</dt>
        <dd>
            <span rel="schema:creator">
            <span about="userURI#1" typeof="schema:Person">
            <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
            <a href="#">
                <span about="userURI#1" property="schema:name">
                    Wrishi
                </span>
            </a>
        </span></span></dd>
    </dl>
    <dl class="published">
        <dt>Published</dt>
        <dd>
            <a href="http://localhost:8000/document/16/#3023898809">
                <time datetime="1540461449388" datatype="xsd:dateTime" property="schema:datePublished" content="1540461449388">
                        1540461449388
                </time>
            </a>
        </dd>
        <section id="answer-3023898809" rel="oa:hasBody" resource="i:#answer-3023898809">
            <h2 property="schema:name">Answer</h2>
            <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#answer-3023898809" typeof="oa:TextualBody">
                
            </div>
        </section>
<br/><br/></dl></article></blockquote></aside></section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Bamman, Eisenstein, &amp; Schnoebelen (Eds.). (2014). Gender and variation in social media.</cite></li>
  <li><cite>Bandura, Underwood, &amp; Fromson (Eds.). (1975). Disinhibition of aggression through diffusion of responsibility and dehumanization of victims.</cite></li>
</ol>
			            </div>
                    </section><script>jQuery( document ).ready(function() {
    			        jQuery(this).find('span.comment').each(function () {
                            var id=jQuery(this).attr('data-id');
                            var top=jQuery(this).offset().top - 40;
                            jQuery(document).find('article[id="'+id+'"]').each(function () {
                                jQuery(this).css('top',top);
                            });
                        });
                    });</script>
			    </div>
			</article>
		</main>
	</body>
</html>