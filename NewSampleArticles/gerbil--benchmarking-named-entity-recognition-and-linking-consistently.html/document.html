<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>GERBIL – Benchmarking Named Entity Recognition and Linking Consistently</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">GERBIL – Benchmarking Named Entity Recognition and Linking Consistently</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="MichaelRöderEmail:roeder@informatik.uni-leipzig.de(AKSW,LeipzigUniversity,Germany)" rel="bibo:authorList" resource="#MichaelRöderEmail:roeder@informatik.uni-leipzig.de(AKSW,LeipzigUniversity,Germany)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Michael  Röder <i>Email: roeder@informatik.uni-leipzig.de</i> ( AKSW, Leipzig University, Germany)
                        </span>
                    </dd><dd id="NgongaNgomoEmail:ngonga@informatik.uni-leipzig.de(AKSW,LeipzigUniversity,Germany)" rel="bibo:authorList" resource="#NgongaNgomoEmail:ngonga@informatik.uni-leipzig.de(AKSW,LeipzigUniversity,Germany)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                             Ngonga  Ngomo <i>Email: ngonga@informatik.uni-leipzig.de</i> ( AKSW, Leipzig University, Germany)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>The ability to compare systems from the same domain is of central importance for their introduction into complex applications. In the domains of named entity recognition and entity linking, the large number of systems and their orthogonal evaluation w.r.t. measures and datasets has led to an unclear landscape regarding the abilities and weaknesses of the different approaches. We present GERBIL—an improved platform for repeatable, storable and citable semantic annotation experiments— and its extension since being release. GERBIL has narrowed this evaluation gap by generating concise, archivable, human- and machine-readable experiments, analytics and diagnostics. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights into the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers, simplifying the discovery of strengths and weaknesses of their implementations with respect to the state-of-the-art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in a machine-processable format, allowing for the efficient querying and postprocessing of evaluation results. Additionally, the tool diagnostics provided by GERBIL provide insights into the areas where tools need further refinement, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. Finally, we implemented additional types of experiments including entity typing. GERBIL aims to become a focal point for the state-of-the-art, driving the research agenda of the community by presenting comparable objective evaluation results. Furthermore, we tackle the central problem of the evaluation of entity linking, i.e., we answer the question of how an evaluation algorithm can compare two URIs to each other without being bound to a specific knowledge base. Our approach to this problem opens a way to address the deprecation of URIs of existing gold standards for named entity recognition and entity linking, a feature which is currently not supported by the state-of-the-art. We derived the importance of this feature from usage and dataset requirements collected from the GERBIL user community, which has already carried out more than 24.000 single evaluations using our framework. Through the resulting updates, GERBIL now supports 8 tasks, 46 datasets and 20 systems</p></section></p><div class="article-part metadata article-keywords"><span class="keyword"> Semantic Entity Annotation System</span><span class="keyword"> Reusability</span><span class="keyword"> Archivability</span><span class="keyword"> Benchmarking Framework</span><span class="keyword"> Named Entity Recognition</span><span class="keyword">
Linking</span><span class="keyword"> Disambiguation</span></div><div class="article-part article-body"><section id="1.Introduction" inlist="" rel="schema:hasPart" resource="#1.Introduction">
                            <h2 property="schema:name">1. Introduction</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1.Introduction" typeof="deo:Introduction">
			                <p>Named Entity Recognition (NER) and Named Entity Linking/Disambiguation (NEL/D) as well as other natural language processing (NLP) tasks play a key 1570-0844/0-1900/$35.00 c 0 – IOS Press and the authors. All rights reserved 2 Roeder et al. / General Entity Annotator Benchmarking Framework role in annotating RDF knowledge from unstructured data. While manifold annotation tools have been developed over recent years to address (some of) the subtasks related to the extraction of structured data from unstructured data [21,27,39,41,43,49,53,60,63], the provision of comparable results for these tools remains a tedious problem. The issue of comparability of results is not to be regarded as being intrinsic to the annotation task. Indeed, it is now well established that scientists spend between 60 and 80% of their time preparing data for experiments [24,31,48]. Data preparation being such a tedious problem in the annotation domain is mostly due to the different formats of gold standards as well as the different data representations across reference datasets. These restrictions have led to authors evaluating their approaches on datasets (1) that are available to them and (2) for which writing a parser and an evaluation tool can be carried out with reasonable effort. In addition, many different quality measures have been developed and used actively across the annotation research community to evaluate the same task, creating difficulties when comparing results across publications on the same topics. For example, while some authors publish macroF-measures and simply call them F-measures, others publish micro-F-measures for the same purpose, leading to significant discrepancies across the scores. The same holds for the evaluation of how well entities match. Indeed, partial matches and complete matches have been used in previous evaluations of annotation tools [11,59]. This heterogeneous landscape of tools, datasets and measures leads to a poor repeatability of experiments, which makes the evaluation of the real performance of novel approaches against the state-ofthe-art rather difficult. Thus, we present GERBIL—a general framework for benchmarking semantic entity annotation systems which introduces a platform and a software for comparable, archivable and efficient semantic annotation experiments fostering a more efficient and effective community.1 In the rest of this paper, we explain the core principles which we followed to create GERBIL and detail our new contributions. Thereafter, we present the stateof-the-art in benchmarking Named Entity Recognition, Typing and Linking. In Section 4, we present the GERBIL framework. We focus in particular on the provided 1This paper is a significant extension of [65] including the progress of the GERBIL project since its initial release in 2015. features such as annotators, datasets, metrices and the evaluation processes including our new approach to match URIs. We then present an evaluation of the framework by indirectly qualifying the interaction of the community with our platform since its release. We conclude with a discussion of the current state of GERBIL and a presentation of future work. More information can be found at our project webpage http:// gerbil.aksw.org and at the code repository page https://github.com/AKSW/gerbil. The online version of GERBIL can be accessed at http: //gerbil.aksw.org/gerbil. </p></div>
                    	</section><section id="2.Principles" inlist="" rel="schema:hasPart" resource="#2.Principles">
                            <h2 property="schema:name">2. Principles </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2.Principles" typeof="">
			                <p>Insights into the difficulties of current evaluation setups have led to a movement towards the creation of frameworks to ease the evaluation of solutions that address the same annotation problem, see Section 3. GERBIL is a community-driven effort to enable the continuous evaluation of annotation tools. Our approach is an open-source, extensible framework that allows an evaluation of tools against (currently) 20 different annotators on 12 different datasets in 6 different experiment types. By integrating such a large number of datasets, experiment types and frameworks, GERBIL allows users to evaluate their tools against other semantic entity annotation systems (short: entity annotation systems) by using the same setting, leading to fair comparisons based on the same measures. Our approach goes beyond the state-of-the-art in several respects: </p><ul><li><p><strong>Repeatable settings</strong>: GERBIL provides persistent URLs for experimental settings. Hence, by using GERBIL for experiments, tool developers can ensure that the settings for their experiments (measures, datasets, versions of the reference frameworks, etc.) can be reconstructed in a unique manner in future works. </p></li></ul><ul><li><p><strong>Archivable experiments</strong>: Through experiment URLs, GERBIL also addresses the problem of archiving experimental results and allows end users to gather all pieces of information required to choose annotation frameworks for practical applications. </p></li></ul><ul><li><p><strong>Open software and service</strong>: GERBIL aims to be a central repository for annotation results without being a central point of failure: While we make experiment URLs available, we also provide users Roeder et al. / General Entity Annotator Benchmarking Framework 3 directly with their results to ensure that they use them locally without having to rely on GERBIL.</p></li></ul><ul><li><p><strong>Leveraging RDF for storage</strong>: The results of GERBIL are published in a machine-readable format. In particular, our use of DataID [3] and DataCube [14] to denote tools and datasets ensures that results can be easily combined and queried (for example to study the evolution of the performance of frameworks) while the exact configuration of the experiments remains uniquely reconstructable. By these means, we also tackle the problem of reproducibility.</p></li></ul><ul><li><p><strong>Fast configuration</strong>: Through the provision of results on different datasets of different types and the provision of results on a simple user interface, GERBIL also provides efficient means to gain an overview of the current performance of annotation tools. This provides (1) developers with insights into the type of data on which their accuracy needs improvement and (2) end users with insights allowing them to choose the right tool for the tasks at hand. </p></li></ul><ul><li><p><strong>Any knowledge base</strong>: With GERBIL we introduce the notion of knowledge-base-agnostic benchmarking of entity annotation systems through generalized experiment types. By these means, we allow the benchmarking of tools against reference datasets from any domain grounded in any reference knowledge base. </p></li></ul><p>To ensure that the GERBIL framework is useful to both end users and tool developers, its architecture and interface were designed with the following requirements in mind: </p><ul><li><p><strong>Easy integration of annotators</strong>: We provide a wrapping interface that allows annotators to be evaluated via their HTTP interface. In particular, we integrated 15 additional annotators not evaluated against each other in previous works (e.g., [11]). </p></li></ul><ul><li><p><strong>Easy integration of datasets</strong>: We also provide means to gather datasets for evaluation directly from data services such as DataHub.2 In particular, we added 37 new datasets to GERBIL. </p></li></ul><ul><li><p><strong>Easy addition of new measures</strong>: The evaluation measures used by GERBIL are implemented as interfaces. Thus, the framework can be easily extended with novel measures devised by the annotation community. 2http://datahub.io </p></li></ul><ul><li><p><strong>Extensibility</strong>: GERBIL is provided as an opensource platform3 that can be extended by members of the community both to new tasks and different purposes.</p></li></ul><ul><li><p><strong>Diagnostics</strong>: The interface of the tool was designed to provide developers with means to easily detect aspects in which their tool(s) need(s) to be improved. </p></li></ul><ul><li><p><strong>Portability of results</strong>: We generate human- and machine-readable results to ensure maximum usefulness and portability of the results generated by our framework. </p></li></ul><p>After the release of GERBIL and several hundred experiments, a list of drawbacks of current datasets stated by GERBIL’s community and developers led to requirements for further development of the platform. In particular, the requirements pertained to: </p><ul><li><p><strong>Entity Matching</strong>. The comparison of two strings representing entity URIs is not sufficient to determine whether an annotator has linked an entity correctly. For example, the two URIs http: //dbpedia.org/resource/Berlin and http://en.wikipedia.org/wiki/ Berlin stand for the same real-world object. Hence, the result of an annotation system should be marked as true positive if it generates any of these two URIs to signify the corresponding real-world object. The need to address this drawback of current datasets (which only provide one of these URIs) is amplified by the diversity of annotators and the corresponding diversity of knowledge bases (KB) on which they rely. </p></li></ul><ul><li><p><strong>Deprecated entities in datasets</strong>. Most of the gold standards in the NER and NED research area have not been updated after their first creation. Thus, the URIs they rely on have remained static over the years while the underlying KBs may have been refined or changed. This leads to some URIs in a gold standard being deprecated. As in the first requirement, there is hence a need to provide means to assess a result as true positive when the URI generated by a framework is a novel URI which corresponds to the deprecated URI. </p></li></ul><ul><li><p><strong>New tasks and Adapters</strong> GERBIL was requested for use in the two OKE challenges in 2015 and 3Available at http://gerbil.aksw.org. 4 Roeder et al. / General Entity Annotator Benchmarking Framework 2016.4,5 Thus, we implemented corresponding tasks and supported the execution of each respective campaign. Additionally, we added several state-of-the-art annotators and datasets upon community request. </p></li></ul><p>Finally, GERBIL was designed primarily for benchmarking entity annotation tools with the aim of ensuring repeatable and archiveable experiments in compliance with the FAIR principles [67]. Table 1 depicts the details of GERBIL’s implementation of the FAIR principles. </p></div>
                    	</section><section id="3.RelatedWork" inlist="" rel="schema:hasPart" resource="#3.RelatedWork">
                            <h2 property="schema:name">3. Related Work </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#3.RelatedWork" typeof="deo:RelatedWork">
			                <p>Named Entity Recognition and Entity Linking have gained significant momentum with the growth of Linked Data and structured knowledge bases. Over the past few years, the problem of result comparability has thus led to the development of a handful of frameworks. The BAT-framework [11] is designed to facilitate the benchmarking of NER, NEL/D and concept tagging approaches. BAT compares seven existing entity annotation approaches using Wikipedia as reference. Moreover, it defines six different task types, five different matchings and six evaluation measures providing five datasets. Rizzo et al. [53] present a state-ofthe-art study of NER and NEL systems for annotating newswire and micropost documents using well-known benchmark datasets, namely CoNLL2003 and Microposts 2013 for NER as well as AIDA/CoNLL and Microposts2014 [4] for NED. The authors propose a common schema, named the NERD ontology,6 to align the different taxonomies used by various extractors. To tackle the disambiguation ambiguity, they propose a method to identify the closest DBpedia resource by (exact-)matching the entity mention. Recently, Chen et al. [8] published EUEF, the easy-to-use evaluation framework which addresses three more challenges as opposed to the standard GERBIL algorithm. First, EUEF introduces a new matching metric based on fuzzy matching to account for annotator mistakes. Second, the framework introduces a new methodology for handling NIL annotations. Third, Chen et al.’s frame4https://github.com/anuzzolese/okechallenge 5https://github.com/anuzzolese/okechallenge-2016 6http://nerd.eurecom.fr/ontology work analyzes sub-components of NER/NED systems. However, EUEF only includes three systems and seven datasets and is not yet open source or online available. Ling et al. [35] recently presented a good overview and comparison of NEL systems together with their VINCULUM system. VINCULUM is not part of GERBIL as it does not provide a public endpoint. Over the course of the last 25 years several challenges, workshops and conferences dedicated themselves to the comparable evaluation of information extraction (IE) systems. Starting in 1993, the Message Understanding Conference (MUC) introduced a first systematic comparison of information extraction approaches [62]. Ten years later, the Conference on Computational Natural Language Learning (CoNLL) offered the beginnings of a shared task on named entity recognition and published the CoNLL corpus [57]. In addition, the Automatic Content Extraction (ACE) challenge [17], organized by NIST, evaluated several approaches but was discontinued in 2008. Since 2009, the text analytics conference has hosted the workshop on knowledge base population (TAC-KBP) [38] where mainly linguistic-based approaches are published. The Senseval challenge, originally concerned with classical NLP disciplines, widened its focus in 2007 and changed its name to SemEval to account for the recently recognized impact of semantic technologies [32]. The Making Sense of Microposts workshop series (#Microposts) established in 2013 an entity recognition and in 2014 an entity linking challenge focusing on tweets and microposts [56]. In 2014, Carmel et al. [6] introduced one of the first Web-based evaluation systems for NER and NED and the centerpiece of the entity recognition and disambiguation (ERD) challenge. Here, all frameworks are evaluated against the same unseen dataset and provided with corresponding results. GERBIL goes beyond the state-of-the-art by extending the BAT-framework as well as [53] enhancing reproducibility, diagnostics and publishability of entity annotation systems. In particular, we provide 37 additional datasets and 15 additional annotators. The framework addresses the lack of treatment of NIL values within the BAT-framework and provides more wrapping approaches for annotators and datasets. Moreover, GERBIL provides persistent URLs for experiment results, unique URIs for frameworks and datasets, a machine-readable output and automatic dataset updates from data portals. Thus, it allows for a holistic comparison of existing annotators while simplifying the archiving of experimental results. Moreover, our framework offers opportunities for the fast and simple evaluation of entity annotation system prototypes via novel NIF-based [26] interfaces, which are designed to simplify the exchange of data and binding of services.</p><table><tbody><tr><td><p>F1. (meta)data are assigned a globally unique and persistent identifier</p></td><td><p>Unique W3ID URIs per experiment</p></td></tr><tr><td><p>F2. data are described with rich metadata (defined by R1 below)</p></td><td><p>Experimental configuration as RDF</p></td></tr><tr><td><p>F3. metadata clearly and explicitly include the identifier of the data it describes</p></td><td><p>Relates via RDF</p></td></tr><tr><td><p>F4. (meta)data are registered or indexed in a searchable resource</p></td><td><p>batch-updated SPARQL endpoint: http://gerbil.aksw.org/sparql</p></td></tr></tbody></table><figure data-equation="" data-image="25" data-figure-category="figure" data-caption="" id="F17442051" data-image-src="/media/images/8789efff-ea7c-4807-97ef-074efe63e906.jpg"><div><img src="8789efff-ea7c-4807-97ef-074efe63e906.jpg"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Alexander, Cyganiak, Hausenblas, &amp; Zhao, 2011)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:1734142282}]">(Blanco, Ottaviano, &amp; Meij, 2015)</span></p></div>
                    	</section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Alexander, K., Cyganiak, R., Hausenblas, M., &amp; Zhao, J. (Eds.). (2011). Describing Linked Datasets with the VoID Vocabulary. W3C Interest Group Note, 03 .</cite></li>
  <li><cite>Blanco, R., Ottaviano, G., &amp; Meij, E. (Eds.). (2015). Fast and space-efficient entity linking for queries. In Xueqi Cheng, Hang Li, Evgeniy Gabrilovich, and Jie Tang, editors, Proceedings of the Eighth ACM International Conference on Web Search and Data Mining.</cite></li>
</ol>
			            </div>
                    </section>
			    </div>
			</article>
		</main>
	</body>
</html>