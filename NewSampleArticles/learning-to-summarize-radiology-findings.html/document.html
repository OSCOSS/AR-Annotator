<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>Learning to Summarize Radiology Findings</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">Learning to Summarize Radiology Findings</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="YuhaoZhangEmail:yuhaozhang@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#YuhaoZhangEmail:yuhaozhang@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Yuhao Zhang <i>Email: yuhaozhang@stanford.edu</i> (Stanford University)
                        </span>
                    </dd><dd id="ChristopherD.ManningEmail:manning@stanford.edu(StanfordUniversity)" rel="bibo:authorList" resource="#ChristopherD.ManningEmail:manning@stanford.edu(StanfordUniversity)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Christopher D. Manning <i>Email: manning@stanford.edu</i> (Stanford University)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>The Impression section of a radiology report summarizes crucial radiology findings in natural language and plays a central role in communicating these findings to physicians. However, the process of generating impressions by summarizing findings is time-consuming for radiologists and prone to errors. We propose to automate the generation of radiology impressions with neural sequence-to-sequence learning. We further propose a customized neural model for this task which learns to encode the study background information and use this information to guide the decoding process. On a large dataset of radiology reports collected from actual hospital studies, our model outperforms existing non-neural and neural baselines under the ROUGE metrics. In a blind experiment, a board-certified radiologist indicated that 67% of sampled system summaries are at least as good as the corresponding human written summaries, suggesting significant clinical validity. To our knowledge our work represents the first attempt in this direction.</p></section></p><div class="article-part article-body"><section id="1Introduction" inlist="" rel="schema:hasPart" resource="#1Introduction">
                            <h2 property="schema:name">1 Introduction </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1Introduction" typeof="deo:Introduction">
			                <p>The radiology report documents and communicates crucial findings in a radiology study. As shown in Figure 1, a standard radiology report usually consists of a Background section that describes the exam and patient information, a Findings section, and an Impression section (Kahn Jr et al., 2009). In a typical workflow, a radiologist first dictates the detailed findings into the report, and then summarizes the salient findings into the more concise Impression section based also on the condition of the patient. The impressions are the most significant part of a radiology report that communicate the findings. Previous studies have shown that over 50% of referring physicians read only the impression statements in a report (Lafortune et al., 1988; Background: history: swelling; pain. technique: 3 views of the left ankle were acquired. comparison: no prior study available. Findings: there is normal mineralization and alignment. no fracture or osseous lesion is identified. the ankle mortise and hindfoot joint spaces are maintained. there is no joint effusion. the soft tissues are normal. Human Impression: normal left ankle radiographs. Extractive Baseline: there is no joint effusion. Pointer-Generator: normal right ankle. Our model: normal radiographs of the left ankle. Figure 1: An example radiology report with study background information organized into a Background Section, and radiology findings in a Findings Section. The human-written summary (or impression) and predicted summaries from different models are also shown. The extractive baseline does not summarize well, the baseline pointer-generator model generates spurious sequence, while our model gives correct summary by incorporating the background information. Bosmans et al., 2011). Despite its importance, the generation of the impression statements is errorprone. For example, crucial findings may be forgotten, which would cause significant miscommunications (Gershanik et al., 2011). Additionally, the process of writing the impression statements is time-consuming and highly repetitive with the dictation of the findings. This suggests a crucial need to automate the radiology impression generation process. In this work, we propose to automate the generation of radiology impressions with neural sequence-to-sequence learning. In particular, we argue that this task could be viewed as a text summarization problem, where the source sequence is the radiology findings and the target sequence the impression statements. We collect a dataset of radiology reports from actual hospital radiographic studies, and find that this task involves both extractive summarization where descriptions of radiology observations can be taken directly from the findings, and abstractive summarization where new words and phrases, such as conclusions of the study, need to be generated from scratch. We empirically evaluate existing popular summarization systems on this task and find that, while existing neural models such as the pointer-generater network can generate plausible summaries, they sometimes fail to model the study background information and thus generate spurious results. To solve this problem, we propose a customized summarization model that properly encodes the study background information and uses the encoded information to guide the decoding process. We show that our model outperforms existing non-neural and neural baselines on our dataset measured by the standard ROUGE metrics. Moreover, in a blind experiment, a board-certified radiologist indicated that 67% of sampled system summaries are at least as good as the reference summaries written by well-trained radiologists, suggesting significant clinical validity of the resulting system. We further show through detailed analysis that our model could be reliably transferred to radiology reports from another organization, and that the model can sometimes summarize radiology studies for body parts unseen during training. To review, our main contributions in this paper include: (i) we propose to summarize radiology findings into impression statements with neural sequence-to-sequence learning, and to our knowledge our work represents the first attempt in this direction; (ii) we propose a new customized summarization model to this task that improves over existing methods by better leveraging study background information; (iii) we further show via a radiologist evaluation that the summaries generated by our model have significant clinical validity. </p></div>
                    	</section><section id="2RelatedWork" inlist="" rel="schema:hasPart" resource="#2RelatedWork">
                            <h2 property="schema:name">2 Related Work</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2RelatedWork" typeof="deo:RelatedWork">
			                <p><strong>Early Summarization Systems.</strong> Early work on summarization systems mainly focused on extractive approaches, where the summaries are generated by scoring and selecting sentences from the input. Luhn (1958) proposed to represent the input by topic words and score each sentence by the amount of topic words it contains. Kupiec et al. (1995) scored sentences with a feature-based statistical classifier. Steinberger and Jezek (2004) applied latent semantic analysis to cluster the topics and then select sentences that cover the most topics. Meanwhile, various graph-based methods, such as the LexRank (Mihalcea and Tarau, 2004) and the TextRank algorithm (Erkan and Radev, 2004), were applied to model sentence dependency by representing sentences as vertices and similarities as edges. Sentences are then scored and selected via modeling of the graph properties. </p><p><strong>Neural Summarization Systems.</strong> Summarization systems based on neural network models enable abstractive summarization, where new words and phrases are generated to form the summaries. Rush et al. (2015) first applied an attention-based neural encoder and a neural language model decoder to this task. Nallapati et al. (2016) used recurrent neural networks for both the encoder and the decoder. To address the limitation that neural models with a fixed vocabulary cannot handle outof-vocabulary words, a pointer-generator model was proposed which uses an attention mechanism that copies elements directly from the input (Nallapati et al., 2016; Merity et al., 2017; See et al., 2017). See et al. (2017) further proposed a coverage mechanism to address the repetition problem in the generated summaries. Paulus et al. (2018) applied reinforcement learning to summarization and more recently, Chen and Bansal (2018) obtained improved result with a model that first selects sentences and then rewrites them. </p><p><strong>Summarization of Radiology Reports.</strong> Most prior work that attempts to ‚Äúsummarize‚Äù radiology reports focused on classifying and extracting information from the report text (Friedman et al., 1995; Hripcsak et al., 1998; Elkins et al., 2000; Hripcsak et al., 2002). More recently, Hassanpour and Langlotz (2016) studied extracting named entities from multi-institutional radiology reports using traditional feature-based classifiers. Goff and Loehfelm (2018) built an NLP pipeline to identify asserted and negated disease entities in the impression section of radiology reports as a step towards report summarization. Cornegruta et al. (2016) proposed to use a recurrent neural network architecture to model radiological language in solving the medical named entity recognition and negation detection tasks on radiology reports. To our knowledge, our work represents the first attempt at automatic summarization of radiology findings into natural language impression statements. </p></div>
                    	</section><section id="3TaskDefinition" inlist="" rel="schema:hasPart" resource="#3TaskDefinition">
                            <h2 property="schema:name">3 Task Definition</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#3TaskDefinition" typeof="">
			                <p> We now give a formal definition of the task of summarizing radiology findings. Given a passage of findings represented as a sequence of tokens x = {x1, x2, . . . , xN }, with N being the length of the findings, our goal is to find a sequence of tokens y = {y1, y2, . . . , yL} that best summarizes the salient and clinically significant findings in x, with L being an arbitrary length of the summary.1 Note that the mapping between x and y can either be modeled in an unsupervised way (as done in unsupervised summarization systems), or be learned from a dataset of findings-summary pairs. </p></div>
                    	</section><section id="4Models" inlist="" rel="schema:hasPart" resource="#4Models">
                            <h2 property="schema:name">4 Models </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#4Models" typeof="deo:Model">
			                <p>In this section we introduce our model for the task of summarizing radiology findings. As our model builds on top of existing work on neural sequence-to-sequence learning and the pointergenerator model, we start by introducing them. </p></div>
                    	</section><section id="4.1NeuralSequence-to-SequenceModel" inlist="" resource="#4.1NeuralSequence-to-SequenceModel">
                            <h3 property="schema:name">4.1 Neural Sequence-to-Sequence Model </h3>
                        <p>At a high-level, our model implements the summarization task with an encoder-decoder architecture, where the encoder learns hidden state representations of the input, and the decoder decodes the input representations into an output sequence. For the encoder, we use a Bi-directional Long Short-Term Memory (Bi-LSTM) network. Given the findings sequence x = {x1, x2, . . . , xN }, we encode x into hidden state vectors with: h = Bi-LSTM(x), (1) where h = {h1, h2, . . . , hN }. Here hN combines the last hidden states from both directions in the encoder. After the entire input sequence is encoded, we generate the output sequence step by step with a separate LSTM decoder. Formally, at the t-th step, given the previously generated token yt‚àí1 and the previous decoder state st‚àí1, the decoder calculates the current state st with: st = LSTM(st‚àí1, yt‚àí1). (2) We then use st to predict the output word. For the initial decoder state we set s0 = hN . 1While the name ‚Äúimpression‚Äù is often used in clinical settings, we use ‚Äúsummary‚Äù and ‚Äúimpression‚Äù interchangably. The vanilla sequence-to-sequence model that uses only st to predict the output word has a major limitation: it generates the entire output sequence based solely on a vector representation of the input (i.e., hN ), which may result in significant information loss. For better decoding we therefore employ the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which uses a weighted sum of all input states at every decoding step. Given the decoder state st and an input hidden state hi , we calculate an input distribution a t as: e t i = v &gt; tanh(Whhi + Wsst), (3) a t = softmax(e t ), (4) where Wh, Ws and v are learnable parameters.2 We then calculate a weighted input vector as: h ‚àó t = X i a t ihi . (5) h ‚àó t encodes the salient input information that is useful at decoding step t. Lastly, we obtain the output vocabulary distribution at step t as: P(yt |x, y h‚àó h ‚àó t + w &gt; s st + wyyt‚àí1), (7) where yt‚àí1 denotes the previous decoder output, wh‚àó , ws and wy learnable parameters and œÉ a sigmoid function. For the copy distribution, we reuse the attention distribution a t calculated in (4). Therefore, the overall output distribution in the pointer-generator network is: P(yt |x, y no acute Findings Attention  Distribution Background Attention  Distribution a zoster Findings Hidden  States Background Hidden States Vocabulary Distribution Impressions Hidden  States a zoster arthritis Final Ditribution ... there     is no acute fracture , dislocation , prominent arthritis Findings Findings Attention  Distribution a zoster Findings Hidden  States Vocabulary Distribution Impressions Hidden  States a zoster arthritis Final Ditribution ... there is no acute fracture , dislocation , prominent arthritis pain following a fall , two view right knee , no acute findings Attention Findings Encoder Decoder a zoster "found" Final Distribution ... ... Attention  Background Encoder ... ... there     is no acute fracture , dislocation , prominent arthritis pain following a fall , two view right knee , no acute findings Findings Attention  Distribution a zoster Findings Hidden  States Impressions Hidden  States a zoster found Final Ditribution ... a zoster Pgen ... Background Attention  Distribution Background Hidden  States ... a zoster a zoster Copy Distribution Vocab Distribution Text Figure 2: Overall architecture of our summarization model. where Pvocab(yt) is the same as the output distribution in (6).</p><figure data-equation="" data-image="16" data-figure-category="figure" data-caption="" id="F37864361" data-image-src="/media/images/f478c09c-21a2-442b-a7b4-2bf15c5ed7d1.png"><div><img src="f478c09c-21a2-442b-a7b4-2bf15c5ed7d1.png"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure></section><section id="4.3IncorporatingStudyBackgroundInformation" inlist="" resource="#4.3IncorporatingStudyBackgroundInformation">
                            <h3 property="schema:name">4.3 Incorporating Study Background Information </h3>
                        <p>The background part of a radiology report is also important, since crucial information such as the purpose of the study, the body part involved and the condition of the patient are often mentioned only in the background. A straightforward way of incorporating the background information is to prepend all the background text to the findings, and treat the entire sequence as input to the pointer-generator network. However, as we show in Section 6, this naive method in fact hurts the summarization quality, presumably because the model cannot sufficiently distinguish between the findings and the background information, which as a result leads to insufficient modeling of both the findings and the background. To solve this, we propose to encode the background text with a separate attentional encoder, and use the resulting background representation to guide the decoding process in the summarization model (Figure 2). For clarity we now use x b to denote the background token sequence, and x to denote the actual findings section. Our goal is then to find y that maximizes P(y|x, x b ). To do this, we again obtain the hidden state vectors h of the findings section as in (1). Similarly, we obtain the hidden state vectors of the background text with x b as input using a separate Bi-LSTM encoder: h b = Bi-LSTMb (x b ). (9) Next, we calculate a distribution over h b as: e 0 i = v 0&gt; tanh(Wbh b i + WhhN ), (10) a 0 = softmax(e 0 ), (11) where Wb and Wh are learnable parameters and hN the last hidden state of the findings encoder. The distribution a 0 models the importance of tokens in the background section. We then obtain a weighted representation of the background text as: b = X i a 0 ih b i , (12) where vector b has the same size as h b , and encodes the salient background information. Lastly, we use the background vector b to guide the decoding process, by modifying the recurrent kernel of the decoder LSTM in (2) to be: Ô£Æ Ô£Ø Ô£Ø Ô£∞ it ft ot ut Ô£π Ô£∫ Ô£∫ Ô£ª = Ô£Æ Ô£Ø Ô£Ø Ô£∞ œÉ œÉ œÉ tanh Ô£π Ô£∫ Ô£∫ Ô£ª W ¬∑ Ô£Æ Ô£∞ st‚àí1 yt‚àí1 b Ô£π Ô£ª , (13) ct = ft ¬∑ ct‚àí1 + it ¬∑ ut , (14) st = ot ¬∑ tanh(ct), (15) where it , ft , ot denotes the input, forget, and output gates, W the weight matrix and ct the internal cell of the LSTM repectively, and ¬∑ represents an element-wise multiplication. Again for clarity we leave out the bias terms in (13). As a result, every state in the decoding process is directly in- fluenced by the information encoded by the background vector b. The rest of the model, including chest abdomen pelvis spine knee ankle shoulder foot wrist hand elbow tibia 4 k 6 k 8 k 10 k # Examples Figure 3: Number of examples split by body part in the collected Stanford Hospital dataset. the calculation of the vocabulary distribution and the copy distribution, remains the same.</p></section><section id="5Experiments" inlist="" rel="schema:hasPart" resource="#5Experiments">
                            <h2 property="schema:name">5 Experiments</h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#5Experiments" typeof="deo:Evaluation">
			                <p>To test the effectiveness of our summarization model, we collected reports of radiographic studies from the picture archiving and communication system (PACS) at the Stanford Hospital. <span class="comment ref do" data-id="1042071550" rel="schema:hasPart" typeof="dctypes:Text" resource="r-1042071550"><mark id="1042071550" property="schema:description">
                        We describe our data collection process, baseline models and experimental setup in this section, and present the results and discussions in Section 6
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#1042071550" resource="http://localhost:8000/document/11//comment-1042071550">
       		              üí¨
                    </a>
                </sup></span>. </p></div>
                    	<aside class="note do"><blockquote cite="1042071550"><article id="1042071550" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/11/#1042071550"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/11/#1042071550">
            <time datetime="1540461245105" datatype="xsd:dateTime" property="schema:datePublished" content="1540461245105">
                1540461245105
            </time>
        </a>
    </dd>
</dl>
<section id="comment-1042071550" rel="oa:hasBody" resource="i:#comment-1042071550">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-1042071550" typeof="oa:TextualBody">
        floccinaucinihilipilification
    </div>
</section>

<br/><br/></article></blockquote></aside></section><section id="5.1DataCollection" inlist="" resource="#5.1DataCollection">
                            <h3 property="schema:name">5.1 Data Collection</h3>
                        <p>Reports of all radiographic studies from 2000 to 2014 were collected. We first tokenized all reports with Stanford CoreNLP (Manning et al., 2014), and filtered the dataset by excluding reports where (1) no findings or impression section can be found; (2) multiple findings or impression sections can be found but cannot be aligned; or (3) the findings have fewer than 10 words or the impression has fewer than 2 words. We removed body parts where only a small number of cases are available, and included reports of the top 12 body parts in the PACS system to maintain generalizability. For common body parts with more than 10k reports (e.g., chest), we subsampled 10k reports from them. This results in a dataset with a total of 87,127 reports. We further randomly split the dataset into a 70% training (60,990), a 10% development (8,712) and a 20% test set (17,425). We show the dataset statistics split by body part in Figure 3. </p></section><section id="5.2BaselineModels" inlist="" resource="#5.2BaselineModels">
                            <h3 property="schema:name">5.2 Baseline Models</h3>
                        <p> For our main experiments, we compare our model against several competitive non-neural and neural systems on the collected dataset. Unless otherwise stated, the baseline models take only the findings section as input.3 S&amp;J-LSA. This is an extractive approach described by Steinberger and Jezek (2004), which applies Latent Semantic Analysis (LSA) to summarization. It first scores ‚Äúconcept‚Äù clusters by applying singular value decomposition to the termby-sentence co-occurence matrix derived from the passage. Sentences with the top scored concepts are then kept as the summaries. LexRank. LexRank is another popular extractive model introduced by Erkan and Radev (2004). In LexRank, a passage is represented as a graph of sentences, and a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph. Sentences are scored by the eigenvector centrality in the graph, and the highest scored sentences are kept. Pointer-Generator. We also run the baseline pointer-generator model introduced by See et al. (2017). We find the ‚Äúcoverage‚Äù mechanism described in the paper did not improve summary quality in our task and therefore did not use it for simplicity. We compare our model with two versions of the pointer-generator model: one with only the findings section as input and another one with the background sections prepended to the findings section as input. </p></section><section id="5.3ExperimentalSetup" inlist="" resource="#5.3ExperimentalSetup">
                            <h3 property="schema:name">5.3 Experimental Setup </h3>
                        <p><strong>Evaluation Metrics.</strong> In our main experiments we evaluate the models with the widely-used ROUGE metric (Lin, 2004). We report the F1 scores for ROUGE-1, ROUGE-2 and ROUGE-L, which measure the word-level unigram-overlap, bigram-overlap and the longest common sequence between the reference summary and the system predicted summary respectively. </p><p><strong>Word Vectors.</strong> <span class="comment ref do" data-id="2984971234" rel="schema:hasPart" typeof="dctypes:Text" resource="r-2984971234"><mark id="2984971234" property="schema:description">
                        To enable knowledge transfer from a larger corpus, we applied the GloVe algorithm (Pennington et al., 2014) to a corpus of 4.5 million radiology reports of all modalities (e.g., X-ray, CT) and body parts
                    </mark><sup class="ref-annotation">
    		        <a rel="cito:hasReplyFrom" href="#2984971234" resource="http://localhost:8000/document/11//comment-2984971234">
       		              üí¨
                    </a>
                </sup></span>. We used the resulting 100-dimensional word vectors to initialize all word embedding layers in our neural models, and empirically found this to improve the performance of our neural models by about 1 ROUGE-L score. </p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">(Bahdanau, Cho, &amp; Bengio, 2015)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:116737139}]">(Bosmans &amp; Weyler, 2011)</span></p><aside class="note do"><blockquote cite="2984971234"><article id="2984971234" about="i:" typeof="oa:Annotation" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# schema: http://schema.org/ dcterms: http://purl.org/dc/terms/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# i: http://localhost:8000/document/11/#2984971234"><h3 property="schema:name" style="display:none">
    Wrishi
    <span rel="oa:motivatedBy" resource="oa:replying">replies</span>
</h3>
<dl class="author-name"><dt>Authors</dt><dd><span rel="schema:creator">
    <span about="userURI#1" typeof="schema:Person">
       <img alt="" rel="schema:image" src="https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80" width="48" height="48"/>
       <a href="#"><span about="userURI#1" property="schema:name">
           Wrishi
       </span></a>
    </span>
</span></dd></dl>
<dl class="published">
    <dt>Published</dt>
    <dd>
        <a href="http://localhost:8000/document/11/#2984971234">
            <time datetime="1540227737520" datatype="xsd:dateTime" property="schema:datePublished" content="1540227737520">
                1540227737520
            </time>
        </a>
    </dd>
</dl>
<section id="comment-2984971234" rel="oa:hasBody" resource="i:#comment-2984971234">
    <h2 property="schema:name">Comment</h2>
    <div datatype="rdf:HTML" property="rdf:value schema:description" resource="i:#comment-2984971234" typeof="oa:TextualBody">
        We find that when the background section is prepended to the input, the extractive baseline models may select sentences in the background part as the summary, resulting in deteriorated performance.
    </div>
</section>

<br/><br/></article></blockquote></aside></section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite>Bahdanau, D., Cho, K., &amp; Bengio, Y. (Eds.). (2015). Neural machine translation by jointly learning to align and translate. The 2015 International Conference on Learning Representations.</cite></li>
  <li><cite>Bosmans, J. M., &amp; Weyler, J. J. (Eds.). (2011). The radiology report as seen by radiologists and referring clinicians: results of the COVER and ROVER surveys.</cite></li>
</ol>
			            </div>
                    </section><script>jQuery( document ).ready(function() {
    			        jQuery(this).find('span.comment').each(function () {
                            var id=jQuery(this).attr('data-id');
                            var top=jQuery(this).offset().top - 40;
                            jQuery(document).find('article[id="'+id+'"]').each(function () {
                                jQuery(this).css('top',top);
                            });
                        });
                    });</script>
			    </div>
			</article>
		</main>
	</body>
</html>