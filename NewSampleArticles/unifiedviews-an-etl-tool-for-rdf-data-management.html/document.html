<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:base="http://www.dc4plus.com/references/rdf_sem.html" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:foaf="http://xmlns.com/foaf/0.1/" >
    <head>
        <title>UnifiedViews: An ETL Tool for RDF Data Management</title>
        
        <meta charset="utf-8" />
        <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link href="https://dokie.li/media/css/basic.css" media="all" rel="stylesheet" title="Basic" />
        <link disabled="" href="https://dokie.li/media/css/lncs.css" media="all" rel="stylesheet alternate" title="LNCS" />
        <link href="https://dokie.li/media/css/acm.css" media="all" rel="stylesheet" title="ACM" />
        <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
        <script src="https://dokie.li/scripts/simplerdf.js"></script>
        <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
        <script src="https://dokie.li/scripts/do.js"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    </head>
	<body about="" id="article" typeof="schema:ScholarlyArticle sioc:Post prov:Entity foaf:Document sioc:Post biblio:Paper bibo:Document as:Article" prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio#">
        <main>
            <article about="" typeof="schema:Article">
	  	        <div class="article-content" id="content">
                    
                    <p><div class="article-part article-title" property="schema:name"><h1 class="article-part article-title" property="schema:name">UnifiedViews: An ETL Tool for RDF Data Management</h1></div></p><div class="article-part metadata article-authors" id="authors"><dd id="TomášKnapEmail:toknap@gmail.com(CharlesUniversityinPrague,FacultyofMathematicsandPhysicsMalostranskénám.25,11800Praha1,CzechRepublic)" rel="bibo:authorList" resource="#TomášKnapEmail:toknap@gmail.com(CharlesUniversityinPrague,FacultyofMathematicsandPhysicsMalostranskénám.25,11800Praha1,CzechRepublic)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Tomáš  Knap <i>Email: toknap@gmail.com</i> (Charles University in Prague, Faculty of Mathematics and Physics Malostranské nám. 25, 118 00 Praha 1, Czech Republic)
                        </span>
                    </dd><dd id="PeterHanecákEmail:tomas.knap@semantic-web.com(SemanticWebCompany,Neubaugasse1,1070Vienna,Austria)" rel="bibo:authorList" resource="#PeterHanecákEmail:tomas.knap@semantic-web.com(SemanticWebCompany,Neubaugasse1,1070Vienna,Austria)">
                        <span rel="schema:creator schema:publisher schema:author" typeof="schema:person">
                            Peter  Hanecák <i>Email: tomas.knap@semantic-web.com</i> (Semantic Web Company, Neubaugasse 1, 1070 Vienna, Austria)
                        </span>
                    </dd></div><p><section id="Abstract" class="article-part metadata article-abstract" datatype="rdf:HTML" property="schema:abstract"><p>We present UnifiedViews, an Extract-TransformLoad (ETL) framework that allows users to define, execute, monitor, debug, schedule, and share data processing tasks, which may employ custom plugins (data processing units) created by users. UnifiedViews natively supports processing of RDF data. In this paper, we: (1) introduce UnifiedViews’ basic concepts and features, (2) demonstrate the maturity of the tool by presenting exemplary projects where UnifiedViews is successfully deployed, and (3) outline research projects and future directions in which UnifiedViews is exploited. Based on our practical experience with the tool, we found that UnifiedViews simplifies the creation and maintenance of Linked Data publication processes.</p></section></p><div class="article-part metadata article-keywords"><span class="keyword"> ETL</span><span class="keyword"> Data management</span><span class="keyword"> Data processing</span><span class="keyword"> RDF
data</span><span class="keyword"> Linked Data</span></div><div class="article-part article-body"><section id="1.Introduction" inlist="" rel="schema:hasPart" resource="#1.Introduction">
                            <h2 property="schema:name">1. Introduction </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#1.Introduction" typeof="deo:Introduction">
			                <p>The advent of Linked Data [1] accelerates the evolution of the Web into an exponentially growing informa- *Corresponding author. E-mail: tomas.knap@semantic-web.com *Corresponding author. E-mail: tomas.knap@semantic-web.com tion space where the unprecedented volume of structured data offers information consumers a level of information integration that has up to now not been possible. Data consumers can create mashups of Linked Data that leverage various data sources to support use cases which were not intended by the original data publishers. Suppose a data wrangler wants to build an RDF data mart that integrates information from various RDF and non-RDF sources. So the wrangler’s data processing task involves the activities of: </p><ol><li><p>getting the data from certain data sources, </p></li><li><p>transforming the data to RDF data format, </p></li><li><p>cleaning the data, </p></li><li><p>interlinking it with other (external) data sources, </p></li><li><p>solving data conflicts to prepare the integrated data mart. </p></li></ol><p>There are numerous tools used by the Linked Data community1 , which may support various phases of the data mart preparation; e.g., the wrangler may use 1http://semanticweb.org/wiki/Tools 1570-0844/0-1900/$27.50 c 0 – IOS Press and the authors. All rights reserved 2 any232 to extract non-RDF data and convert such data to RDF data format, OpenLink Virtuoso3 database for storing RDF data and executing SPARQL (Update) queries [4,5], Silk [14] to interlink RDF data based on the declarative rules, or LD-FusionTool [8] to solve RDF data conflicts4 . Nevertheless, using such tools directly, the data wrangler has to: </p><ul><li><p>configure every such tool differently, </p></li><li><p>write a custom script downloading and unpacking data from various data sources, </p></li><li><p>prepare a script executing a set of SPARQL Update queries curating the data, </p></li><li><p>implement custom transformers, which, e.g., enrich processed data with the data from the DBpedia knowledge base [2], and </p></li><li><p>write a custom script ensuring that the tools are executed in the required order, so that every tool has all the desired inputs when being launched. </p></li></ul><p>Maintaining data processing tasks of increasing complexity is challenging. Suppose for example that the wrangler defines tens of such data processing tasks, which should run every month. So apart from the activities described above, the data wrangler has to also configure a scheduling script or use an external tool, such as cron5 , to ensure that the task is executed regularly. Furthermore, suppose that certain data processing task does not end as expected. To find the problem, the wrangler needs to query and browse through the intermediate results of the data processing task; this typically involves the need for manually setting up a triple store such as OpenLink Virtuoso and loading the suspicious intermediate RDF data data into it. Furthermore, if the data wrangler needs to review/adjust one of the data processing tasks later in the future, he has to re-examine the prepared scripts, recall the general idea of the task, the data flow between the tools etc.; in contrary, if there was a graphical visualization of the prepared task, which shows tools being used and the data flow between these tools, the data processing task would be documented and maintenance of such task would be much easier. Finally, the data wrangler cannot easily reuse configurations of the tools among the data processing tasks, so he cannot effectively reuse the already prepared tasks. 2https://any23.apache.org/ 3http://virtuoso.openlinksw.com/ 4https://github.com/mifeet/LD-FusionTool 5http://linux.die.net/man/8/cron The task of compiling and setting up various tools of different vendors for multiple data analyses settings is cumbersome and often repetitive. In combination with the lack of an integrated debugging and maintenance support, the immediate consequence is a negative impact on a data wrangler’s productivity. On a larger scale, we believe that the current lack of easyto-use frameworks for Linked Data preparation and publication prevents many institutions to provide their datasets for public utilization as Linked Data. Therefore, instead of requiring data wranglers to write most of the logic for defining, executing, monitoring, scheduling, and sharing the data processing tasks themselves, we provide UnifiedViews6 , an opensource Extract-Transform-Load (ETL) framework. It is an integrated solution that provides standard maintenance interfaces and lets data wranglers choose from various pre-defined and customizable ”building blocks” to set up the individual data processing steps. This paper is organized as follows. In Section 2, we present the basic concepts of UnifiedViews, how a data wrangler may interact with the tool, and describe its architecture. To demonstrate the maturity of UnifiedViews, we introduce in Section 3 a number of exemplary projects in which the framework is successfully used. In Section 4, we summarize lessons learned from Section 3 and introduce current UnifiedViews research projects and future work. In Section 5 we provide an outline of related work in this problem domain and draw our conclusions in Section 6. </p></div>
                    	</section><section id="2.UnifiedViewsFramework" inlist="" rel="schema:hasPart" resource="#2.UnifiedViewsFramework">
                            <h2 property="schema:name">2. UnifiedViews Framework </h2>
			                <div datatype="rdf:HTML" property="schema:description" resource="#2.UnifiedViewsFramework" typeof="ssn:System">
			                <p>An Extract-Transform-Load (ETL) process performs: (1) extraction of data from original data sources, (2) transformation of the data to the proper format and structure for the purposes of the consequent querying and/or data analysis, and (3) load of the resulting data into the target data source (also called operational data store, or data warehouse/data mart). An Extract-Transform-Load (ETL) framework allows users at least to define, monitor, and execute ETL processes. UnifiedViews is an open-source ETL framework with a focus on processing RDF data. It allows users to define, execute, monitor, debug, schedule, and share Linked Data processing and publishing tasks. A sam6http://unifiedviews.eu 3 ple task is the preparation of a data mart by the wrangler as introduced in Section 1. A data processing task is represented in UnifiedViews as a data processing pipeline (or simply pipeline). Every pipeline consists of one or more data processing units (DPUs) and data flows between these DPUs. Every DPU may declare certain mandatory or optional inputs, encapsulates certain business logic that processes the data (e.g., a DPU may extract data from a SPARQL endpoint, apply a SPARQL query, or transform CSV data to RDF data), and may produce certain outputs. DPUs may also provide a configuration dialog, so that a pipeline designer (e.g., the data wrangler mentioned above) may configure them differently in different pipelines. Administrators of the particular UnifiedViews installations may set up the default configurations of the DPUs and also prepare various alternative configurations, which may be directly used by pipeline designers. A data unit is a container for data being consumed or produced by a DPU. We distinguish input and output data units. An input data unit contains data used as the input during a DPU’s execution. An output data unit holds the data which is produced in the course of a DPU’s execution. Every data flow between two DPUs X and Y consists of the output data unit of DPU X producing the data and the input data unit of DPU Y consuming the data. Every data unit has its name, which is assigned by a DPU developer and is mandatory. Data unit names are then visible to pipeline designers, so that pipeline designers may, e.g., map an output data unit of one DPU to input data unit of another DPUs. Every DPU may declare zero or more input data units and zero or more output data units. UnifiedViews supports three types of data units which can be both input and output data units and are distinguished by the type of information they can hold: </p><ul><li><p>RDF data units can contain RDF graphs, </p></li><li><p>Files data units can contain files and folders, and </p></li><li><p>Relational data units can contain tables from relational databases. </p></li></ul><p>Every data unit can hold zero or more entries of the particular supported type. There are four types of DPUs, which are determined by the number of input and output data units they declare, as well as their intended purpose: </p><ul><li><p><em>Extractor</em>: A DPU that does not define any input data unit. Input data to such a DPU is not provided by the UnifiedViews framework, but rather obtained from external sources by the business logic of the DPU. For instance, an extractor may query data from a remote SPARQL endpoint or download files from a certain set of URLs. </p></li><li><p><em>Transformer</em>: A DPU that transforms inputs to outputs. It defines both input and output data units. UnifiedViews must ensure that proper inputs are prepared for the DPU and must also handle the outputs produced by the DPU. Examples of transformers are DPUs that transform tabular data to RDF data or execute SPARQL (Update) queries. </p></li><li><p><em>Loader</em>: A DPU that defines an input data unit, but does not define any output data unit. Output data produced by such a DPU is not maintained by the UnifiedViews framework, but rather intended for storage in external repositories outside of UnifiedViews. DPUs uploading data to a remote SPARQL endpoint or disseminating new records to the CKAN catalog7 are examples of loaders. </p></li><li><p><em>Quality</em> Assessor: A DPU that assesses the quality of the input data and produces a quality assessment report as the output. We decided to distinguish this type of DPUs from transformers, because they work differently – they do not produce transformed data at the output, but rather produce quality assessment report. For instance, quality assessor DPUs may check to which extent the input data is complete or whether data type literals contain correct values in the resulting data. </p></li></ul><p>UnifiedViews distinguishes DPU templates and DPU instances. The DPUs available in the system to be used on the pipeline by data wrangles are called DPU templates. Every DPU template defines a template configuration of the DPU. When a data wrangler places such DPU template on the pipeline, such placement is called DPU instantiation and the result is called a DPU instance. The DPU instance has its own instance configuration being based on the template configuration of the DPU. The DPU instance is always associated with the given pipeline. One pipeline may contain more different DPU instances of the same DPU template. Every DPU instance always points to the DPU template from which it was created. 7http://ckan.org/ 4 Fig. 1. UnifiedViews Framework – Definition of a Data Processing Task</p><figure data-equation="" data-image="27" data-figure-category="figure" data-caption="" id="F27995801" data-image-src="/media/images/b3d7513b-dda1-4bf9-8262-5a1cbac6dea4.png"><div><img src="b3d7513b-dda1-4bf9-8262-5a1cbac6dea4.png"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 1: </span></figcaption></figure><p><em>2.1. Interacting with UnifiedViews </em></p><p>UnifiedViews provides a graphical user interface to define, manage, execute, monitor, debug, schedule, and share DPUs and pipelines. A screenshot of this interface is illustrated in Figure 1. It shows a data processing pipeline, consisting of five DPUs (colored boxes) and four data flows (arrows connecting the boxes) between these DPUs. DPUs may be added to a pipeline by drag &amp; dropping them on a pipeline canvas. Data flows between two DPUs may be denoted by drawing an edge between them. Labels on the data flow edges clarify which output data units are mapped to which input data units of the DPUs. In Figure 1, all data flows always map output data unit with the name output to input data unit of the next DPU with the name input; however, custom data unit names may be used by DPU developers, which are then reflected in the user interface. As the pipeline is being prepared, UnifiedViews provides data wranglers with debugging capabilities; the data wrangler may execute a selected fragment of the pipeline at any time and browse or query (using the SPARQL query language) the entries in the input/output data units that are consumed/produced by each DPU. When data wranglers are satisfied with the prepared pipelines, they can manually execute them and verify the results. Alternatively, it is possible to schedule pipelines for execution (1) once at certain time, (2) every certain period of time, or (3) after another pipeline is successfully executed. Data wranglers may also get notifications about the pipelines’ execution states – either for all executions of the selected pipelines or only for those which ended with an error. It is also possible to get daily summaries about the executions of the selected pipelines in the last 24 hours. UnifiedViews currently provides more than 35 core DPUs8 . These are available for data wranglers in each deployment of UnifiedViews and provide basic functionality needed for </p><p>– obtaining data from external sources (CSV, DBF, XLS, XML files, RDF data, or relational tables), </p><p>– transforming data between various formats (e.g. CSV files to RDF data, relational tables to RDF data), </p><p>– executing typical transformations such as executing SPARQL Construct/Update queries, executing XSL transformations, linking/fusing RDF data, unpacking/packing/filtering files, and </p><p>– loading the transformed and curated data to external systems (loaders to various RDF databases and generic SPARQL endpoints, loaders of files via FTP/SFTP/etc., loaders to relational databases). </p><p>Apart from that, the UnifiedViews team9 also provides additional DPUs. If a specific required functionality is not covered by a DPU available among the core DPUs or those provided by the UnifiedViews team, data wranglers may easily create and deploy their own custom DPUs. For this purpose, extensive documentation, such as tutorials, is provided online10. Basically, data wranglers may use UI wizards to generate skeletons for new DPUs and then they may use provided set of tutorials to declare input/output data units, consume inputs, produce outputs, generate events about the DPU’s execution, add extensions to DPUs, etc. All pipelines and DPU templates prepared by data wranglers are by default private (available only to the user who prepared them and also to administrators – 8https://github.com/UnifiedViews/Plugins 9http://unifiedviews.eu/#get-team 10https://grips.semantic-web.at/pages/ viewpage.action?pageId=50929588 5 see Section 2.2 for more details on roles). Nevertheless, UnifiedViews allows data wranglers to share prepared pipelines with other data wranglers, so that others can either see the pipeline (sharing in read-only mode) or even collaborate on the pipeline preparation (sharing with write access to the pipeline). Data wranglers may also create custom DPU templates (DPU templates available in the system with custom configurations) and share such DPU templates with others, so that others can use such custom DPU templates in their pipelines. </p><p><em>2.2. Architecture of UnifiedViews </em></p><p>UnifiedViews is composed of three main components: </p><p>– <em>Graphical user interface</em>: Being the primary means of interaction with the framework, the graphical user interface (henceforth referred to as frontend) supports definition, management, execution, monitoring, debugging, scheduling, and sharing of data processing pipelines and management of DPUs. The frontend is implemented in Java as a Web application using the Vaadin11 framework. Figure 1 shows an excerpt of the screen where pipelines may be designed. Screencast of UnifiedViews user interface is available at http://unifiedviews.eu. </p><p>– <em>Pipeline execution engine</em>: It is responsible for running the (scheduled) pipelines, and implemented as stand-alone Java application (henceforth referred to as backend).</p><p>– <em>REST API administration service</em>: It allows to define, manage, execute, and monitor data processing pipelines without using the frontend. For example, external applications may execute pipelines and get results of the executions by interacting with this component. </p><p>Frontend and backend communicate via a relational database, which stores all configuration information such as pipeline setups, DPU configurations, execution states, or scheduled events. To support scalability, multiple backend instances can run on different machines, effectively executing pipelines in parallel (see Figure 2). Each backend has its own identifier and all backends observe pending executions. If there is any execution pending, the first 11http://vaadin.com backend realizing that marks its identifier next to that pending execution and executes it. Every backend uses its own RDF Working Store for storing temporary data which is produced by the pipeline during its execution. As the RDF Working Store, we currently support Sesame12 repositories, either as a local native store or on a remote server, and GraphDB13. Experimental support for OpenLink Virtuoso is also in place and can be enabled by a configuration option. In general, any repository which supports the OpenRDF Sesame API (in the recent days renamed to Eclipse RDF4J API14), can be used as an RDF Working Store. Every DPU is an OSGi15 bundle. As a result, UnifiedViews can easily load/unload DPUs during runtime, allowing to, e.g., upgrade DPUs without restarting the framework. OSGi also ensures that two DPUs loaded to one UnifiedViews instance may use different versions of the same library without causing any conflicts. UnifiedViews also supports authentication and authorization of users. Two roles are supported by default - Users and Administrators. Each such role can be associated with a list of permissions, e.g., a permission to import new DPU templates. Anytime a data wrangler wants to interact (view, edit, save, delete, etc.) with a certain entity (pipeline, DPU, scheduled event, etc.), UnifiedViews checks whether that user is authorized to do so. Spring Security16 is used to ensure authentication and authorization of users. There are two ways how DPUs on the pipeline may communicate certain information to a data wrangler when being executed. They may either publish an event (important message), e.g., that DPU esparqlExtractor was successfully executed and extracted 1000 triples, or just log something using standard mechanism for logging in Java. Both messages – events and logs – support various levels of severity (error, warning, info, etc.) and are displayed in the frontend of UnifiedViews as the pipeline is being executed. The reason why we distinguish events and logs (and also display them differently in the frontend) is to give data wranglers high-level overview of what happened during pipeline execution (through events) and to give them the possibility to examine logs in case more in12http://rdf4j.org/ 13https://ontotext.com/products/graphdb/ 14http://rdf4j.org/ 15https://www.osgi.org/ 16http://projects.spring.io/spring-security/ 6 Fig. 2. UnifiedViews Framework – Architecture formation is needed. DPUs may also throw an execution exception, which is semantically equivalent to sending an event with the severity error. When such an exception is thrown, the pipeline execution is stopped.</p><figure data-equation="" data-image="28" data-figure-category="figure" data-caption="" id="F84920791" data-image-src="/media/images/60ae22de-1f53-4f86-abdc-2a8bf0b73087.jpg"><div><img src="60ae22de-1f53-4f86-abdc-2a8bf0b73087.jpg"/></div><figcaption><span class="figure-cat-figure" data-figure-category="figure">figure 2: </span></figcaption></figure><p> </p><p><em>2.3. Availability of UnifiedViews </em></p><p>The source code of UnifiedViews is published under an open-source license which is a combination of GPLv317 and LGPLv318 licenses. It is hosted at GitHub19 and divided into the following repositories: </p><ul><li><p><em>Plugin-devEnv</em>: It contains UnifiedViews APIs (for DPUs, configuration dialogs, data units, etc.) and also a set of helper classes, which simplify development of new DPUs. 17http://www.gnu.org/licenses/gpl.txt 18http://www.gnu.org/licenses/lgpl.txt 19https://github.com/UnifiedViews </p></li><li><p><em>Core</em>: It contains implementations of the UnifiedViews APIs from Plugin-devEnv, including also implementations of the supported data units, frontend, backend, and REST API administration service components. </p></li><li><p><em>Plugins</em>: It contains a set of core DPUs. </p></li><li><p><em>Plugins-QualityAssessment</em>: It contains DPUs which assess the quality of processed data. </p></li></ul><p>All information about UnifiedViews, including the documentation and tutorial for building DPUs, may be found at the project’s website20. For a quick start, 20http://unifiedviews.eu 7 UnifiedViews can be also easily deployed using the Docker21 or Vagrant22 images</p><p><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">( Bizer, Heath, &amp; Berners-Lee, 2009)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:2439572461}]">(Bizer et al., 2009)</span><span class="citation" data-format="autocite" data-references="[{&quot;id&quot;:0}]">( Bizer et al., 2009)</span></p></div>
                    	</section></div><h1 class="article-bibliography-header"></h1><section id="references">
			            <h2>References</h2>
                        <div datatype="rdf:HTML" rel="schema:hasPart" typeof="deo:Reference">
                            <ol>
  <li><cite> Bizer, C., Heath, T., &amp; Berners-Lee, T. (Eds.). (2009). Linked data - the story so far. International Journal on Semantic Web and Information Systems, 5(3):1–22.</cite></li>
  <li><cite>Bizer, C., Lehmann, J., Kobilarov, G., Auer, S., Becker, C., Cyganiak, R., &amp; Hellmann, S. (Eds.). (2009). DBpedia - A crystallization point for the web of data. Journal of Web Semantics, 7(3):154–165, .</cite></li>
</ol>
			            </div>
                    </section>
			    </div>
			</article>
		</main>
	</body>
</html>