<http://localhost:8000/document/7/#923871783#comment-923871783> <http://www.w3.org/1999/02/22-rdf-syntax-ns#value> "\n        Up in the air\n    "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#4.2Humanaccuracy> <http://schema.org/name> "4.2 Human accuracy" .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Document> .
<http://www.dc4plus.com/references/rdf_sem.html#3Existingdatasets> <http://schema.org/name> "3 Existing datasets" .
<http://localhost:8000/document/7/#923871783> <http://schema.org/creator> <http://www.dc4plus.com/references/userURI#1> .
_:N5163f97eb4a240e8b34b56aabf4a606a <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:Ndf7dd9190a464172a49b7c2c37244c90 .
_:N6b602b7ae01b428a968c9d2ea8e3ff32 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N5163f97eb4a240e8b34b56aabf4a606a .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Article> .
<http://localhost:8000/document/7/#923871783> <http://www.w3.org/ns/oa#hasBody> <http://localhost:8000/document/7/#923871783#comment-923871783> .
_:Ndf7dd9190a464172a49b7c2c37244c90 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N94f044da9cf04092afc412b7e9d5334b .
_:Nbf62a770b6074824ae17d765c0515381 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://www.dc4plus.com/references/r-923871783> <http://purl.org/spar/cito/hasReplyFrom> <http://localhost:8000/document/7//comment-923871783> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/ontology/bibo/Document> .
<http://www.dc4plus.com/references/rdf_sem.html#5.2Mainresults> <http://schema.org/name> "5.2 Main results" .
<http://www.dc4plus.com/references/rdf_sem.html#6Discussion> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Discussion> .
<http://www.dc4plus.com/references/rdf_sem.html#4SQuAD2.0> <http://schema.org/description> "\n			                <p> We now describe our new dataset, which we constructed to satisfy both the relevance and plausible answer desiderata from Section 2. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://localhost:8000/document/7/#923871783> <http://www.w3.org/ns/oa#motivatedBy> <http://www.w3.org/ns/oa#replying> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/prov#Entity> .
<http://www.dc4plus.com/references/rdf_sem.html#4.1Datasetcreation> <http://schema.org/name> "4.1 Dataset creation" .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/hasPart> _:N18edae7640874158adefd7178d2babbf .
<http://www.dc4plus.com/references/userURI#1> <http://schema.org/name> "\n           Wrishi\n       " .
<http://www.dc4plus.com/references/rdf_sem.html#3.1Extractivedatasets> <http://schema.org/name> "3.1 Extractive datasets" .
<http://www.dc4plus.com/references/rdf_sem.html#4SQuAD2.0> <http://schema.org/name> "4 SQuAD 2.0" .
_:N18edae7640874158adefd7178d2babbf <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#1Introduction> .
<http://www.dc4plus.com/references/rdf_sem.html#RobinJiaEmail:robinjia@cs.stanford.edu(StanfordUniversity)> <http://schema.org/author> _:Nd97f1f3683b14cf6a9d47ab794744e59 .
<http://www.dc4plus.com/references/rdf_sem.html#5Experiments> <http://schema.org/description> "\n			                "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://schema.org/description> "\n			                <p>Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (Hermann et al., 2015; Hewlett et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017). In turn, these datasets have spurred a diverse array of model architecture improvements (Seo et al., 2016; Hu et al., 2017; Wang et al., 2017; Clark and Gardner, 2017; Huang et al., 2018). Recent work has \u2217 The first two authors contributed equally to this paper. Article: Endangered Species Act Paragraph: \u201C . . . Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales, and the Bald Eagle Protection Act of 1940. These later laws had a low cost to society\u2014the species were relatively rare\u2014and little opposition was raised.\u201D Question 1: \u201CWhich laws faced significant opposition?\u201D Plausible Answer: later laws Question 2: \u201CWhat was the name of the 1937 treaty?\u201D Plausible Answer: Bald Eagle Protection Act Figure 1: Two unanswerable questions written by crowdworkers, along with plausible (but incorrect) answers. Relevant keywords are shown in blue. even produced systems that surpass human-level exact match accuracy on the Stanford Question Answering Dataset (SQuAD), one of the most widely-used reading comprehension benchmarks (Rajpurkar et al., 2016). Nonetheless, these systems are still far from true language understanding. Recent analysis shows that models can do well at SQuAD by learning context and type-matching heuristics (Weissenborn et al., 2017), and that success on SQuAD does not ensure robustness to distracting sentences (Jia and Liang, 2017). One root cause of these problems is SQuAD\u2019s focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQuAD 2.0,1 a new dataset that combines answerable questions from the previous version of SQuAD (SQuAD 1.1) 1 In the ACL version of this paper, we called our new dataset SQUADRUN; here we use the name SQuAD 2.0, to emphasize that it is in fact the new version of SQuAD. with 53,775 new, unanswerable questions about the same paragraphs. Crowdworkers crafted these questions so that (1) they are relevant to the paragraph, and (2) the paragraph contains a plausible answer\u2014something of the same type as what the question asks for. Two such examples are shown in Figure 1. We confirm that SQuAD 2.0 is both challenging and high-quality. A state-of-the-art model achieves only 66.3% F1 score when trained and tested on SQuAD 2.0, whereas human accuracy is 89.5% F1, a full 23.2 points higher. The same model architecture trained on SQuAD 1.1 gets 85.8% F1, only 5.4 points worse than humans. We also show that our unanswerable questions are more challenging than ones created automatically, either via distant supervision (Clark and Gardner, 2017) or a rule-based method (Jia and Liang, 2017). We release SQuAD 2.0 to the public as new version of SQuAD, and make it the primary benchmark on the official SQuAD leaderboard.2 We are optimistic that this new dataset will encourage the development of reading comprehension systems that know what they don\u2019t know. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
_:N221e145012c444019a1335e9fd0f0914 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
_:Nd97f1f3683b14cf6a9d47ab794744e59 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/person> .
<http://www.dc4plus.com/references/userURI#1> <http://schema.org/image> <https://www.gravatar.com/avatar/0397eeb87e26782f66df823775d58a71/?s=80> .
<http://localhost:8000/document/7/#923871783#comment-923871783> <http://schema.org/description> "\n        Up in the air\n    "^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#PranavRajpurkarEmail:pranavsr@cs.stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:Nbf62a770b6074824ae17d765c0515381 .
<http://www.dc4plus.com/references/rdf_sem.html#5Experiments> <http://schema.org/name> "5 Experiments" .
<http://www.dc4plus.com/references/rdf_sem.html#PercyLiangEmail:pliang@cs.stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:N221e145012c444019a1335e9fd0f0914 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#RobinJiaEmail:robinjia@cs.stanford.edu(StanfordUniversity)> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#PercyLiangEmail:pliang@cs.stanford.edu(StanfordUniversity)> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://purl.org/ontology/bibo/authorList> <http://www.dc4plus.com/references/rdf_sem.html#PranavRajpurkarEmail:pranavsr@cs.stanford.edu(StanfordUniversity)> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/hasPart> _:N0f185982b90a44068c4555965af6c894 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://www.w3.org/ns/activitystreams#Article> .
<http://www.dc4plus.com/references/rdf_sem.html#2Desiderata> <http://schema.org/description> "\n			                <p>We first outline our goals for SQuAD 2.0. Besides the generic goals of large size, diversity, and low noise, we posit two desiderata specific to unanswerable questions: </p><p><strong>Relevance</strong>. The unanswerable questions should appear relevant to the topic of the context paragraph. Otherwise, simple heuristics (e.g., based on word overlap) could distinguish answerable and unanswerable questions (Yih et al., 2013). </p><p><strong>Existence of plausible answers</strong>. There should be some span in the context whose type matches the type of answer the question asks for. For example, if the question asks, \u201CWhat company was founded in 1992?\u201D, then some company should appear in the context. Otherwise, type-matching heuristics could distinguish answerable and unanswerable questions (Weissenborn et al., 2017). </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#2Desiderata> <http://schema.org/name> "2 Desiderata" .
<http://www.dc4plus.com/references/rdf_sem.html#5.4Plausibleanswersasdistractors> <http://schema.org/name> "5.4 Plausible answers as distractors " .
<http://www.dc4plus.com/references/rdf_sem.html#6Discussion> <http://schema.org/description> "\n			                <p>SQuAD 2.0 forces models to understand whether a paragraph entails that a certain span is the answer to a question. Similarly, recognizing textual entailment (RTE) requires systems to decide whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise (Marelli et al., 2014; Bowman et al., 2015). Relation extraction systems must understand when a possible relationship between two entities is not entailed by the text (Zhang et al., 2017). Jia and Liang (2017) created adversarial test examples that fool models trained on SQuAD 1.1. However, models that are trained on similar examples are not easily fooled by their method. In contrast, the adversarial examples in SQuAD 2.0 are difficult even for models trained on examples from the same distribution. In conclusion, we have presented SQuAD 2.0, a challenging, diverse, and large-scale dataset that forces models to understand when a question cannot be answered given the context. We are optimistic that SQuAD 2.0 will encourage the development of new reading comprehension models that know what they don\u2019t know, and therefore understand language at a deeper level.</p><p><strong>Reproducibility</strong>. All code, data, experiments are available on the CodaLab platform at https: //bit.ly/2rDHBgY. </p><p><strong>Acknowledgments</strong>. We would like to thank the anonymous reviewers, Arun Chaganty, Peng Qi, and Sharon Zhou for their constructive feedback. We are grateful to Durim Morina and Michael Bernstein for their help with the Daemo platform. This work was supported by funding from Facebook. R.J. is supported by an NSF Graduate Research Fellowship under Grant No. DGE-114747.</p><figure data-caption=\"\" data-equation=\"\" data-figure-category=\"figure\" data-image=\"8\" data-image-src=\"/media/images/44762b5e-3d81-4b5f-b802-f663919cb7c8.png\" id=\"F64569211\"><div><img src=\"44762b5e-3d81-4b5f-b802-f663919cb7c8.png\"/></div><figcaption><span class=\"figure-cat-figure\" data-figure-category=\"figure\">figure 1: </span></figcaption></figure><p> </p><p><span class=\"citation\" data-format=\"autocite\" data-references=\"[{&quot;id&quot;:0}]\">(Bowman, Angeli, Potts, &amp; Manning, 2015)</span></p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#PranavRajpurkarEmail:pranavsr@cs.stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:Nbf62a770b6074824ae17d765c0515381 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/abstract> "<p>Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/rdf_sem.html#PercyLiangEmail:pliang@cs.stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:N221e145012c444019a1335e9fd0f0914 .
_:N0f185982b90a44068c4555965af6c894 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Reference> .
<http://www.dc4plus.com/references/userURI#1> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/Person> .
<http://www.dc4plus.com/references/rdf_sem.html#3.3Multiplechoicedatasets> <http://schema.org/name> "3.3 Multiple choice datasets" .
<http://www.dc4plus.com/references/rdf_sem.html#5.1Models> <http://schema.org/name> "5.1 Models " .
_:N94f044da9cf04092afc412b7e9d5334b <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N97f9f6887d4d41c9871f621d9579fb78 .
<http://www.dc4plus.com/references/rdf_sem.html#3.2Answersentenceselectiondatasets> <http://schema.org/name> "3.2 Answer sentence selection datasets " .
_:N94f044da9cf04092afc412b7e9d5334b <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#5Experiments> .
<http://localhost:8000/document/7/#923871783> <http://schema.org/name> "\n    Wrishi\n    replies\n" .
_:N6b602b7ae01b428a968c9d2ea8e3ff32 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#2Desiderata> .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://schema.org/name> "1 Introduction " .
<http://localhost:8000/document/7/#923871783#comment-923871783> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/oa#TextualBody> .
<http://localhost:8000/document/7/#923871783#comment-923871783> <http://schema.org/name> "Comment" .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://rdfs.org/sioc/ns#Post> .
<http://www.dc4plus.com/references/rdf_sem.html#RobinJiaEmail:robinjia@cs.stanford.edu(StanfordUniversity)> <http://schema.org/creator> _:Nd97f1f3683b14cf6a9d47ab794744e59 .
<http://www.dc4plus.com/references/rdf_sem.html#5.4Plausibleanswersasdistractors> <http://schema.org/hasPart> <http://www.dc4plus.com/references/r-923871783> .
_:N5163f97eb4a240e8b34b56aabf4a606a <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#3Existingdatasets> .
_:N97f9f6887d4d41c9871f621d9579fb78 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#nil> .
<http://localhost:8000/document/7/#923871783> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/oa#Annotation> .
<http://www.dc4plus.com/references/rdf_sem.html#5Experiments> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Evaluation> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://schema.org/name> "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD" .
_:N18edae7640874158adefd7178d2babbf <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:N6b602b7ae01b428a968c9d2ea8e3ff32 .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/net/biblio#Paper> .
<http://www.dc4plus.com/references/r-923871783> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/dc/dcmitype/Text> .
<http://www.dc4plus.com/references/rdf_sem.html> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/ScholarlyArticle> .
<http://www.dc4plus.com/references/rdf_sem.html#4.3Analysis> <http://schema.org/name> "4.3 Analysis " .
<http://www.dc4plus.com/references/rdf_sem.html#PercyLiangEmail:pliang@cs.stanford.edu(StanfordUniversity)> <http://schema.org/author> _:N221e145012c444019a1335e9fd0f0914 .
_:Ndf7dd9190a464172a49b7c2c37244c90 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#4SQuAD2.0> .
<http://www.dc4plus.com/references/rdf_sem.html#RobinJiaEmail:robinjia@cs.stanford.edu(StanfordUniversity)> <http://schema.org/publisher> _:Nd97f1f3683b14cf6a9d47ab794744e59 .
<http://www.dc4plus.com/references/rdf_sem.html#PranavRajpurkarEmail:pranavsr@cs.stanford.edu(StanfordUniversity)> <http://schema.org/author> _:Nbf62a770b6074824ae17d765c0515381 .
<http://www.dc4plus.com/references/rdf_sem.html#6Discussion> <http://schema.org/name> "6 Discussion " .
<http://localhost:8000/document/7/#923871783> <http://schema.org/datePublished> "1540460832301"^^<http://www.w3.org/2001/XMLSchema#dateTime> .
_:N97f9f6887d4d41c9871f621d9579fb78 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://www.dc4plus.com/references/rdf_sem.html#6Discussion> .
<http://www.dc4plus.com/references/rdf_sem.html#1Introduction> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.org/spar/deo/Introduction> .
<http://www.dc4plus.com/references/rdf_sem.html#5.3Automaticallygeneratednegatives> <http://schema.org/name> "5.3 Automatically generated negatives" .
<http://www.dc4plus.com/references/rdf_sem.html#3Existingdatasets> <http://schema.org/description> "\n			                <p> Next, we survey existing reading comprehension datasets with these criteria in mind. We use the 2 As with previous versions of SQuAD, we release SQuAD 2.0 under the CC BY-SA 4.0 license. term \u201Cnegative example\u201D to refer to a context passage paired with an unanswerable question. </p>"^^<http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML> .
<http://www.dc4plus.com/references/r-923871783> <http://schema.org/description> "\n                        Finally, we measured how often systems were fooled into answering the plausible but incorrect answers provided by crowdworkers for our unanswerable questions. For both computer systems and humans, roughly half of all wrong answers on unanswerable questions exactly matched the plausible answers. This suggests that the plausible answers do indeed serve as effective distractors\n                    " .

